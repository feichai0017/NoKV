<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>NoKV Docs</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="docs/custom-7218a61a.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-fe1861e1.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-d8cfaf9e.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">NoKV Docs</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/feichai0017/NoKV" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<div align="center">
  <img src="https://raw.githubusercontent.com/feichai0017/NoKV/main/img/logo.svg" width="180" alt="NoKV Logo">
  
<h1>NoKV</h1>

  
<p style="font-size: 1.2em; color: #666;">
    <strong>High-Performance, Cloud-Native Distributed Key-Value Database</strong>
  </p>

  
<p>
    <!-- Build / Quality -->
    <a href="https://github.com/feichai0017/NoKV/actions">
      <img alt="CI" src="https://img.shields.io/github/actions/workflow/status/feichai0017/NoKV/go.yml?branch=main" />
    </a>
    <a href="https://codecov.io/gh/feichai0017/NoKV">
      <img alt="Coverage" src="https://img.shields.io/codecov/c/gh/feichai0017/NoKV" />
    </a>
    <a href="https://goreportcard.com/report/github.com/feichai0017/NoKV">
      <img alt="Go Report Card" src="https://img.shields.io/badge/go%20report-A+-brightgreen" />
    </a>
    <a href="https://pkg.go.dev/github.com/feichai0017/NoKV">
      <img alt="Go Reference" src="https://img.shields.io/badge/go.dev-reference-007d9c?logo=go&amp;logoColor=white" />
    </a>
    <a href="https://github.com/avelino/awesome-go#databases-implemented-in-go">
      <img alt="Mentioned in Awesome" src="https://awesome.re/mentioned-badge.svg" />
    </a>
  </p>

  
<p>
    <!-- Meta -->
    <img alt="Go Version" src="https://img.shields.io/badge/go-1.24%2B-00ADD8?logo=go&amp;logoColor=white" />
    <img alt="License" src="https://img.shields.io/badge/license-Apache--2.0-yellow" />
    <a href="https://deepwiki.com/feichai0017/NoKV">
      <img alt="DeepWiki" src="https://img.shields.io/badge/DeepWiki-Ask-6f42c1" />
    </a>
  </p>

  
<p>
    <a href="#getting-started" style="text-decoration: none;">
      <button style="background-color: #007bff; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; font-size: 1em;">ğŸš€ Quick Start</button>
    </a>
    Â Â 
    <a href="#nokv-architecture-overview" style="text-decoration: none;">
      <button style="background-color: #6c757d; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; font-size: 1em;">ğŸ—ï¸ Architecture</button>
    </a>
  </p>

</div>

<br>
<hr>
<h2 id="-why-nokv"><a class="header" href="#-why-nokv">ğŸ”¥ Why NoKV?</a></h2>
<p>NoKV is designed for <strong>modern hardware</strong> and <strong>distributed workloads</strong>. It combines the best of academic research (WiscKey, W-TinyLFU) with industrial-grade engineering (Raft, Percolator).</p>
<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-top: 20px;">
  
<div style="border: 1px solid #e1e4e8; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); background-color: var(--bg);">
    
<h3 style="margin-top: 0;">ğŸï¸ Extreme Performance</h3>

    
<p><strong>Lock-light</strong> commit queue and <strong>Batch WAL</strong> writing deliver write throughput that saturates NVMe SSDs.</p>

  </div>

  
<div style="border: 1px solid #e1e4e8; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); background-color: var(--bg);">
    
<h3 style="margin-top: 0;">ğŸ§  Smart Caching</h3>

    
<p>Built-in <strong>W-TinyLFU</strong> Block Cache (via Ristretto) and <strong>HotRing</strong> implementation ensure 99% cache hit rates and adapt to skew access patterns.</p>

  </div>

  
<div style="border: 1px solid #e1e4e8; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); background-color: var(--bg);">
    
<h3 style="margin-top: 0;">ğŸŒ Distributed Consistency</h3>

    
<p><strong>Multi-Raft</strong> replication for high availability. <strong>Percolator</strong> model for cross-row ACID transactions. Snapshot Isolation by default.</p>

  </div>

  
<div style="border: 1px solid #e1e4e8; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); background-color: var(--bg);">
    
<h3 style="margin-top: 0;">ğŸ”Œ Redis Compatible</h3>

    
<p>Drop-in replacement for Redis. Supports the <strong>RESP protocol</strong> so you can use your existing tools and client libraries.</p>

  </div>

</div>

<br>
<h2 id="-performance-benchmark"><a class="header" href="#-performance-benchmark">ğŸ“Š Performance Benchmark</a></h2>
<p>NoKV outperforms BadgerDB significantly in read-heavy and mixed workloads.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th style="text-align: left">Workload</th><th style="text-align: left">Operation</th><th style="text-align: left">NoKV (OPS)</th><th style="text-align: left">Badger (OPS)</th><th style="text-align: left">Improvement</th></tr>
</thead>
<tbody>
<tr><td style="text-align: left"><strong>YCSB-C</strong></td><td style="text-align: left">100% Read</td><td style="text-align: left"><strong>1,540,744</strong></td><td style="text-align: left">521,586</td><td style="text-align: left"><span style="color:green"><strong>+195%</strong></span> ğŸš€</td></tr>
<tr><td style="text-align: left"><strong>YCSB-B</strong></td><td style="text-align: left">95% Read</td><td style="text-align: left"><strong>911,199</strong></td><td style="text-align: left">349,608</td><td style="text-align: left"><span style="color:green"><strong>+160%</strong></span></td></tr>
<tr><td style="text-align: left"><strong>YCSB-A</strong></td><td style="text-align: left">50% Update</td><td style="text-align: left"><strong>410,578</strong></td><td style="text-align: left">262,153</td><td style="text-align: left"><span style="color:green"><strong>+56%</strong></span></td></tr>
<tr><td style="text-align: left"><strong>YCSB-D</strong></td><td style="text-align: left">5% Insert</td><td style="text-align: left"><strong>1,270,717</strong></td><td style="text-align: left">707,607</td><td style="text-align: left"><span style="color:green"><strong>+79%</strong></span></td></tr>
</tbody>
</table>
</div>
<details>
<summary><em>Click to view detailed latency stats</em></summary>
<pre><code class="language-text">Summary:
ENGINE  OPERATION  MODE                          OPS/S    AVG LATENCY  P99
NoKV    YCSB-C     100% read                     1540744  649ns        128Âµs
NoKV    YCSB-A     50/50 read/update             410578   2.435Âµs      155Âµs
Badger  YCSB-C     100% read                     521586   1.917Âµs      427Âµs
Badger  YCSB-A     50/50 read/update             262153   3.814Âµs      160Âµs
</code></pre>
</details>
<br>
<h2 id="-architecture"><a class="header" href="#-architecture">ğŸ—ï¸ Architecture</a></h2>
<pre class="mermaid">graph TD
    Client["Client / Redis"] --&gt;|RESP Protocol| Gateway["Redis Gateway"]
    Gateway --&gt;|RaftCmd| RaftStore
    
    subgraph "RaftStore (Distributed Layer)"
        RaftStore --&gt;|Propose| RaftLog["Raft Log (WAL)"]
        RaftLog --&gt;|Consensus| Apply["Apply Worker"]
    end
    
    subgraph "Storage Engine (LSM)"
        Apply --&gt;|Batch Set| MemTable
        MemTable --&gt;|Flush| SSTable["SSTables (L0-L6)"]
        SSTable --&gt;|Compact| SSTable
        
        Apply --&gt;|Large Value| VLog["Value Log"]
    end
    
    subgraph "Cache Layer"
        BlockCache["Block Cache (Ristretto)"] -.-&gt; SSTable
        IndexCache["Index Cache (W-TinyLFU)"] -.-&gt; SSTable
    end
</pre>

<h2 id="-roadmap"><a class="header" href="#-roadmap">ğŸ—ºï¸ Roadmap</a></h2>
<ul>
<li><input disabled="" type="checkbox" checked=""> <strong>Core</strong>: LSM Tree, VLog, WAL</li>
<li><input disabled="" type="checkbox" checked=""> <strong>Distributed</strong>: Multi-Raft, Split/Merge</li>
<li><input disabled="" type="checkbox" checked=""> <strong>Transaction</strong>: Percolator (Snapshot Isolation)</li>
<li><input disabled="" type="checkbox"> <strong>Optimization</strong>: Async Apply, SSTable-based Snapshot</li>
<li><input disabled="" type="checkbox"> <strong>Redis</strong>: Hash/Set/ZSet support</li>
</ul>
<h2 id="-contributing"><a class="header" href="#-contributing">ğŸ¤ Contributing</a></h2>
<p>We welcome contributions! Please see <a href="CONTRIBUTING.html">CONTRIBUTING.md</a> for details.</p>
<div align="center">
  <sub>Built with â¤ï¸ by <a href="https://github.com/feichai0017">feichai0017</a> and contributors.</sub>
</div>

<div style="break-before: page; page-break-before: always;"></div>
<h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>This guide gets you from zero to a running NoKV cluster (or an embedded DB) in a few minutes.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ul>
<li>Go 1.24+</li>
<li>Git</li>
<li>(Optional) Docker + Docker Compose for containerized runs</li>
</ul>
<h2 id="option-a-local-cluster-recommended-for-dev"><a class="header" href="#option-a-local-cluster-recommended-for-dev">Option A: Local Cluster (recommended for dev)</a></h2>
<p>This launches a 3-node Raft cluster plus the optional TSO helper.</p>
<pre><code class="language-bash">./scripts/run_local_cluster.sh --config ./raft_config.example.json
</code></pre>
<p>Start the Redis-compatible gateway in another shell:</p>
<pre><code class="language-bash">go run ./cmd/nokv-redis --addr 127.0.0.1:6380 --raft-config raft_config.example.json
</code></pre>
<p>Quick smoke test:</p>
<pre><code class="language-bash">redis-cli -p 6380 ping
</code></pre>
<h3 id="inspect-stats"><a class="header" href="#inspect-stats">Inspect stats</a></h3>
<pre><code class="language-bash">go run ./cmd/nokv stats --workdir ./artifacts/cluster/store-1
</code></pre>
<h2 id="option-b-docker-compose"><a class="header" href="#option-b-docker-compose">Option B: Docker Compose</a></h2>
<p>This runs the cluster and gateway in containers.</p>
<pre><code class="language-bash">docker compose up --build
</code></pre>
<p>Tear down:</p>
<pre><code class="language-bash">docker compose down -v
</code></pre>
<h2 id="embedded-usage-single-process"><a class="header" href="#embedded-usage-single-process">Embedded Usage (single-process)</a></h2>
<p>Use NoKV as a library when you do not need raftstore.</p>
<pre><code class="language-go">package main

import (
	"fmt"
	"log"

	NoKV "github.com/feichai0017/NoKV"
)

func main() {
	opt := NoKV.NewDefaultOptions()
	opt.WorkDir = "./workdir-demo"

	db := NoKV.Open(opt)
	defer db.Close()

	key := []byte("hello")
	if err := db.Set(key, []byte("world")); err != nil {
		log.Fatalf("set failed: %v", err)
	}

	entry, err := db.Get(key)
	if err != nil {
		log.Fatalf("get failed: %v", err)
	}
	fmt.Printf("value=%s\n", entry.Value)
	entry.DecrRef()
}
</code></pre>
<h2 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h2>
<p>Micro benchmarks:</p>
<pre><code class="language-bash">go test -bench=. -run=^$ ./...
</code></pre>
<p>YCSB (NoKV engine only by default):</p>
<pre><code class="language-bash">make bench
</code></pre>
<p>Override defaults with env vars:</p>
<pre><code class="language-bash">YCSB_RECORDS=1000000 YCSB_OPS=1000000 YCSB_CONC=8 make bench
</code></pre>
<h2 id="cleanup"><a class="header" href="#cleanup">Cleanup</a></h2>
<p>If a local run crashes or you want a clean slate:</p>
<pre><code class="language-bash">make clean
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<ul>
<li><strong>WAL replay errors after crash</strong>: wipe the workdir and restart the cluster.</li>
<li><strong>Port conflicts</strong>: adjust addresses in <code>raft_config.example.json</code>.</li>
<li><strong>Slow startup</strong>: reduce <code>YCSB_RECORDS</code> or <code>YCSB_OPS</code> when benchmarking locally.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="nokv-architecture-overview"><a class="header" href="#nokv-architecture-overview">NoKV Architecture Overview</a></h1>
<p>NoKV delivers a hybrid storage engine that can operate as a standalone embedded KV store or as a TinyKv-compatible distributed service. This document captures the key building blocks, how they interact, and the execution flow from client to disk.</p>
<hr>
<h2 id="1-high-level-layout"><a class="header" href="#1-high-level-layout">1. High-Level Layout</a></h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   TinyKv gRPC   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ raftstore Service       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ raftstore/client        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  (Get / Scan / Mutate)  â”‚
            â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ ReadCommand / ProposeCommand
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ store.Store / peer.Peer â”‚  â† multi-Raft region lifecycle
â”‚  â”œ Manifest snapshot    â”‚
â”‚  â”œ Router / RegionHooks â”‚
â”‚  â”” transport (gRPC)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Apply via kv.Apply
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ kv.Apply + percolator   â”‚
â”‚  â”œ Get / Scan           â”‚
â”‚  â”œ Prewrite / Commit    â”‚
â”‚  â”” Latch manager        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedded NoKV core      â”‚
â”‚  â”œ WAL Manager          â”‚
â”‚  â”œ MemTable / Flush     â”‚
â”‚  â”œ ValueLog + GC        â”‚
â”‚  â”” Manifest / Stats     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<ul>
<li><strong>Embedded mode</strong> uses <code>NoKV.Open</code> directly: WALâ†’MemTableâ†’SST durability, ValueLog separation, MVCC semantics, rich stats.</li>
<li><strong>Distributed mode</strong> layers <code>raftstore</code> on top: multi-Raft regions reuse the same WAL/Manifest, expose metrics, and serve TinyKv RPCs.</li>
<li><strong>Clients</strong> obtain leader-aware routing, automatic NotLeader/EpochNotMatch retries, and two-phase commit helpers.</li>
</ul>
<hr>
<h2 id="2-embedded-engine"><a class="header" href="#2-embedded-engine">2. Embedded Engine</a></h2>
<h3 id="21-wal--memtable"><a class="header" href="#21-wal--memtable">2.1 WAL &amp; MemTable</a></h3>
<ul>
<li><code>wal.Manager</code> appends <code>[len|type|payload|crc]</code> records (typed WAL), rotates segments, and replays logs on crash.</li>
<li><code>MemTable</code> accumulates writes until full, then enters the flush queue; <code>flush.Manager</code> runs <code>Prepare â†’ Build â†’ Install â†’ Release</code>, logs edits, and releases WAL segments.</li>
<li>Writes are handled by a single commit worker that performs value-log append first, then WAL/memtable apply, keeping durability ordering simple and consistent.</li>
</ul>
<h3 id="22-valuelog"><a class="header" href="#22-valuelog">2.2 ValueLog</a></h3>
<ul>
<li>Large values are written to the ValueLog before the WAL append; the resulting <code>ValuePtr</code> is stored in WAL/LSM so replay can recover.</li>
<li><code>vlog.Manager</code> tracks the active head and uses flush discard stats to trigger GC; manifest records new heads and removed segments.</li>
</ul>
<h3 id="23-manifest"><a class="header" href="#23-manifest">2.3 Manifest</a></h3>
<ul>
<li><code>manifest.Manager</code> stores SST metadata, WAL checkpoints, ValueLog metadata, and (importantly) Region descriptors used by raftstore.</li>
<li><code>CURRENT</code> provides crash-safe pointer updates; Region state is replicated through manifest edits.</li>
</ul>
<h3 id="24-lsm-compaction--ingest-buffer"><a class="header" href="#24-lsm-compaction--ingest-buffer">2.4 LSM Compaction &amp; Ingest Buffer</a></h3>
<ul>
<li><code>compact.Manager</code> drives compaction cycles; <code>lsm.levelManager</code> supplies table metadata and executes the plan.</li>
<li>Planning is split: <code>compact.PlanFor*</code> selects table IDs + key ranges, then LSM resolves IDs back to tables and runs the merge.</li>
<li><code>compact.State</code> guards overlapping key ranges and tracks in-flight table IDs.</li>
<li>Ingest shard selection is policy-driven in <code>compact</code> (<code>PickShardOrder</code> / <code>PickShardByBacklog</code>) while the ingest buffer remains in <code>lsm</code>.</li>
</ul>
<pre class="mermaid">flowchart TD
  Manager["compact.Manager"] --&gt; LSM["lsm.levelManager"]
  LSM --&gt;|TableMeta snapshot| Planner["compact.PlanFor*"]
  Planner --&gt; Plan["compact.Plan (fid+range)"]
  Plan --&gt;|resolvePlanLocked| Exec["LSM executor"]
  Exec --&gt; State["compact.State guard"]
  Exec --&gt; Build["subcompact/build SST"]
  Build --&gt; Manifest["manifest edits"]
  L0["L0 tables"] --&gt;|moveToIngest| Ingest["ingest buffer shards"]
  Ingest --&gt;|IngestDrain: ingest-only| Main["Main tables"]
  Ingest --&gt;|IngestKeep: ingest-merge| Ingest
</pre>

<h3 id="25-mvcc"><a class="header" href="#25-mvcc">2.5 MVCC</a></h3>
<ul>
<li><code>txn.go</code> exposes MVCC transactions with timestamps from <code>oracle</code>.</li>
<li><code>percolator</code> package implements Prewrite/Commit/ResolveLock/CheckTxnStatus; <code>kv.Apply</code> simply dispatches Raft commands to these helpers.</li>
<li>Watermarks (<code>utils.WaterMark</code>) gate read snapshots and commit visibility. They have no background goroutine; waiters block on per-index channels while advancement uses a mutex + atomics.</li>
</ul>
<h3 id="26-write-pipeline--backpressure"><a class="header" href="#26-write-pipeline--backpressure">2.6 Write Pipeline &amp; Backpressure</a></h3>
<ul>
<li>Writes enqueue into a commit queue (<code>db_write.go</code>) where requests are coalesced into batches before a commit worker drains them.</li>
<li>The commit worker always writes the value log first (when needed), then applies WAL/LSM updates; <code>SyncWrites</code> adds a WAL fsync step.</li>
<li>Batch sizing adapts to backlog (<code>WriteBatchMaxCount/Size</code>, <code>WriteBatchWait</code>) and hot-key pressure can expand batch limits temporarily to drain spikes.</li>
<li>Backpressure is enforced in two places: LSM throttling toggles <code>db.blockWrites</code> when L0 backlog grows, and HotRing can reject hot keys via <code>WriteHotKeyLimit</code>.</li>
</ul>
<hr>
<h2 id="3-replication-layer-raftstore"><a class="header" href="#3-replication-layer-raftstore">3. Replication Layer (raftstore)</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Package</th><th>Responsibility</th></tr>
</thead>
<tbody>
<tr><td><a href="../raftstore/store"><code>store</code></a></td><td>Region catalog, router, RegionMetrics, Region hooks, manifest integration, helpers such as <code>StartPeer</code> / <code>SplitRegion</code>.</td></tr>
<tr><td><a href="../raftstore/peer"><code>peer</code></a></td><td>Wraps etcd/raft <code>RawNode</code>, handles Ready pipeline, snapshot resend queue, backlog instrumentation.</td></tr>
<tr><td><a href="../raftstore/engine"><code>engine</code></a></td><td>WALStorage/DiskStorage/MemoryStorage, reusing the DBâ€™s WAL while keeping manifest metadata in sync.</td></tr>
<tr><td><a href="../raftstore/transport"><code>transport</code></a></td><td>gRPC transport for Raft Step messages, connection management, retries/blocks/TLS. Also acts as the host for TinyKv RPC.</td></tr>
<tr><td><a href="../raftstore/kv"><code>kv</code></a></td><td>TinyKv RPC handler plus <code>kv.Apply</code> bridging Raft commands to MVCC logic.</td></tr>
<tr><td><a href="../raftstore/server"><code>server</code></a></td><td><code>ServerConfig</code> + <code>New</code> combine DB, Store, transport, and TinyKv service into a reusable node instance.</td></tr>
</tbody>
</table>
</div>
<h3 id="31-bootstrap-sequence"><a class="header" href="#31-bootstrap-sequence">3.1 Bootstrap Sequence</a></h3>
<ol>
<li><code>raftstore.NewServer</code> wires DB, store configuration (StoreID, hooks, scheduler), Raft config, and transport address. It registers TinyKv RPC on the shared gRPC server and sets <code>transport.SetHandler(store.Step)</code>.</li>
<li>CLI (<code>nokv serve</code>) or application enumerates <code>Manifest.RegionSnapshot()</code> and calls <code>Store.StartPeer</code> for every Region containing the local store:
<ul>
<li><code>peer.Config</code> includes Raft params, transport, <code>kv.NewEntryApplier</code>, WAL/Manifest handles, Region metadata.</li>
<li>Router registration, regionManager bookkeeping, optional <code>Peer.Bootstrap</code> with initial peer list, leader campaign.</li>
</ul>
</li>
<li>Peers from other stores can be configured through <code>transport.SetPeer(storeID, addr)</code>, allowing dynamic updates from a scheduler.</li>
</ol>
<h3 id="32-command-paths"><a class="header" href="#32-command-paths">3.2 Command Paths</a></h3>
<ul>
<li><strong>ReadCommand</strong> (<code>KvGet</code>/<code>KvScan</code>): validate Region &amp; leader, flush pending Ready, then run <code>commandApplier</code> (i.e. <code>kv.Apply</code> in read mode) to fetch data directly from the DB. This yields leader-strong reads without a Raft round trip.</li>
<li><strong>ProposeCommand</strong> (write): encode the request, push through Router to the leader peer, replicate via Raft, and apply in <code>kv.Apply</code> which maps to MVCC operations.</li>
</ul>
<h3 id="33-transport"><a class="header" href="#33-transport">3.3 Transport</a></h3>
<ul>
<li>gRPC server handles Step RPCs and TinyKv RPCs on the same endpoint; peers are registered via <code>SetPeer</code>.</li>
<li>Retry policies (<code>WithRetry</code>) and TLS credentials are configurable. Tests cover partitions, blocked peers, and slow followers.</li>
</ul>
<hr>
<h2 id="4-tinykv-service"><a class="header" href="#4-tinykv-service">4. TinyKv Service</a></h2>
<p><code>raftstore/kv/service.go</code> exposes pb.TinyKv RPCs:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>RPC</th><th>Execution</th><th>Result</th></tr>
</thead>
<tbody>
<tr><td><code>KvGet</code></td><td><code>store.ReadCommand</code> â†’ <code>kv.Apply</code> GET</td><td><code>pb.GetResponse</code> / <code>RegionError</code></td></tr>
<tr><td><code>KvScan</code></td><td><code>store.ReadCommand</code> â†’ <code>kv.Apply</code> SCAN</td><td><code>pb.ScanResponse</code> / <code>RegionError</code></td></tr>
<tr><td><code>KvPrewrite</code></td><td><code>store.ProposeCommand</code> â†’ <code>percolator.Prewrite</code></td><td><code>pb.PrewriteResponse</code></td></tr>
<tr><td><code>KvCommit</code></td><td><code>store.ProposeCommand</code> â†’ <code>percolator.Commit</code></td><td><code>pb.CommitResponse</code></td></tr>
<tr><td><code>KvResolveLock</code></td><td><code>percolator.ResolveLock</code></td><td><code>pb.ResolveLockResponse</code></td></tr>
<tr><td><code>KvCheckTxnStatus</code></td><td><code>percolator.CheckTxnStatus</code></td><td><code>pb.CheckTxnStatusResponse</code></td></tr>
</tbody>
</table>
</div>
<p><code>nokv serve</code> is the CLI entry pointâ€”open the DB, construct <code>raftstore.Server</code>, register peers, start local Raft peers, and display a manifest summary (Regions, key ranges, peers). <code>scripts/run_local_cluster.sh</code> builds the CLI, writes a minimal region manifest, launches multiple <code>nokv serve</code> processes on localhost, and handles cleanup on Ctrl+C.</p>
<hr>
<h2 id="5-client-workflow"><a class="header" href="#5-client-workflow">5. Client Workflow</a></h2>
<p><code>raftstore/client</code> offers a leader-aware client with retry logic and convenient helpers:</p>
<ul>
<li><strong>Initialization</strong>: provide <code>[]StoreEndpoint</code> + <code>[]RegionConfig</code> describing region boundaries and known leaders.</li>
<li><strong>Reads</strong>: <code>Get</code> and <code>Scan</code> pick the leader store for a key range, issue TinyKv RPCs, and retry on NotLeader/EpochNotMatch.</li>
<li><strong>Writes</strong>: <code>Mutate</code> bundles operations per region and drives Prewrite/Commit (primary first, secondaries after); <code>Put</code> and <code>Delete</code> are convenience wrappers using the same 2PC path.</li>
<li><strong>Timestamps</strong>: clients must supply <code>startVersion</code>/<code>commitVersion</code>. For distributed demos, reuse the TSO sample under <code>scripts/tso</code> to obtain globally increasing values before calling <code>TwoPhaseCommit</code>.</li>
<li><strong>Bootstrap helpers</strong>: <code>scripts/run_local_cluster.sh --config raft_config.example.json</code> builds the binaries, seeds manifests via <code>nokv-config manifest</code>, launches the stores declared in the config, and starts the HTTP TSO allocator when the <code>tso</code> block is present.</li>
</ul>
<p><strong>Example (two regions)</strong></p>
<ol>
<li>Regions <code>[a,m)</code> and <code>[m,+âˆ)</code>, each led by a different store.</li>
<li><code>Mutate(ctx, primary="alfa", mutations, startTs, commitTs, ttl)</code> prewrites and commits across the relevant regions.</li>
<li><code>Get/Scan</code> retries automatically if the leader changes.</li>
<li>See <code>raftstore/server/server_client_integration_test.go</code> for a full end-to-end example using real <code>raftstore.Server</code> instances.</li>
</ol>
<hr>
<h2 id="6-failure-handling"><a class="header" href="#6-failure-handling">6. Failure Handling</a></h2>
<ul>
<li>Manifest edits capture Region metadata, WAL checkpoints, and ValueLog pointers. Restart simply reads <code>CURRENT</code> and replays edits.</li>
<li>WAL replay reconstructs memtables and Raft groups; ValueLog recovery trims partial records.</li>
<li><code>Stats.StartStats</code> resumes metrics sampling immediately after restart, making it easy to verify recovery correctness via <code>nokv stats</code>.</li>
</ul>
<hr>
<h2 id="7-observability--tooling"><a class="header" href="#7-observability--tooling">7. Observability &amp; Tooling</a></h2>
<ul>
<li><code>StatsSnapshot</code> publishes flush/compaction/WAL/VLog/txn/region metrics. <code>nokv stats</code> and the expvar endpoint expose the same data.</li>
<li><code>nokv regions</code> inspects Manifest-backed Region metadata.</li>
<li><code>nokv serve</code> advertises Region samples on startup (ID, key range, peers) for quick verification.</li>
<li>Scripts:
<ul>
<li><code>scripts/run_local_cluster.sh</code> â€“ launch a multi-node TinyKv cluster locally.</li>
<li><code>scripts/recovery_scenarios.sh</code> â€“ crash-recovery test harness.</li>
<li><code>scripts/transport_chaos.sh</code> â€“ inject network faults and observe transport metrics.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="8-when-to-use-nokv"><a class="header" href="#8-when-to-use-nokv">8. When to Use NoKV</a></h2>
<ul>
<li><strong>Embedded</strong>: call <code>NoKV.Open</code>, use the MVCC store locally.</li>
<li><strong>Distributed</strong>: deploy <code>nokv serve</code> nodes, use <code>raftstore/client</code> (or any TinyKv gRPC client) to perform reads, scans, and 2PC writes.</li>
<li><strong>Observability-first</strong>: inspection via CLI or expvar is built-in; Region, WAL, Flush, and Raft metrics are accessible without extra instrumentation.</li>
</ul>
<p>See also <a href="#raftstore-deep-dive"><code>docs/raftstore.md</code></a> for deeper internals and <a href="#testing--validation-matrix"><code>docs/testing.md</code></a> for coverage details.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="configuration--options"><a class="header" href="#configuration--options">Configuration &amp; Options</a></h1>
<p>NoKV exposes two configuration surfaces:</p>
<ol>
<li><strong>Runtime options</strong> for the embedded engine (<code>Options</code> in <code>options.go</code>).</li>
<li><strong>Cluster topology</strong> for distributed mode (<code>raft_config.example.json</code> via
<code>config.LoadFile/Validate</code>).</li>
</ol>
<hr>
<h2 id="1-runtime-options-embedded-engine"><a class="header" href="#1-runtime-options-embedded-engine">1. Runtime Options (Embedded Engine)</a></h2>
<p><code>NoKV.NewDefaultOptions()</code> returns a tuned baseline. Override fields before
calling <code>NoKV.Open(opt)</code>.</p>
<p>Key option groups (see <code>options.go</code> for the full list):</p>
<ul>
<li><strong>Paths &amp; durability</strong>
<ul>
<li><code>WorkDir</code>, <code>SyncWrites</code>, <code>ManifestSync</code>, <code>ManifestRewriteThreshold</code></li>
</ul>
</li>
<li><strong>Write pipeline</strong>
<ul>
<li><code>WriteBatchMaxCount</code>, <code>WriteBatchMaxSize</code>, <code>WriteBatchWait</code></li>
<li><code>CommitPipelineDepth</code>, <code>CommitApplyConcurrency</code></li>
</ul>
</li>
<li><strong>Value log</strong>
<ul>
<li><code>ValueThreshold</code>, <code>ValueLogFileSize</code>, <code>ValueLogMaxEntries</code></li>
<li><code>ValueLogGCInterval</code>, <code>ValueLogGCDiscardRatio</code></li>
<li><code>ValueLogGCSampleSizeRatio</code>, <code>ValueLogGCSampleCountRatio</code>,
<code>ValueLogGCSampleFromHead</code></li>
</ul>
</li>
<li><strong>LSM &amp; compaction</strong>
<ul>
<li><code>MemTableSize</code>, <code>MemTableEngine</code>, <code>SSTableMaxSz</code>, <code>NumCompactors</code></li>
<li><code>NumLevelZeroTables</code>, <code>IngestCompactBatchSize</code>, <code>IngestBacklogMergeScore</code></li>
<li><code>CompactionValueWeight</code>, <code>CompactionValueAlertThreshold</code></li>
</ul>
</li>
<li><strong>Caches</strong>
<ul>
<li><code>BlockCacheSize</code>, <code>BloomCacheSize</code></li>
</ul>
</li>
<li><strong>Hot key throttling</strong>
<ul>
<li><code>WriteHotKeyLimit</code>, <code>HotWriteBurstThreshold</code>, <code>HotWriteBatchMultiplier</code></li>
<li><code>HotRingEnabled</code>, <code>HotRingTopK</code>, decay/window settings</li>
</ul>
</li>
<li><strong>WAL watchdog</strong>
<ul>
<li><code>EnableWALWatchdog</code>, <code>WALAutoGCInterval</code></li>
<li><code>WALAutoGCMinRemovable</code>, <code>WALAutoGCMaxBatch</code></li>
<li><code>WALTypedRecordWarnRatio</code>, <code>WALTypedRecordWarnSegments</code></li>
</ul>
</li>
<li><strong>Raft lag warnings (stats only)</strong>
<ul>
<li><code>RaftLagWarnSegments</code></li>
</ul>
</li>
</ul>
<p>Example:</p>
<pre><code class="language-go">opt := NoKV.NewDefaultOptions()
opt.WorkDir = "./data"
opt.SyncWrites = true
opt.ValueThreshold = 1024
opt.WriteBatchMaxCount = 128
db := NoKV.Open(opt)
defer db.Close()
</code></pre>
<hr>
<h2 id="2-raft-topology-file"><a class="header" href="#2-raft-topology-file">2. Raft Topology File</a></h2>
<p><code>raft_config.example.json</code> is the single source of truth for distributed
topology. It is consumed by scripts, <code>cmd/nokv-redis</code>, and the <code>config</code> package.</p>
<p>Minimal shape:</p>
<pre><code class="language-jsonc">{
  "max_retries": 8,
  "tso": { "listen_addr": "127.0.0.1:9494", "advertise_url": "http://127.0.0.1:9494" },
  "stores": [
    { "store_id": 1, "listen_addr": "127.0.0.1:20170", "addr": "127.0.0.1:20170" }
  ],
  "regions": [
    {
      "id": 1,
      "start_key": "-",
      "end_key": "-",
      "epoch": { "version": 1, "conf_version": 1 },
      "peers": [{ "store_id": 1, "peer_id": 101 }],
      "leader_store_id": 1
    }
  ]
}
</code></pre>
<p>Notes:</p>
<ul>
<li><code>start_key</code> / <code>end_key</code> accept plain strings, <code>hex:&lt;bytes&gt;</code>, or base64. Use
<code>"-"</code> or empty for unbounded ranges.</li>
<li><code>stores</code> define both host and docker addresses for local runs vs containers.</li>
<li><code>leader_store_id</code> is optional; clients use it for initial routing hints.</li>
</ul>
<p>Programmatic loading:</p>
<pre><code class="language-go">cfg, _ := config.LoadFile("raft_config.example.json")
if err := cfg.Validate(); err != nil { /* handle */ }
</code></pre>
<p>Related tools:</p>
<ul>
<li><code>scripts/run_local_cluster.sh --config raft_config.example.json</code></li>
<li><code>go run ./cmd/nokv-redis --raft-config raft_config.example.json</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cli-cmdnokv-reference"><a class="header" href="#cli-cmdnokv-reference">CLI (<code>cmd/nokv</code>) Reference</a></h1>
<p>The <code>nokv</code> command provides operational visibility similar to RocksDBâ€™s <code>ldb</code> and Badgerâ€™s <code>badger</code> CLI, but emits JSON to integrate easily with scripts and CI pipelines.</p>
<hr>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<pre><code class="language-bash">go install ./cmd/nokv
</code></pre>
<p>Use <code>GOBIN</code> if you prefer a custom binary directory.</p>
<hr>
<h2 id="shared-flags"><a class="header" href="#shared-flags">Shared Flags</a></h2>
<ul>
<li><code>--workdir &lt;path&gt;</code> â€“ location of the NoKV database (must contain <code>CURRENT</code>).</li>
<li><code>--json</code> â€“ emit structured JSON (default is human-readable tables).</li>
<li><code>--expvar &lt;url&gt;</code> â€“ for <code>stats</code> command, pull metrics from a running process exposing <code>expvar</code>.</li>
<li><code>--no-region-metrics</code> â€“ for <code>stats</code> offline mode; skip attaching <code>RegionMetrics</code> and report manifest-only figures.</li>
</ul>
<hr>
<h2 id="subcommands"><a class="header" href="#subcommands">Subcommands</a></h2>
<h3 id="nokv-stats"><a class="header" href="#nokv-stats"><code>nokv stats</code></a></h3>
<ul>
<li>Reads <code>StatsSnapshot</code> either offline (<code>--workdir</code>) or via HTTP (<code>--expvar</code>).</li>
<li>Output fields include:
<ul>
<li><code>flush_queue_length</code>, <code>flush_wait_ms</code>, <code>flush_build_ms</code></li>
<li><code>compaction_backlog</code>, <code>wal_active_segment</code>, <code>wal_segments_removed</code></li>
<li><code>vlog_head</code>, <code>vlog_segments</code>, <code>vlog_pending_deletes</code>, <code>vlog_discard_queue</code></li>
<li><code>txns_active</code>, <code>txns_committed</code>, <code>txns_conflicts</code></li>
<li><code>region_total</code> (plus <code>region_new</code>, <code>region_running</code>, <code>region_removing</code>, <code>region_tombstone</code>, <code>region_other</code>)</li>
<li><code>hot_keys</code> (Top-N hits captured by <code>hotring</code>)</li>
</ul>
</li>
<li>Example:</li>
</ul>
<pre><code class="language-bash">nokv stats --workdir ./testdata/db --json | jq '.flush_queue_length'
</code></pre>
<h3 id="nokv-manifest"><a class="header" href="#nokv-manifest"><code>nokv manifest</code></a></h3>
<ul>
<li>Parses the manifest using <code>manifest.Manager.Version()</code>.</li>
<li>Reports per-level file counts, smallest/largest keys, WAL checkpoint, and ValueLog metadata.</li>
<li>Helpful for verifying flush/compaction results and ensuring manifest rewrites succeeded.</li>
</ul>
<h3 id="nokv-vlog"><a class="header" href="#nokv-vlog"><code>nokv vlog</code></a></h3>
<ul>
<li>Lists vlog segments with status flags (<code>active</code>, <code>candidate_for_gc</code>, <code>deleted</code>).</li>
<li>Shows head file/offset and pending GC actions.</li>
<li>Use after running GC or recovery to confirm stale segments are purged.</li>
</ul>
<hr>
<h2 id="integration-tips"><a class="header" href="#integration-tips">Integration Tips</a></h2>
<ul>
<li>Combine with <code>RECOVERY_TRACE_METRICS=1</code> to cross-check logs: run tests, then inspect CLI output to ensure metrics match expectations.</li>
<li>In CI, capture JSON output and diff against golden files to detect regressions (see <code>cmd/nokv/main_test.go</code>).</li>
<li>When comparing against RocksDB/Badger, treat <code>nokv manifest</code> + <code>nokv vlog</code> as equivalents to <code>ldb manifest_dump</code> and Badgerâ€™s <code>badger</code> <code>inspect vlog</code> commands.</li>
</ul>
<hr>
<p>For architecture context, see <a href="#nokv-architecture-overview">architecture.md</a> and the module deep dives.</p>
<ul>
<li><strong><code>nokv regions</code></strong> â€“ Dumps the manifest-backed Region catalog (ID/state/key range/peers). Supports <code>--json</code> for automation and complements the Region metrics shown in <code>nokv stats</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memtable-design--lifecycle"><a class="header" href="#memtable-design--lifecycle">Memtable Design &amp; Lifecycle</a></h1>
<p>NoKVâ€™s write path mirrors RocksDB: every write lands in the <strong>WAL</strong> and an in-memory <strong>memtable</strong> backed by a selectable in-memory index (skiplist or ART). The implementation lives in <a href="../lsm/memtable.go"><code>lsm/memtable.go</code></a> and ties directly into the flush manager (<code>lsm/flush</code>).</p>
<hr>
<h2 id="1-structure"><a class="header" href="#1-structure">1. Structure</a></h2>
<pre><code class="language-go">type memTable struct {
    lsm        *LSM
    segmentID  uint32       // WAL segment backing this memtable
    index      memIndex
    maxVersion uint64
    walSize    int64
}
</code></pre>
<p>The memtable index is an interface that can be backed by either a skiplist or ART:</p>
<pre><code class="language-go">type memIndex interface {
    Add(*kv.Entry)
    Search([]byte) kv.ValueStruct
    NewIterator(*utils.Options) utils.Iterator
    MemSize() int64
    IncrRef()
    DecrRef()
}
</code></pre>
<ul>
<li><strong>Memtable engine</strong> â€“ <code>Options.MemTableEngine</code> selects <code>skiplist</code> (default) or <code>art</code> via <code>newMemIndex</code>. Skiplist favors simpler writes; ART favors tighter memory and ordered scans.</li>
<li><strong>Arena sizing</strong> â€“ <code>utils.NewSkiplist</code> uses <code>arenaSizeFor</code>; <code>utils.NewART</code> uses <code>arenaSizeForART</code> to reserve more space for variable node payloads and prefix spills.</li>
<li><strong>WAL coupling</strong> â€“ every <code>Set</code> uses <code>kv.EncodeEntry</code> to materialise the payload to the active WAL segment before inserting into the chosen index. <code>walSize</code> tracks how much of the segment is consumed so flush can release it later.</li>
<li><strong>Segment ID</strong> â€“ <code>LSM.NewMemtable</code> atomically increments <code>levels.maxFID</code>, switches the WAL to a new segment (<code>wal.Manager.SwitchSegment</code>), and tags the memtable with that FID. This matches RocksDBâ€™s <code>logfile_number</code> field.</li>
<li><strong>ART specifics</strong> â€“ ART stores prefix-compressed inner nodes (Node4/16/48/256), uses optimistic version checks for reads with localized locks for writes, and iterators walk the tree in key order.</li>
</ul>
<hr>
<h2 id="2-lifecycle"><a class="header" href="#2-lifecycle">2. Lifecycle</a></h2>
<pre class="mermaid">sequenceDiagram
    participant WAL
    participant MT as MemTable
    participant Flush
    participant Manifest
    WAL-&gt;&gt;MT: Append+Set(entry)
    MT-&gt;&gt;Flush: freeze (Size() &gt;= limit)
    Flush-&gt;&gt;Manifest: LogPointer + AddFile
    Manifest--&gt;&gt;Flush: ack
    Flush-&gt;&gt;WAL: Release segments â‰¤ segmentID
</pre>

<ol>
<li><strong>Active â†’ Immutable</strong> â€“ when <code>mt.Size()</code> crosses thresholds (<code>Options.MemTableSize</code>), the memtable is swapped out and pushed onto the flush queue. The new active memtable triggers another WAL segment switch.</li>
<li><strong>Flush</strong> â€“ the flush manager drains immutable memtables, builds SSTables, logs manifest edits, and releases the WAL segment ID recorded in <code>memTable.segmentID</code> once the SST is durably installed.</li>
<li><strong>Recovery</strong> â€“ <code>LSM.recovery</code> scans WAL files, reopens memtables per segment (most recent becomes active), and deletes segments â‰¤ the manifestâ€™s log pointer. Entries are replayed via <code>wal.Manager.ReplaySegment</code> into fresh indexes, rebuilding <code>maxVersion</code> for the oracle.</li>
</ol>
<p>Badger follows the same pattern, while RocksDB often uses skiplist-backed arenas with reference countingâ€”NoKV reuses Badgerâ€™s arena allocator for simplicity.</p>
<hr>
<h2 id="3-read-semantics"><a class="header" href="#3-read-semantics">3. Read Semantics</a></h2>
<ul>
<li><code>memTable.Get</code> looks up the chosen index and returns a copy of the entry. MVCC versions stay encoded in the key suffix (<code>KeyWithTs</code>), so iterators naturally merge across memtables and SSTables.</li>
<li><code>MemTable.IncrRef/DecrRef</code> delegate to the index, allowing iterators to hold references while the flush manager processes immutable tablesâ€”mirroring RocksDBâ€™s <code>MemTable::Ref/Unref</code> lifecycle.</li>
<li>WAL-backed values that exceed the value threshold are stored as pointers; the memtable stores the encoded pointer, and the transaction/iterator logic reads from the vlog on demand.</li>
</ul>
<hr>
<h2 id="4-integration-with-other-subsystems"><a class="header" href="#4-integration-with-other-subsystems">4. Integration with Other Subsystems</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Subsystem</th><th>Interaction</th></tr>
</thead>
<tbody>
<tr><td>Transactions</td><td><code>Txn.commitAndSend</code> writes entries into the active memtable after WAL append; pending writes bypass the memtable until commit so per-txn isolation is preserved.</td></tr>
<tr><td>Manifest</td><td>Flush completion logs <code>EditLogPointer(segmentID)</code> so restart can discard WAL files already persisted into SSTs.</td></tr>
<tr><td>Stats</td><td><code>Stats.Snapshot</code> pulls <code>FlushPending/Active/Queue</code> counters via <a href="../lsm/lsm.go#L120-L128"><code>lsm.FlushMetrics</code></a>, exposing how many immutables are waiting.</td></tr>
<tr><td>Value Log</td><td><code>lsm.flush</code> emits discard stats keyed by <code>segmentID</code>, letting the value log GC know when entries become obsolete.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="5-comparison"><a class="header" href="#5-comparison">5. Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>RocksDB</th><th>BadgerDB</th><th>NoKV</th></tr>
</thead>
<tbody>
<tr><td>Data structure</td><td>Skiplist + arena</td><td>Skiplist + arena</td><td>Skiplist or ART + arena</td></tr>
<tr><td>WAL linkage</td><td><code>logfile_number</code> per memtable</td><td>Segment ID stored in vlog entries</td><td><code>segmentID</code> on <code>memTable</code>, logged via manifest</td></tr>
<tr><td>Recovery</td><td>Memtable replays from WAL, referencing <code>MANIFEST</code></td><td>Replays WAL segments</td><td>Replays WAL segments, prunes â‰¤ manifest log pointer</td></tr>
<tr><td>Flush trigger</td><td>Size/entries/time</td><td>Size-based</td><td>Size-based with explicit queue metrics</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="6-operational-notes"><a class="header" href="#6-operational-notes">6. Operational Notes</a></h2>
<ul>
<li>Tuning <code>Options.MemTableSize</code> affects WAL segment count and flush latency. Larger memtables reduce flush churn but increase crash recovery time.</li>
<li>Monitor <code>NoKV.Stats.Flush.*</code> metrics to catch stalled immutablesâ€”an ever-growing queue often indicates slow SST builds or manifest contention.</li>
<li>Because memtables carry WAL segment IDs, deleting WAL files manually can lead to recovery failures; always rely on the engineâ€™s manifest-driven cleanup.</li>
</ul>
<p>See <a href="#memtable-flush-pipeline"><code>docs/flush.md</code></a> for the end-to-end flush scheduler and <code>[docs/architecture.md](architecture.md#3-end-to-end-write-flow)</code> for where memtables sit in the write pipeline.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="memtable-flush-pipeline"><a class="header" href="#memtable-flush-pipeline">MemTable Flush Pipeline</a></h1>
<p>NoKVâ€™s flush subsystem translates immutable memtables into persisted SSTables while coordinating WAL checkpoints and ValueLog discard statistics. The code lives in <a href="../lsm/flush"><code>lsm/flush</code></a> and is tightly integrated with <code>DB.doWrites</code> and <code>manifest.Manager</code>.</p>
<hr>
<h2 id="1-responsibilities"><a class="header" href="#1-responsibilities">1. Responsibilities</a></h2>
<ol>
<li><strong>Reliability</strong> â€“ ensure immutables become SSTables atomically, and failures are recoverable.</li>
<li><strong>Coordination</strong> â€“ release WAL segments only after manifest commits, and feed discard stats to ValueLog GC.</li>
<li><strong>Observability</strong> â€“ expose queue depth, stage durations, and task counts through <code>Stats.collect</code> and the CLI.</li>
</ol>
<p>Compared with RocksDB: the stage transitions mirror RocksDBâ€™s flush job lifecycle (<code>PickMemTable</code>, <code>WriteLevel0Table</code>, <code>InstallMemTable</code>), while the discard stats channel is inspired by Badgerâ€™s integration with vlog GC.</p>
<hr>
<h2 id="2-stage-machine"><a class="header" href="#2-stage-machine">2. Stage Machine</a></h2>
<pre class="mermaid">flowchart LR
    Active[Active MemTable]
    Immutable[Immutable MemTable]
    FlushQ[flush.Manager queue]
    Build[StageBuild]
    Install[StageInstall]
    Release[StageRelease]

    Active --&gt;|threshold reached| Immutable --&gt; FlushQ
    FlushQ --&gt; Build --&gt; Install --&gt; Release --&gt; Active
</pre>

<ul>
<li><strong>StagePrepare</strong> â€“ <code>Manager.Submit</code> assigns a task ID, records enqueue time, and bumps queue metrics.</li>
<li><strong>StageBuild</strong> â€“ <code>Manager.Next</code> hands tasks to background workers. <code>buildTable</code> serialises data into a temporary <code>.sst.tmp</code> using <code>lsm/builder.go</code>.</li>
<li><strong>StageInstall</strong> â€“ manifest edits (<code>EditAddFile</code>, <code>EditLogPointer</code>) are logged. Only on success is the temp file renamed and the WAL checkpoint advanced.</li>
<li><strong>StageRelease</strong> â€“ metrics record release duration, discard stats are flushed to <code>valueLog.lfDiscardStats</code>, and <code>wal.Manager.Remove</code> drops obsolete segments.</li>
</ul>
<p><code>Manager.Update</code> transitions between stages and collects timing data (<code>WaitNs</code>, <code>BuildNs</code>, <code>ReleaseNs</code>). These appear as <code>NoKV.Flush.Queue</code>, <code>NoKV.Flush.BuildAvgMs</code>, etc., in CLI output.</p>
<hr>
<h2 id="3-key-types"><a class="header" href="#3-key-types">3. Key Types</a></h2>
<pre><code class="language-go">type Task struct {
    ID        uint64
    SegmentID uint32
    Stage     Stage
    Data      any      // memtable pointer, temp file info, etc.
    Err       error
}

type Manager struct {
    queue []*Task
    active map[uint64]*Task
    cond  *sync.Cond
    // atomic metrics fields (pending, queueLen, waitNs...)
}
</code></pre>
<ul>
<li><code>Stage</code> enumerates <code>StagePrepare</code>, <code>StageBuild</code>, <code>StageInstall</code>, <code>StageRelease</code>.</li>
<li><code>Metrics</code> aggregates pending/active counts and nanosecond accumulators; the CLI converts them to human-friendly durations.</li>
<li>The queue uses condition variables to coordinate between background workers and producers; the design avoids busy waiting, unlike some RocksDB flush queues.</li>
</ul>
<hr>
<h2 id="4-execution-path-in-code"><a class="header" href="#4-execution-path-in-code">4. Execution Path in Code</a></h2>
<ol>
<li><code>DB.applyBatches</code> detects when the active memtable is full and hands it to <code>lsm.LSM.scheduleFlush</code>, which calls <code>flush.Manager.Submit</code>.</li>
<li>Background goroutines call <code>Next</code> to retrieve tasks; <code>lsm.(*LSM).runFlushMemTable</code> performs the build and install phases.</li>
<li><code>lsm.(*LSM).installLevel0Table</code> writes the manifest edit and renames the SST (atomic <code>os.Rename</code>, same as RocksDBâ€™s flush job).</li>
<li>After install, <code>valueLog.updateDiscardStats</code> is called so GC can reclaim vlog entries belonging to dropped keys.</li>
<li>Once release completes, <code>wal.Manager.Remove</code> evicts segments whose entries are fully represented in SSTs, matching RocksDBâ€™s <code>LogFileManager::PurgeObsoleteLogs</code>.</li>
</ol>
<hr>
<h2 id="5-recovery-considerations"><a class="header" href="#5-recovery-considerations">5. Recovery Considerations</a></h2>
<ul>
<li><strong>Before Install</strong> â€“ temp files remain in <code>tmp/</code>. On restart, no manifest entry exists, so <code>lsm.LSM.replayManifest</code> ignores them and the memtable is rebuilt from WAL.</li>
<li><strong>After Install but before Release</strong> â€“ manifest records the SST while WAL segments may still exist. Recovery sees the edit, ensures the file exists, and release metrics resume from StageRelease.</li>
<li><strong>Metrics</strong> â€“ because timing data is stored atomically in the manager, recovery resets counters but does not prevent the CLI from reporting backlog immediately after restart.</li>
</ul>
<p>RocksDB uses flush job logs; NoKV reuses metrics and CLI output for similar visibility.</p>
<hr>
<h2 id="6-observability--cli"><a class="header" href="#6-observability--cli">6. Observability &amp; CLI</a></h2>
<ul>
<li><code>StatsSnapshot.Flush.Queue</code> â€“ number of pending tasks.</li>
<li><code>StatsSnapshot.Flush.WaitMs</code> â€“ average wait time before build.</li>
<li><code>StatsSnapshot.Flush.BuildMs</code> â€“ average build duration.</li>
<li><code>StatsSnapshot.Flush.Completed</code> â€“ cumulative tasks finished.</li>
</ul>
<p>The CLI command <code>nokv stats --workdir &lt;dir&gt;</code> prints these metrics alongside compaction and transaction statistics, enabling operators to detect stalled flush workers or WAL backlog quickly.</p>
<hr>
<h2 id="7-interplay-with-valuelog-gc"><a class="header" href="#7-interplay-with-valuelog-gc">7. Interplay with ValueLog GC</a></h2>
<p>Flush completion sends discard stats via <code>db.lsm.SetDiscardStatsCh(&amp;(db.vlog.lfDiscardStats.flushChan))</code>. ValueLog GC uses this feed to determine how much of each vlog segment is obsolete, similar to Badgerâ€™s discard ratio heuristic. Without flush-driven stats, vlog GC would have to rescan SSTables, so this channel is crucial for keeping GC cheap.</p>
<hr>
<h2 id="8-testing-matrix"><a class="header" href="#8-testing-matrix">8. Testing Matrix</a></h2>
<ul>
<li><code>lsm/flush/manager_test.go</code> (implicit via <code>lsm/lsm_test.go</code>) validates stage transitions and metrics.</li>
<li><code>db_recovery_test.go</code> covers crash scenarios before/after install, ensuring WAL replay plus manifest reconciliation recovers gracefully.</li>
<li>Future additions: inject write failures during <code>StageBuild</code> to test retry logic, analogous to RocksDBâ€™s simulated IO errors.</li>
</ul>
<p>See the <a href="#crash-recovery-playbook">recovery plan</a> and <a href="#testing--validation-matrix">testing matrix</a> for more context.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="compaction--cache-strategy"><a class="header" href="#compaction--cache-strategy">Compaction &amp; Cache Strategy</a></h1>
<blockquote>
<p>NoKVâ€™s compaction pipeline borrows the leveledâ€‘LSM layout from RocksDB, but layers it with an ingest buffer, lightweight cache telemetry, and simple concurrency guards so the implementation stays approachable while still handling bursty workloads.</p>
</blockquote>
<hr>
<h2 id="1-overview"><a class="header" href="#1-overview">1. Overview</a></h2>
<p>Compactions are orchestrated by <code>compact.Manager</code> with <code>lsm.levelManager</code> implementing the executor hooks. Each level owns two lists of tables:</p>
<ul>
<li><code>tables</code> â€“ the canonical sorted run for the level.</li>
<li><code>ingest</code> â€“ a staging buffer that temporarily holds SSTables moved from the level above when there is not yet enough work (or bandwidth) to do a full merge.</li>
</ul>
<p>The compaction manager periodically calls into the executor to build a list of <code>compact.Priority</code> entries.  The priorities consider three signals:</p>
<ol>
<li><strong>L0 table count</strong> â€“ loosely capped by <code>Options.NumLevelZeroTables</code>.</li>
<li><strong>Level size vs target</strong> â€“ computed by <code>levelTargets()</code>, which dynamically adjusts the â€œbaseâ€ level depending on total data volume.</li>
<li><strong>Ingest buffer backlog</strong> â€“ if a levelâ€™s <code>ingest</code> shards have data, they receive elevated scores so staged tables are merged promptly.</li>
</ol>
<p>The highest adjusted score is processed first.  L0 compactions can either move tables into the ingest buffer of the base level (cheap reâ€‘parenting) or compact directly into a lower level when the overlap warrants it.</p>
<p>Planning now happens via <code>compact.Plan</code>: LSM snapshots table metadata into <code>compact.TableMeta</code>, <code>compact.PlanFor*</code> selects table IDs + key ranges, and LSM resolves the plan back to <code>*table</code> before executing.</p>
<hr>
<h2 id="2-ingest-buffer"><a class="header" href="#2-ingest-buffer">2. Ingest Buffer</a></h2>
<p><code>moveToIngest</code> (see <code>lsm/executor.go</code>) performs a metadata-only migration:</p>
<ol>
<li>Records a <code>manifest.EditDeleteFile</code> for the source level.</li>
<li>Logs a new <code>manifest.EditAddFile</code> targeting the destination level.</li>
<li>Removes the table from <code>thisLevel.tables</code> and appends it to <code>nextLevel.ingest</code>.</li>
</ol>
<p>This keeps write amplification low when many small L0 tables arrive at once.  Reads still see the newest data because <code>levelHandler.searchIngestSST</code> checks <code>ingest</code> before consulting <code>tables</code>.</p>
<p>Compaction tests (<code>lsm/compaction_cache_test.go</code>) now assert that after calling <code>moveToIngest</code> the table disappears from the source level and shows up in the ingest buffer.</p>
<hr>
<h2 id="3-concurrency-guards"><a class="header" href="#3-concurrency-guards">3. Concurrency Guards</a></h2>
<p>To prevent overlapping compactions:</p>
<ul>
<li><code>compact.State.CompareAndAdd</code> tracks the key range of each in-flight compaction per level.</li>
<li>Attempts to register a compaction whose ranges intersect an existing one are rejected.</li>
<li>When a compaction finishes, <code>compact.State.Delete</code> removes the ranges and table IDs from the guard.</li>
</ul>
<p>This mechanism is intentionally simpleâ€”just a mutexâ€protected sliceâ€”yet effective in tests (<code>TestCompactStatusGuards</code>) that simulate backâ€‘toâ€‘back registrations on the same key range.</p>
<hr>
<h2 id="4-cache-telemetry"><a class="header" href="#4-cache-telemetry">4. Cache Telemetry</a></h2>
<p>NoKVâ€™s cache is split into three parts (<code>lsm/cache.go</code>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Purpose</th><th>Metrics hook</th></tr>
</thead>
<tbody>
<tr><td>Block cache (hot)</td><td>LRU list capturing most recent hits (typically L0/L1).</td><td><code>cacheMetrics.recordBlock(level, hit)</code></td></tr>
<tr><td>Block cache (cold)</td><td>CLOCK cache for deeper levels, keeping the memory footprint bounded.</td><td>Same as above</td></tr>
<tr><td>Bloom cache</td><td>Stores decoded bloom filters to reduce disk touches.</td><td><code>recordBloom(hit)</code></td></tr>
</tbody>
</table>
</div>
<p><code>CacheMetrics()</code> on <code>DB</code> surfaces hits/misses per layer, which is especially helpful when tuning ingest behaviourâ€”if L0/L1 cache misses spike, the ingest buffer likely needs to be drained faster.  <code>TestCacheHotColdMetrics</code> verifies that the hot and cold tiers tick the counters as expected.</p>
<hr>
<h2 id="5-interaction-with-value-log"><a class="header" href="#5-interaction-with-value-log">5. Interaction with Value Log</a></h2>
<p>Compaction informs valueâ€‘log GC via discard statistics:</p>
<ol>
<li>During <code>subcompact</code>, every entry merged out is inspected.  If it stores a <code>ValuePtr</code>, the amount is added to the discard map.</li>
<li>At the end of subcompaction, the accumulated discard map is pushed through <code>setDiscardStatsCh</code>.</li>
<li><code>valueLog</code> receives the stats and can safely rewrite or delete vlog segments with predominantly obsolete data.</li>
</ol>
<p>This tight coupling keeps the value log from growing indefinitely after heavy overwrite workloads.</p>
<hr>
<h2 id="6-testing-checklist"><a class="header" href="#6-testing-checklist">6. Testing Checklist</a></h2>
<p>Relevant tests to keep compaction healthy:</p>
<ul>
<li><code>lsm/compaction_cache_test.go</code>
<ul>
<li><code>TestCompactionMoveToIngest</code> â€“ ensures metadata migration works and the ingest buffer grows.</li>
<li><code>TestCacheHotColdMetrics</code> â€“ validates cache hit accounting.</li>
<li><code>TestCompactStatusGuards</code> â€“ checks overlap detection.</li>
</ul>
</li>
<li><code>lsm/lsm_test.go</code>
<ul>
<li><code>TestCompact</code> / <code>TestHitStorage</code> â€“ endâ€‘toâ€‘end verification that data remains queryable across memtable flushes and compactions.</li>
</ul>
</li>
</ul>
<p>When adding new compaction heuristics or cache tiers, extend these tests (or introduce new ones) so the behaviour stays observable.</p>
<hr>
<h2 id="7-practical-tips"><a class="header" href="#7-practical-tips">7. Practical Tips</a></h2>
<ul>
<li>Tune <code>Options.IngestCompactBatchSize</code> when ingest queues build up; increasing it lets a single move cover more tables.</li>
<li>Observe <code>DB.CacheMetrics()</code> and <code>DB.CompactionStats()</code> via the CLI (<code>nokv stats</code>) to decide whether you need more compaction workers or bigger caches.</li>
<li>For workloads dominated by range scans, consider increasing <code>Options.BlockCacheSize</code> if you want to keep more L0/L1 blocks in the user-space cache; cold dataä¾èµ– OS page cacheã€‚</li>
<li>Keep an eye on <code>NoKV.ValueLog.GcRuns</code> and <code>ValueLogHeadUpdates</code>; if compactions are generating discard stats but the value log head doesnâ€™t move, GC thresholds may be too conservative.</li>
</ul>
<p>With these mechanisms, NoKV stays resilient under bursty writes while keeping the code path small and discoverableâ€”ideal for learning or embedding.  Dive into the source files referenced above for deeper implementation details.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="ingest-buffer-architecture"><a class="header" href="#ingest-buffer-architecture">Ingest Buffer Architecture</a></h1>
<p>The ingest buffer is a per-level staging area for SSTablesâ€”typically promoted from L0â€”designed to <strong>absorb bursts, reduce overlap, and unlock parallel compaction</strong> without touching the main level tables immediately. It combines fixed sharding, adaptive scheduling, and optional <code>IngestKeep</code> (ingest-merge) passes to keep write amplification and contention low.</p>
<pre class="mermaid">flowchart LR
  L0["L0 SSTables"] --&gt;|moveToIngest| Ingest["Ingest Buffer (sharded)"]
  subgraph levelN["Level N"]
    Ingest --&gt;|IngestDrain: ingest-only| MainTables["Main Tables"]
    Ingest --&gt;|IngestKeep: ingest-merge| Ingest
  end
  Ingest -.read path merge.-&gt; ClientReads["Reads/Iterators"]
</pre>

<h2 id="design-highlights"><a class="header" href="#design-highlights">Design Highlights</a></h2>
<ul>
<li><strong>Sharded by key prefix</strong>: ingest tables are routed into fixed shards (top bits of the first byte). Sharding cuts cross-range overlap and enables safe parallel drain.</li>
<li><strong>Snapshot-friendly reads</strong>: ingest tables are read under the level <code>RLock</code>, and iterators hold table refs so mmap-backed data stays valid without additional snapshots.</li>
<li><strong>Two ingest paths</strong>:
<ul>
<li><em>Ingest-only compaction</em>: drain ingest â†’ main level (or next level) with optional multi-shard parallelism guarded by <code>compact.State</code>.</li>
<li><em>Ingest-merge</em>: compact ingest tables back into ingest (stay in-place) to drop superseded versions before promoting, reducing downstream write amplification.</li>
</ul>
</li>
<li><strong>IngestMode enum</strong>: plans carry an <code>IngestMode</code> with <code>IngestNone</code>, <code>IngestDrain</code>, and <code>IngestKeep</code>. <code>IngestDrain</code> corresponds to ingest-only (drain into main tables), while <code>IngestKeep</code> corresponds to ingest-merge (compact within ingest).</li>
<li><strong>Adaptive scheduling</strong>:
<ul>
<li>Shard selection is driven by <code>compact.PickShardOrder</code> / <code>compact.PickShardByBacklog</code> using per-shard size, age, and density.</li>
<li>Shard parallelism scales with backlog score (based on shard size/target file size) bounded by <code>IngestShardParallelism</code>.</li>
<li>Batch size scales with shard backlog to drain faster under pressure.</li>
<li>Ingest-merge triggers when backlog score exceeds <code>IngestBacklogMergeScore</code> (default 2.0), with dynamic lowering under extreme backlog/age.</li>
</ul>
</li>
<li><strong>Observability</strong>: expvar/stats expose <code>IngestDrain</code> vs <code>IngestKeep</code> counts, duration, and tables processed, plus ingest size/value density per level/shard.</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<ul>
<li><code>IngestShardParallelism</code>: max shards to compact in parallel (default <code>max(NumCompactors/2, 2)</code>, auto-scaled by backlog).</li>
<li><code>IngestCompactBatchSize</code>: base batch size per ingest compaction (auto-boosted by shard backlog).</li>
<li><code>IngestBacklogMergeScore</code>: backlog score threshold to trigger <code>IngestKeep</code>/ingest-merge (default 2.0).</li>
</ul>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<ul>
<li><strong>Lower write amplification</strong>: bursty L0 SSTables land in ingest first; <code>IngestKeep</code>/ingest-merge prunes duplicates before full compaction.</li>
<li><strong>Reduced contention</strong>: sharding + <code>compact.State</code> allow parallel ingest drain with minimal overlap.</li>
<li><strong>Predictable reads</strong>: ingest is part of the read snapshot, so moving tables in/out does not change read semantics.</li>
<li><strong>Tunable and observable</strong>: knobs for parallelism and merge aggressiveness, with per-path metrics to guide tuning.</li>
</ul>
<h2 id="future-work"><a class="header" href="#future-work">Future Work</a></h2>
<ul>
<li>Deeper adaptive policies (IO/latency-aware), richer shard-level metrics, and more exhaustive parallel/restart testing under fault injection.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="wal-subsystem"><a class="header" href="#wal-subsystem">WAL Subsystem</a></h1>
<p>NoKVâ€™s write-ahead log mirrors RocksDBâ€™s durability model and is implemented as a compact Go module similar to Badgerâ€™s journal. WAL appends happen alongside memtable writes (via <code>lsm.Set</code>), while values that are routed to the value log are written <em>before</em> the WAL so that replay always sees durable value pointers.</p>
<hr>
<h2 id="1-file-layout--naming"><a class="header" href="#1-file-layout--naming">1. File Layout &amp; Naming</a></h2>
<ul>
<li>Location: <code>${Options.WorkDir}/wal/</code>.</li>
<li>Naming pattern: <code>%05d.wal</code> (e.g. <code>00001.wal</code>).</li>
<li>Rotation threshold: configurable via <code>wal.Config.SegmentSize</code> (defaults to 64 MiB, minimum 64 KiB).</li>
<li>Verification: <code>wal.VerifyDir</code> ensures the directory exists prior to <code>DB.Open</code>.</li>
</ul>
<p>On open, <code>wal.Manager</code> scans the directory, sorts segment IDs, and resumes the highest IDâ€”exactly how RocksDB re-opens its MANIFEST and WAL sequence files.</p>
<hr>
<h2 id="2-record-format"><a class="header" href="#2-record-format">2. Record Format</a></h2>
<pre><code class="language-text">uint32 length (big-endian, includes type + payload)
uint8  type
[]byte payload
uint32 checksum (CRC32 Castagnoli over type + payload)
</code></pre>
<ul>
<li>Checksums use <code>kv.CastagnoliCrcTable</code>, the same polynomial used by RocksDB (Castagnoli). Record encoding/decoding lives in <code>wal/record.go</code>.</li>
<li>The type byte allows mixing LSM mutations with raft log/state/snapshot records in the same WAL segment.</li>
<li>Appends are buffered by <code>bufio.Writer</code> so batches become single system calls.</li>
<li>Replay stops cleanly at truncated tails; tests simulate torn writes by truncating the final bytes and verifying replay remains idempotent (<code>wal/manager_test.go::TestReplayTruncatedTail</code>).</li>
</ul>
<hr>
<h2 id="3-public-api-go"><a class="header" href="#3-public-api-go">3. Public API (Go)</a></h2>
<pre><code class="language-go">mgr, _ := wal.Open(wal.Config{Dir: path})
infos, _ := mgr.Append(batchPayload)
_ = mgr.Sync()
_ = mgr.Rotate()
_ = mgr.Replay(func(info wal.EntryInfo, payload []byte) error {
    // reapply to memtable
    return nil
})
</code></pre>
<p>Key behaviours:</p>
<ul>
<li><code>Append</code> automatically calls <code>ensureCapacity</code> to decide when to rotate; it returns <code>EntryInfo{SegmentID, Offset, Length}</code> for each payload so higher layers can build value pointers or manifest checkpoints.</li>
<li><code>Sync</code> flushes the active file (used for <code>Options.SyncWrites</code>).</li>
<li><code>Rotate</code> forces a new segment (used after flush/compaction checkpoints similar to RocksDBâ€™s <code>LogFileManager::SwitchLog</code>).</li>
<li><code>Replay</code> iterates segments in numeric order, forwarding each payload to the callback. Errors abort replay so recovery can surface corruption early.</li>
<li>Metrics (<code>wal.Manager.Metrics</code>) reveal the active segment ID, total segments, and number of removed segmentsâ€”these feed directly into <code>StatsSnapshot</code> and <code>nokv stats</code> output.</li>
</ul>
<p>Compared with Badger: Badger keeps a single vlog for both data and durability. NoKV splits WAL (durability) from vlog (value separation), matching RocksDBâ€™s separation of WAL and blob files.</p>
<hr>
<h2 id="4-integration-points"><a class="header" href="#4-integration-points">4. Integration Points</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Call Site</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>lsm.memTable.set</code></td><td>Encodes each entry (<code>kv.EncodeEntry</code>) and appends to WAL before inserting into the skiplist.</td></tr>
<tr><td><code>DB.commitWorker</code></td><td>Commit worker applies batched writes via <code>writeToLSM</code>, which flows into <code>lsm.Set</code> and thus WAL.</td></tr>
<tr><td><code>DB.Set</code></td><td>Direct write path: calls <code>lsm.Set</code>, which appends to WAL and updates the memtable.</td></tr>
<tr><td><code>manifest.Manager.LogEdit</code></td><td>Uses <code>EntryInfo.SegmentID</code> to persist the WAL checkpoint (<code>EditLogPointer</code>). This acts as the <code>log number</code> seen in RocksDB manifest entries.</td></tr>
<tr><td><code>lsm/flush.Manager.Update</code></td><td>Once an SST is installed, WAL segments older than the checkpoint are released (<code>wal.Manager.Remove</code>).</td></tr>
<tr><td><code>db.runRecoveryChecks</code></td><td>Ensures WAL directory invariants before manifest replay, similar to Badgerâ€™s directory bootstrap.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="5-metrics--observability"><a class="header" href="#5-metrics--observability">5. Metrics &amp; Observability</a></h2>
<p><code>Stats.collect</code> reads the manager metrics and exposes them as:</p>
<ul>
<li><code>NoKV.WAL.ActiveSegment</code></li>
<li><code>NoKV.WAL.SegmentCount</code></li>
<li><code>NoKV.WAL.RemovedSegments</code></li>
</ul>
<p>The CLI command <code>nokv stats --workdir &lt;dir&gt;</code> prints these alongside backlog, making WAL health visible without manual inspection. In high-throughput scenarios the active segment ID mirrors RocksDBâ€™s <code>LOG</code> number growth.</p>
<hr>
<h2 id="6-wal-watchdog-auto-gc"><a class="header" href="#6-wal-watchdog-auto-gc">6. WAL Watchdog (Auto GC)</a></h2>
<p>The WAL watchdog runs inside the DB process to keep WAL backlog in check and
surface warnings when raft-typed records dominate the log. It:</p>
<ul>
<li>Samples WAL metrics + per-segment metrics and combines them with
<code>manifest.RaftPointerSnapshot()</code> to compute removable segments.</li>
<li>Removes up to <code>WALAutoGCMaxBatch</code> segments when at least
<code>WALAutoGCMinRemovable</code> are eligible.</li>
<li>Exposes counters (<code>WALAutoGCRuns/Removed/LastUnix</code>) and warning state
(<code>WALTypedRecordRatio/Warning/Reason</code>) through <code>StatsSnapshot</code>.</li>
</ul>
<p>Relevant options (see <code>options.go</code> for defaults):</p>
<ul>
<li><code>EnableWALWatchdog</code></li>
<li><code>WALAutoGCInterval</code></li>
<li><code>WALAutoGCMinRemovable</code></li>
<li><code>WALAutoGCMaxBatch</code></li>
<li><code>WALTypedRecordWarnRatio</code></li>
<li><code>WALTypedRecordWarnSegments</code></li>
</ul>
<hr>
<h2 id="7-recovery-walkthrough"><a class="header" href="#7-recovery-walkthrough">7. Recovery Walkthrough</a></h2>
<ol>
<li><code>wal.Open</code> reopens the highest segment, leaving the file pointer at the end (<code>switchSegmentLocked</code>).</li>
<li><code>manifest.Manager</code> supplies the WAL checkpoint (segment + offset) while building the version. Replay skips entries up to this checkpoint, ensuring we only reapply writes not yet materialised in SSTables.</li>
<li><code>wal.Manager.Replay</code> (invoked by the LSM recovery path) rebuilds memtables from entries newer than the manifest checkpoint. Value-log recovery only validates/truncates segments and does not reapply data.</li>
<li>If the final record is partially written, the CRC mismatch stops replay and the segment is truncated during recovery tests, mimicking RocksDBâ€™s tolerant behaviour.</li>
</ol>
<hr>
<h2 id="8-operational-tips"><a class="header" href="#8-operational-tips">8. Operational Tips</a></h2>
<ul>
<li>Configure <code>SyncOnWrite</code> for synchronous durability (default async like RocksDBâ€™s default). For latency-sensitive deployments, consider enabling to emulate Badgerâ€™s <code>SyncWrites</code>.</li>
<li>After large flushes, forcing <code>Rotate</code> keeps WAL files short, reducing replay time.</li>
<li>Archived WAL segments can be copied alongside manifest files for hot-backup strategiesâ€”since the manifest contains the WAL log number, snapshots behave like RocksDBâ€™s <code>Checkpoints</code>.</li>
</ul>
<hr>
<h2 id="9-truncation-metadata"><a class="header" href="#9-truncation-metadata">9. Truncation Metadata</a></h2>
<ul>
<li><code>raftstore/engine/wal_storage</code> keeps a per-group index of <code>[firstIndex,lastIndex]</code> spans for each WAL record so it can map raft log indices back to the segment that stored them.</li>
<li>When a log is truncated (either via snapshot or future compaction hooks), the manifest is updated via <code>LogRaftTruncate</code> with the index/term, segment ID (<code>RaftLogPointer.SegmentIndex</code>), and byte offset (<code>RaftLogPointer.TruncatedOffset</code>) that delimit the remaining WAL data.</li>
<li><code>lsm/levelManager.canRemoveWalSegment</code> now blocks garbage collection whenever any raft group still references a segment through its truncation metadata, preventing slow followers from losing required WAL history while letting aggressively compacted groups release older segments earlier.</li>
</ul>
<p>For broader context, read the <a href="#nokv-architecture-overview">architecture overview</a> and <a href="#memtable-flush-pipeline">flush pipeline</a> documents.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="value-log-vlog-design"><a class="header" href="#value-log-vlog-design">Value Log (vlog) Design</a></h1>
<p>NoKV keeps the LSM tree lean by separating large values into sequential <strong>value log</strong> (vlog) files. The module is split between</p>
<ul>
<li><a href="../vlog/manager.go"><code>vlog/manager.go</code></a> â€“ owns the open file set, rotation, and segment lifecycle helpers.</li>
<li><a href="../vlog/io.go"><code>vlog/io.go</code></a> â€“ append/read/iterate/verify/sample IO paths.</li>
<li><a href="../vlog.go"><code>vlog.go</code></a> â€“ integrates the manager with the DB write path, discard statistics, and garbage collection (GC).</li>
</ul>
<p>The design echoes BadgerDBâ€™s value log while remaining manifest-driven like RocksDBâ€™s <code>blob_db</code>: vlog metadata (head pointer, pending deletions) is persisted inside the manifest so recovery can reconstruct the exact state without scanning the filesystem.</p>
<hr>
<h2 id="1-layering-engine-view"><a class="header" href="#1-layering-engine-view">1. Layering (Engine View)</a></h2>
<p>The value log is split into three layers so IO can stay reusable while DB
policy lives in the core package:</p>
<ul>
<li><strong>DB policy layer (<code>vlog.go</code>, <code>vlog_gc.go</code>)</strong> â€“ integrates the manager with
the DB write path, persists vlog metadata in the manifest, and drives GC
scheduling based on discard stats.</li>
<li><strong>Value-log manager (<code>vlog/</code>)</strong> â€“ owns segment lifecycle (open/rotate/remove),
encodes/decodes entries, and exposes append/read/sample APIs without touching
MVCC or LSM policy.</li>
<li><strong>File IO (<code>file/</code>)</strong> â€“ mmap-backed <code>LogFile</code> primitives (open/close/truncate,
read/write, read-only remap) shared by WAL/vlog/SST. Vlog currently uses
<code>LogFile</code> directly instead of an intermediate store abstraction.</li>
</ul>
<hr>
<h2 id="2-directory-layout--naming"><a class="header" href="#2-directory-layout--naming">2. Directory Layout &amp; Naming</a></h2>
<pre><code class="language-text">&lt;workdir&gt;/
  vlog/
    00000.vlog
    00001.vlog
    ...
</code></pre>
<ul>
<li>Files are named <code>%05d.vlog</code> and live under <code>workdir/vlog/</code>. <a href="../vlog/manager.go"><code>Manager.populate</code></a> discovers existing segments at open.</li>
<li><code>Manager</code> tracks the active file ID (<code>activeID</code>) and byte offset; <a href="../vlog/manager.go"><code>Manager.Head</code></a> exposes these so the manifest can checkpoint them (<code>manifest.EditValueLogHead</code>).</li>
<li>Files created after a crash but never linked in the manifest are removed during <a href="../vlog.go"><code>valueLog.reconcileManifest</code></a>.</li>
</ul>
<hr>
<h2 id="3-record-format"><a class="header" href="#3-record-format">3. Record Format</a></h2>
<p>The vlog uses the shared encoding helper (<code>kv.EncodeEntryTo</code>), so entries written to the value log and the WAL are byte-identical.</p>
<pre><code>+--------+----------+------+-------------+-----------+-------+
| KeyLen | ValueLen | Meta | ExpiresAt   | Key bytes | Value |
+--------+----------+------+-------------+-----------+-------+
                                             + CRC32 (4 B)
</code></pre>
<ul>
<li>Header fields are varint-encoded (<code>kv.EntryHeader</code>).</li>
<li>The first 20 bytes of every segment are reserved (<code>kv.ValueLogHeaderSize</code>) for future metadata; iteration always skips this fixed header.</li>
<li><code>kv.EncodeEntry</code> and the entry iterator (<code>kv.EntryIterator</code>) perform the layout work, and each append finishes with a CRC32 to detect torn writes.</li>
<li><code>vlog.VerifyDir</code> scans all segments with <a href="../vlog/io.go"><code>sanitizeValueLog</code></a> to trim corrupted tails after crashes, mirroring RocksDBâ€™s <code>blob_file::Sanitize</code>. Badger performs a similar truncation pass at startup.</li>
</ul>
<hr>
<h2 id="4-manager-api-surface"><a class="header" href="#4-manager-api-surface">4. Manager API Surface</a></h2>
<pre><code class="language-go">mgr, _ := vlog.Open(vlog.Config{Dir: "...", MaxSize: 1&lt;&lt;29})
ptr, _ := mgr.AppendEntry(entry)
ptrs, _ := mgr.AppendEntries(entries, writeMask)
val, unlock, _ := mgr.Read(ptr)
unlock()             // release per-file lock
_ = mgr.Rewind(*ptr) // rollback partially written batch
_ = mgr.Remove(fid)  // close + delete file
</code></pre>
<p>Key behaviours:</p>
<ol>
<li><strong>Append + Rotate</strong> â€“ <a href="../vlog/io.go"><code>Manager.AppendEntry</code></a> encodes and appends into the active file. The reservation path handles rotation when the active segment would exceed <code>MaxSize</code>; manual rotation is rare.</li>
<li><strong>Crash recovery</strong> â€“ <a href="../vlog/manager.go"><code>Manager.Rewind</code></a> truncates the active file and removes newer files when a write batch fails mid-flight. <code>valueLog.write</code> uses this to guarantee idempotent WAL/value log ordering.</li>
<li><strong>Safe reads</strong> â€“ <a href="../vlog/io.go"><code>Manager.Read</code></a> returns an mmap-backed slice plus an unlock callback. Active segments take a per-file <code>RWMutex</code>, while sealed segments use a pin/unpin path to avoid long-held locks; callers that need ownership should copy the bytes before releasing the lock.</li>
<li><strong>Verification</strong> â€“ <a href="../vlog/io.go"><code>VerifyDir</code></a> validates entire directories (used by CLI and recovery) by parsing headers and CRCs.</li>
</ol>
<p>Compared with RocksDBâ€™s blob manager the surface is intentionally smallâ€”NoKV treats the manager as an append-only log with rewind semantics, while RocksDB maintains index structures inside the blob file metadata.</p>
<hr>
<h2 id="5-integration-with-db-writes"><a class="header" href="#5-integration-with-db-writes">5. Integration with DB Writes</a></h2>
<pre class="mermaid">sequenceDiagram
    participant Commit as commitWorker
    participant Mgr as vlog.Manager
    participant WAL as wal.Manager
    participant Mem as MemTable
    Commit-&gt;&gt;Mgr: AppendEntries(entries, writeMask)
    Mgr--&gt;&gt;Commit: ValuePtr list
    Commit-&gt;&gt;WAL: Append(entries+ptrs)
    Commit-&gt;&gt;Mem: apply to skiplist
</pre>

<ol>
<li><a href="../vlog.go"><code>valueLog.write</code></a> builds a write mask for each batch, then delegates to <a href="../vlog/io.go"><code>Manager.AppendEntries</code></a>. Entries staying in LSM (<code>shouldWriteValueToLSM</code>) receive zero-value pointers.</li>
<li>Rotation is handled inside the manager when the reserved bytes would exceed <code>MaxSize</code>. The WAL append happens <strong>after</strong> the value log append so crash replay observes consistent pointers.</li>
<li>Any error triggers <code>Manager.Rewind</code> back to the saved head pointer, removing new files and truncating partial bytes. <a href="../vlog_test.go"><code>vlog_test.go</code></a> exercises both append- and rotate-failure paths.</li>
<li><code>Txn.Commit</code> and batched writes share the same pipeline: the commit worker always writes the value log first, then applies to WAL/memtable, keeping MVCC and durability ordering consistent.</li>
</ol>
<p>Badger follows the same ordering (value log first, then write batch). RocksDBâ€™s blob DB instead embeds blob references into the WAL entry before the blob file write, relying on two-phase commit between WAL and blob; NoKV avoids the extra coordination by reusing a single batching loop.</p>
<hr>
<h2 id="5-discard-statistics--gc"><a class="header" href="#5-discard-statistics--gc">5. Discard Statistics &amp; GC</a></h2>
<pre class="mermaid">flowchart LR
  FlushMgr -- "obsolete ptrs" --&gt; DiscardStats
  DiscardStats --&gt;|"batch json"| writeCh
  valuePtr["valueLog.newValuePtr(lfDiscardStatsKey)"]
  writeCh --&gt; valuePtr
  valueLog -- "GC trigger" --&gt; Manager

</pre>

<ul>
<li><code>lfDiscardStats</code> aggregates per-file discard counts from <code>lsm.FlushTable</code> completion (<code>valueLog.lfDiscardStats.push</code> inside <code>lsm/flush</code>). Once the in-memory counter crosses <a href="../vlog.go#L27"><code>discardStatsFlushThreshold</code></a>, it marshals the map into JSON and writes it back through the DB pipeline under the special key <code>!NoKV!discard</code>.</li>
<li><code>valueLog.flushDiscardStats</code> consumes those stats, ensuring they are persisted even across crashes. During recovery <code>valueLog.populateDiscardStats</code> replays the JSON payload to repopulate the in-memory map.</li>
<li>GC uses <code>discardRatio = discardedBytes/totalBytes</code> derived from <a href="../vlog/io.go"><code>Manager.Sample</code></a>, which applies windowed iteration based on configurable ratios. If a file exceeds the configured threshold, <a href="../vlog_gc.go"><code>valueLog.doRunGC</code></a> rewrites live entries into the current head via <code>db.batchSet</code> (the normal commit pipeline) and then <a href="../vlog_gc.go"><code>valueLog.rewrite</code></a> triggers manifest delete edits through <code>removeValueLogFile</code>.
<ul>
<li>Sampling behaviour is controlled by <code>Options.ValueLogGCSampleSizeRatio</code> (default 0.10 of the file) and <code>Options.ValueLogGCSampleCountRatio</code> (default 1% of the configured entry limit). Setting either to <code>&lt;=0</code> keeps the default heuristics. <code>Options.ValueLogGCSampleFromHead</code> starts sampling from the beginning instead of a random window.</li>
</ul>
</li>
<li>Completed deletions are logged via <code>lsm.LogValueLogDelete</code> so the manifest can skip them during replay. When GC rotates to a new head, <code>valueLog.updateHead</code> records the pointer and bumps the <code>NoKV.ValueLog.HeadUpdates</code> counter.</li>
</ul>
<p>RocksDBâ€™s blob GC leans on compaction iterators to discover obsolete blobs. NoKV, like Badger, leverages flush/compaction discard stats so GC does not need to rescan SSTs.</p>
<hr>
<h2 id="6-recovery-semantics"><a class="header" href="#6-recovery-semantics">6. Recovery Semantics</a></h2>
<ol>
<li><code>DB.Open</code> restores the manifest and fetches the last persisted head pointer.</li>
<li><a href="../vlog.go"><code>valueLog.open</code></a> launches <code>flushDiscardStats</code> and iterates every vlog file via <a href="../vlog.go"><code>valueLog.replayLog</code></a>. Files marked invalid in the manifest are removed; valid ones are registered in the managerâ€™s file map.</li>
<li><code>valueLog.replayLog</code> streams entries to validate checksums and trims torn tails; it does <strong>not</strong> reapply data into the LSM. WAL replay remains the sole source of committed state.</li>
<li><code>Manager.VerifyDir</code> trims torn records so replay never sees corrupt payloads.</li>
<li>After validation, <code>valueLog.populateDiscardStats</code> rehydrates discard counters from the persisted JSON entry if present.</li>
</ol>
<p>The flow mirrors Badgerâ€™s vlog scanning but keeps state reconstruction anchored on WAL + manifest checkpoints, similar to RocksDBâ€™s reliance on <code>MANIFEST</code> to mark blob files live or obsolete.</p>
<hr>
<h2 id="7-observability--cli"><a class="header" href="#7-observability--cli">7. Observability &amp; CLI</a></h2>
<ul>
<li>Metrics in <a href="../stats.go"><code>stats.go</code></a> report segment counts, pending deletions, discard queue depth, and GC head pointer via <code>expvar</code>.</li>
<li><code>nokv vlog --workdir &lt;dir&gt;</code> loads a manager in read-only mode and prints current head plus file status (valid, gc candidate). It invokes <a href="../vlog/io.go"><code>vlog.VerifyDir</code></a> before describing segments.</li>
<li>Recovery traces controlled by <code>RECOVERY_TRACE_METRICS</code> log every head movement and file removal, aiding pressure testing of GC edge cases. For ad-hoc diagnostics, enable <code>Options.ValueLogVerbose</code> to emit replay/GC messages to stdout.</li>
</ul>
<hr>
<h2 id="8-quick-comparison"><a class="header" href="#8-quick-comparison">8. Quick Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Capability</th><th>RocksDB BlobDB</th><th>BadgerDB</th><th>NoKV</th></tr>
</thead>
<tbody>
<tr><td>Head tracking</td><td>In MANIFEST (blob log number + offset)</td><td>Internal to vlog directory</td><td>Manifest entry via <code>EditValueLogHead</code></td></tr>
<tr><td>GC trigger</td><td>Compaction sampling, blob garbage score</td><td>Discard stats from LSM tables</td><td>Discard stats flushed through <code>lfDiscardStats</code></td></tr>
<tr><td>Failure recovery</td><td>Blob DB and WAL coordinate two-phase commits</td><td>Replays value log then LSM</td><td>Rewind-on-error + manifest-backed deletes</td></tr>
<tr><td>Read path</td><td>Separate blob cache</td><td>Direct read + checksum</td><td><code>Manager.Read</code> with copy + per-file lock</td></tr>
</tbody>
</table>
</div>
<p>By anchoring the vlog state in the manifest and exposing rewind/verify primitives, NoKV maintains the determinism of RocksDB while keeping Badgerâ€™s simple sequential layout.</p>
<hr>
<h2 id="9-further-reading"><a class="header" href="#9-further-reading">9. Further Reading</a></h2>
<ul>
<li><a href="#value-log-recovery"><code>docs/recovery.md</code></a> â€“ failure matrix covering append crashes, GC interruptions, and manifest rewrites.</li>
<li><a href="#value-pointer-reads"><code>docs/cache.md</code></a> â€“ how vlog-backed entries interact with the block cache.</li>
<li><a href="#value-log-metrics"><code>docs/stats.md</code></a> â€“ metric names surfaced for monitoring.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="manifest--version-management"><a class="header" href="#manifest--version-management">Manifest &amp; Version Management</a></h1>
<p>The manifest keeps the source of truth for SST files, WAL checkpoints, and ValueLog heads. NoKVâ€™s implementation (<code>manifest/manager.go</code>, <code>manifest/codec.go</code>, <code>manifest/types.go</code>) borrows RocksDBâ€™s <code>VersionEdit + CURRENT</code> pattern while adding metadata required for value separation.</p>
<hr>
<h2 id="1-file-layout"><a class="header" href="#1-file-layout">1. File Layout</a></h2>
<pre><code class="language-text">WorkDir/
  CURRENT             # stores the active MANIFEST file name
  MANIFEST-000001     # log of manifest edits
  MANIFEST-000002     # newer file after rewrite
</code></pre>
<ul>
<li><code>CURRENT</code> is atomically swapped via <code>CURRENT.tmp</code> â†’ <code>CURRENT</code> rename.</li>
<li>Each <code>MANIFEST-*</code> contains a series of binary edits prefixed by the magic string <code>"NoKV"</code> (encoding lives in <code>manifest/codec.go</code>).</li>
<li>During <code>manifest.Open</code>, <code>loadCurrent</code> opens the file referenced by CURRENT; if missing, <code>createNew</code> bootstraps an empty manifest.</li>
</ul>
<hr>
<h2 id="2-edit-types"><a class="header" href="#2-edit-types">2. Edit Types</a></h2>
<pre><code class="language-go">type EditType uint8
const (
    EditAddFile EditType = iota
    EditDeleteFile
    EditLogPointer
    EditValueLogHead
    EditDeleteValueLog
    EditUpdateValueLog
    EditRaftPointer
    EditRegion
)
</code></pre>
<p>Each edit serialises one logical action:</p>
<ul>
<li><code>EditAddFile</code> / <code>EditDeleteFile</code> â€“ manage SST metadata (<code>FileMeta</code>: level, fileID, size, key bounds, timestamps).</li>
<li><code>EditLogPointer</code> â€“ persists the latest WAL segment + offset checkpoint, analogous to RocksDBâ€™s <code>log_number</code> and <code>prev_log_number</code> fields.</li>
<li><code>EditValueLogHead</code> â€“ records the head pointer for vlog append, ensuring recovery resumes from the correct file/offset.</li>
<li><code>EditDeleteValueLog</code> â€“ marks a vlog segment logically deleted (GC has reclaimed it).</li>
<li><code>EditUpdateValueLog</code> â€“ updates metadata for an existing vlog file (used when GC rewrites a segment).</li>
<li><code>EditRaftPointer</code> â€“ persists raft-group WAL progress (segment, offset, applied/truncated index &amp; term, etc.).</li>
<li><code>EditRegion</code> â€“ persists Region metadata (key range, epoch, peers, lifecycle state).</li>
</ul>
<p><code>manifest.Manager.apply</code> interprets each edit and updates the in-memory <code>Version</code> structure, which is consumed by LSM initialisation and value log recovery.</p>
<hr>
<h2 id="3-version-structure"><a class="header" href="#3-version-structure">3. Version Structure</a></h2>
<pre><code class="language-go">type Version struct {
    Levels       map[int][]FileMeta
    LogSegment   uint32
    LogOffset    uint64
    ValueLogs    map[uint32]ValueLogMeta
    ValueLogHead ValueLogMeta
    RaftPointers map[uint64]RaftLogPointer
    Regions      map[uint64]RegionMeta
}
</code></pre>
<ul>
<li><code>Levels</code> mirrors the LSM tree levels; during recovery <code>lsm.LSM</code> loads files per level.</li>
<li><code>LogSegment</code>/<code>LogOffset</code> ensure WAL replay starts exactly where persistent state ended.</li>
<li><code>ValueLogs</code> holds metadata for every known vlog file; <code>ValueLogHead</code> caches the active head for quick access.</li>
</ul>
<p>Compared with RocksDB: RocksDBâ€™s manifest stores blob file metadata when <code>BlobDB</code> is enabled. NoKV integrates vlog metadata natively to avoid a separate blob manifest.</p>
<hr>
<h2 id="4-lifecycle"><a class="header" href="#4-lifecycle">4. Lifecycle</a></h2>
<pre class="mermaid">sequenceDiagram
    participant DB
    participant Manifest
    participant CURRENT
    DB-&gt;&gt;Manifest: Open(dir)
    Manifest-&gt;&gt;CURRENT: read file name
    Manifest-&gt;&gt;Manifest: replay edits â†’ Version
    DB-&gt;&gt;Manifest: LogEdit(EditAddFile+LogPointer)
    Manifest-&gt;&gt;Manifest: append edit
    Manifest--&gt;&gt;DB: updated Version
    Note over Manifest,CURRENT: On rewrite -&gt; write tmp -&gt; rename CURRENT
</pre>

<ul>
<li><strong>Open/Rebuild</strong> â€“ <code>replay</code> reads all edits, applying them sequentially (<code>bufio.Reader</code> ensures streaming). If any edit fails to decode, recovery aborts so operators can inspect the manifest, similar to RocksDBâ€™s strictness.</li>
<li><strong>LogEdit</strong> â€“ obtains the mutex, appends the encoded edit, flushes, and updates the in-memory <code>Version</code> before returning.</li>
<li><strong>Rewrite</strong> â€“ when the manifest grows beyond <code>Options.ManifestRewriteThreshold</code>, the manager writes a new <code>MANIFEST-xxxxxx</code> containing a full snapshot of the current <code>Version</code>, fsyncs it, updates <code>CURRENT</code>, and removes the old file. This mirrors RocksDBâ€™s <code>max_manifest_file_size</code> behavior while keeping recovery simple.</li>
<li><strong>Close</strong> â€“ flushes and closes the underlying file handle; the version stays available for introspection via <code>Manager.Version()</code> (used by CLI).</li>
</ul>
<hr>
<h2 id="5-interaction-with-other-modules"><a class="header" href="#5-interaction-with-other-modules">5. Interaction with Other Modules</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Manifest usage</th></tr>
</thead>
<tbody>
<tr><td><code>lsm</code></td><td><code>installLevel0Table</code> logs <code>EditAddFile</code> + <code>EditLogPointer</code> to checkpoint WAL progress. Compaction deletes old files via <code>EditDeleteFile</code>.</td></tr>
<tr><td><code>wal</code></td><td>Manifestâ€™s log pointer tells WAL replay where to resume.</td></tr>
<tr><td><code>vlog</code></td><td><code>valueLog.rewrite</code> writes <code>EditUpdateValueLog</code> / <code>EditDeleteValueLog</code> after GC, ensuring stale segments are not reopened.</td></tr>
<tr><td><code>CLI</code></td><td><code>nokv manifest</code> reads <code>manifest.Manager.Version()</code> and prints levels, vlog head, and deletion status.</td></tr>
</tbody>
</table>
</div>
<p>Badger keeps a separate <code>value.log</code> directory without manifest-level bookkeeping; NoKVâ€™s integrated manifest avoids scanning the filesystem during recovery.</p>
<hr>
<h2 id="6-recovery-scenarios"><a class="header" href="#6-recovery-scenarios">6. Recovery Scenarios</a></h2>
<ol>
<li><strong>Missing SST file</strong> â€“ if <code>MANIFEST</code> references <code>000123.sst</code> but the file is absent, <code>db_recovery_test.go::TestRecoveryCleansMissingSSTFromManifest</code> verifies that recovery removes the edit, mimicking RocksDBâ€™s lost table handling.</li>
<li><strong>ValueLog deletion</strong> â€“ <code>TestRecoveryRemovesStaleValueLogSegment</code> ensures <code>EditDeleteValueLog</code> entries trigger file removal during recovery.</li>
<li><strong>Manifest rewrite crash</strong> â€“ <code>TestRecoveryManifestRewriteCrash</code> simulates a crash after writing the new manifest but before updating <code>CURRENT</code>; recovery still points to the old manifest and resumes safely, exactly like RocksDBâ€™s two-phase rewrite.</li>
<li><strong>Stale WAL pointer</strong> â€“ WAL replay respects <code>LogSegment/Offset</code>; tests cover truncated WALs to confirm idempotency.</li>
</ol>
<hr>
<h2 id="7-cli-output"><a class="header" href="#7-cli-output">7. CLI Output</a></h2>
<p><code>nokv manifest --workdir &lt;dir&gt; --json</code> prints:</p>
<ul>
<li>Level file counts and key ranges.</li>
<li><code>wal_log_segment</code> / <code>wal_log_offset</code> checkpoint.</li>
<li><code>value_log_head</code> metadata.</li>
<li>List of vlog files with <code>valid</code> status (mirroring RocksDBâ€™s blob file dump).</li>
</ul>
<p>This structured output enables automated validation in CI and ad-hoc audits.</p>
<hr>
<h2 id="8-extensibility"><a class="header" href="#8-extensibility">8. Extensibility</a></h2>
<ul>
<li><strong>Column families</strong> â€“ add a column family identifier to <code>FileMeta</code> and extend edits accordingly, as RocksDB does.</li>
<li><strong>Snapshots</strong> â€“ persistent snapshots can be derived from manifest versions (keep a copy of the current Version and WAL pointer).</li>
<li><strong>Remote manifests</strong> â€“ similar to RocksDBâ€™s remote compaction, storing manifests in object storage is straightforward because edits are append-only.</li>
</ul>
<p>For end-to-end recovery context, see <a href="#crash-recovery-playbook">recovery.md</a> and the <a href="#nokv-architecture-overview">architecture overview</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="file-abstractions"><a class="header" href="#file-abstractions">File Abstractions</a></h1>
<p>The <code>file</code> package encapsulates direct file-system interaction for WAL, SST, and value-log files. It provides portable mmap helpers, allocation primitives, and log file wrappers.</p>
<hr>
<h2 id="1-core-types"><a class="header" href="#1-core-types">1. Core Types</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Purpose</th><th>Key Methods</th></tr>
</thead>
<tbody>
<tr><td><a href="../file/file.go#L5-L16"><code>Options</code></a></td><td>Parameter bag for opening files (FID, path, size).</td><td>Used by WAL/vlog managers.</td></tr>
<tr><td><a href="../file/file.go#L18-L27"><code>CoreFile</code></a></td><td>Interface abstracting platform-specific operations.</td><td><code>NewReader</code>, <code>Bytes</code>, <code>Sync</code>, <code>Delete</code>.</td></tr>
<tr><td><a href="../file/mmap_linux.go#L12-L98"><code>MmapFile</code></a></td><td>Cross-platform mmap wrapper.</td><td><code>OpenMmapFile</code>, <code>AppendBuffer</code>, <code>Truncature</code>, <code>Sync</code>.</td></tr>
<tr><td><a href="../file/vlog.go#L16-L130"><code>LogFile</code></a></td><td>Value-log specific helper built on <code>MmapFile</code>.</td><td><code>Open</code>, <code>Write</code>, <code>Read</code>, <code>DoneWriting</code>, <code>EncodeEntry</code>.</td></tr>
</tbody>
</table>
</div>
<p>Darwin-specific builds live alongside (<code>mmap_darwin.go</code>, <code>sstable_darwin.go</code>) ensuring the package compiles on macOS without manual tuning.</p>
<hr>
<h2 id="2-mmap-management"><a class="header" href="#2-mmap-management">2. Mmap Management</a></h2>
<ul>
<li><code>OpenMmapFile</code> opens or creates a file, optionally extending it to <code>maxSz</code>, then mmaps it. The returned <code>MmapFile</code> exposes <code>Data []byte</code> and the underlying <code>*os.File</code> handle.</li>
<li>Writes grow the map on demand: <code>AppendBuffer</code> checks if the write would exceed the current mapping and calls <code>Truncature</code> to expand (doubling up to 1 GiB increments).</li>
<li><code>Sync</code> flushes dirty pages (<code>mmap.Msync</code>), while <code>Delete</code> unmaps, truncates, closes, and removes the fileâ€”used when dropping SSTs or value-log segments.</li>
</ul>
<p>RocksDB relies on custom Env implementations for portability; NoKV keeps the logic in Go, relying on build tags for OS differences.</p>
<hr>
<h2 id="3-logfile-semantics"><a class="header" href="#3-logfile-semantics">3. LogFile Semantics</a></h2>
<p><code>LogFile</code> wraps <code>MmapFile</code> to simplify value-log operations:</p>
<pre><code class="language-go">lf := &amp;file.LogFile{}
_ = lf.Open(&amp;file.Options{FID: 1, FileName: "00001.vlog", MaxSz: 1&lt;&lt;29})
ptr, _ := lf.EncodeEntry(entry, buf, offset)
_ = lf.Write(offset, buf.Bytes())
_ = lf.DoneWriting(nextOffset)
</code></pre>
<ul>
<li><code>Open</code> mmaps the file and records current size (guarded to <code>&lt; 4 GiB</code>).</li>
<li><code>Read</code> validates offsets against both the mmap length and tracked size, preventing partial reads when GC or drop operations shrink the file.</li>
<li><code>EncodeEntry</code> uses the shared <code>kv.EntryHeader</code> and CRC32 helpers to produce the exact on-disk layout consumed by <code>vlog.Manager</code> and <code>wal.Manager</code>.</li>
<li><code>DoneWriting</code> syncs, truncates to the provided offset, reinitialises the mmap, and keeps the file open in read-write modeâ€”supporting subsequent appends.</li>
<li><code>Rewind</code> (via <code>vlog.Manager.Rewind</code>) leverages <code>LogFile.Truncate</code> and <code>Init</code> to roll back partial batches after errors.</li>
</ul>
<hr>
<h2 id="4-sst-helpers"><a class="header" href="#4-sst-helpers">4. SST Helpers</a></h2>
<p>While SSTable builders/readers live under <code>lsm/table.go</code>, they rely on <code>file</code> helpers to map index/data blocks efficiently. The build tags (<code>sstable_linux.go</code>, <code>sstable_darwin.go</code>) provide OS-specific tuning for direct I/O hints or mmap flags.</p>
<hr>
<h2 id="5-comparison-1"><a class="header" href="#5-comparison-1">5. Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Engine</th><th>Approach</th></tr>
</thead>
<tbody>
<tr><td>RocksDB</td><td>C++ Env &amp; random-access file wrappers.</td></tr>
<tr><td>Badger</td><td><code>y.File</code> abstraction with mmap.</td></tr>
<tr><td>NoKV</td><td>Go-native mmap wrappers with explicit log helpers.</td></tr>
</tbody>
</table>
</div>
<p>By keeping all filesystem primitives in one package, NoKV ensures WAL, vlog, and SST layers share consistent behaviour (sync semantics, truncation rules) and simplifies testing (<code>mmap_linux_test.go</code>).</p>
<hr>
<h2 id="6-operational-notes-1"><a class="header" href="#6-operational-notes-1">6. Operational Notes</a></h2>
<ul>
<li>Value-log and WAL segments rely on <code>DoneWriting</code>/<code>Truncate</code> to seal files; avoid manipulating files externally or mmap metadata may desynchronise.</li>
<li><code>LogFile.AddSize</code> updates the cached size used by readsâ€”critical when rewinding or rewriting segments.</li>
<li><code>SyncDir</code> (see <code>mmap_linux.go</code>) is invoked when new files are created to persist directory entries, similar to RocksDBâ€™s <code>Env::FsyncDir</code>.</li>
</ul>
<p>For more on how these primitives plug into higher layers, see <a href="#wal-subsystem"><code>docs/wal.md</code></a> and <a href="#value-log-vlog-design"><code>docs/vlog.md</code></a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cache--bloom-filters"><a class="header" href="#cache--bloom-filters">Cache &amp; Bloom Filters</a></h1>
<p>NoKVâ€™s LSM tier layers a multi-level block cache with bloom filter caching to accelerate lookups. The implementation is in <a href="../lsm/cache.go"><code>lsm/cache.go</code></a>.</p>
<hr>
<h2 id="1-components"><a class="header" href="#1-components">1. Components</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Purpose</th><th>Source</th></tr>
</thead>
<tbody>
<tr><td><code>cache.indexs</code> + <code>indexHot</code></td><td>Table index cache (<code>fid</code> â†’ <code>*pb.TableIndex</code>) reused across reopen + small CLOCK hot tier fed by HotRing hits.</td><td><a href="../utils/cache"><code>utils/cache</code></a></td></tr>
<tr><td><code>blockCache</code></td><td>Ristretto-based block cache (L0/L1 only) with per-table direct slots; hot block tier (small CLOCK) keeps hotspot blocks resident.</td><td><a href="../lsm/cache.go"><code>lsm/cache.go</code></a></td></tr>
<tr><td><code>bloomCache</code> + <code>hot</code></td><td>LRU cache of bloom filter bitsets per SST plus small CLOCK hot tier to protect frequent filters.</td><td><a href="../lsm/cache.go"><code>lsm/cache.go</code></a></td></tr>
<tr><td><code>cacheMetrics</code></td><td>Atomic hit/miss counters for L0/L1 blocks and blooms.</td><td><a href="../lsm/cache.go#L30-L110"><code>lsm/cache.go#L30-L110</code></a></td></tr>
</tbody>
</table>
</div>
<p>Badger uses a similar block cache split (<code>Pinner</code>/<code>Cache</code>) while RocksDB exposes block cache(s) via the <code>BlockBasedTableOptions</code>. NoKV keeps it Go-native and GC-friendly.</p>
<hr>
<h3 id="11-index-cache--handles"><a class="header" href="#11-index-cache--handles">1.1 Index Cache &amp; Handles</a></h3>
<ul>
<li>SSTable metadata stays with the <code>table</code> struct, while decoded protobuf indexes are stored in <code>cache.indexs</code>. Lookups first hit the cache before falling back to disk.</li>
<li>SST handles are reopened on demand for lower levels. L0/L1 tables keep their file descriptors pinned, while deeper levels close them once no iterator is using the table.</li>
</ul>
<hr>
<h2 id="2-block-cache-strategy"><a class="header" href="#2-block-cache-strategy">2. Block Cache Strategy</a></h2>
<pre><code class="language-text">User-space block cache (L0/L1, parsed blocks, Ristretto LFU-ish)
Small hot tier (CLOCK) for hotspot blocks
Deeper levels rely on OS page cache + mmap readahead
</code></pre>
<ul>
<li><code>Options.BlockCacheSize</code> sets capacity in <strong>blocks</strong> (cost=1 per block). Entries keep parsed blocks (data slice + offsets/baseKey/checksum), so hits avoid re-parsing.</li>
<li>Hot tier: requests marked <code>hot</code> (prefetch/hotspot reads) promote blocks into the small CLOCK hot set derived from the main capacity, making them harder to evict under long-tail traffic.</li>
<li>Per-table direct slots (<code>table.cacheSlots[idx]</code>) give a lock-free fast path. Misses fall back to the shared Ristretto cache (approx LFU with admission).</li>
<li>Evictions clear the table slot via <code>OnEvict</code>; user-space cache only tracks L0/L1 blocks. Deeper levels depend on the OS page cache.</li>
<li>Access patterns: <code>getBlock</code> also updates hit/miss metrics for L0/L1; deeper levels bypass the cache and do not affect metrics.</li>
</ul>
<pre class="mermaid">flowchart LR
  Read --&gt; CheckHot
  CheckHot --&gt;|hit| Return
  CheckHot --&gt;|miss| LoadFromTable["LoadFromTable (mmap + OS page cache)"]
  LoadFromTable --&gt; InsertHot
  InsertHot --&gt; Return
</pre>

<p>By default only L0 and L1 blocks are cached (<code>level &gt; 1</code> short-circuits), reflecting the higher re-use for top levels.</p>
<hr>
<h2 id="3-bloom-cache"><a class="header" href="#3-bloom-cache">3. Bloom Cache</a></h2>
<ul>
<li><code>bloomCache</code> stores the raw filter bitset (<code>utils.Filter</code>) per table ID. Entries are deep-copied (<code>SafeCopy</code>) to avoid sharing memory with mmaps.</li>
<li>Main tier is LRU with a tiny CLOCK hot set to protect frequently hit filters from being washed out by scans.</li>
<li>Capacity is controlled by <code>Options.BloomCacheSize</code>; the hot CLOCK tier auto-scales from a few dozen up to a few hundred entries.</li>
<li>Bloom hits/misses are recorded via <code>cacheMetrics.recordBloom</code>, feeding into <code>StatsSnapshot.BloomHitRate</code>.</li>
</ul>
<hr>
<h2 id="4-metrics--observability"><a class="header" href="#4-metrics--observability">4. Metrics &amp; Observability</a></h2>
<p><code>cache.metricsSnapshot()</code> produces:</p>
<pre><code class="language-go">type CacheMetrics struct {
    L0Hits, L0Misses uint64
    L1Hits, L1Misses uint64
    BloomHits, BloomMisses uint64
    IndexHits, IndexMisses uint64
}
</code></pre>
<p><code>Stats.Snapshot</code> converts these into hit rates. Monitor them alongside the block cache sizes to decide when to scale memory.</p>
<hr>
<h2 id="5-hot-integration-hotring"><a class="header" href="#5-hot-integration-hotring">5. Hot Integration (HotRing)</a></h2>
<ul>
<li>Hot detection: HotRing counts on read/write paths raise a <code>hot</code> flag once thresholds are met; only hot keys trigger prefetch.</li>
<li>Cache promotion: hot hits/prefetch promote blocks into the CLOCK hot tier and promote indexes/Blooms into their CLOCK tiers; cold data stays in the main cache to avoid pollution.</li>
<li>Compaction coupling: HotRing top-k feeds compaction scoring; levels/ingest shards covering hot ranges get higher scores to trim overlap sooner.</li>
<li>Tuning: Hot thresholds come from HotRing options (window/decay configurable); hot tier capacities are small and derived from existing cache sizes.</li>
</ul>
<hr>
<h2 id="6-interaction-with-value-log"><a class="header" href="#6-interaction-with-value-log">6. Interaction with Value Log</a></h2>
<ul>
<li>Keys stored as value pointers (large values) still populate block cache entries for the key/index block. The value payload is read directly from the vlog (<code>valueLog.read</code>), so block cache hit rates remain meaningful.</li>
<li>Discard stats from flushes can demote cached blocks via <code>cache.dropBlock</code>, ensuring obsolete SST data leaves the cache quickly.</li>
</ul>
<hr>
<h2 id="7-comparison"><a class="header" href="#7-comparison">7. Comparison</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>RocksDB</th><th>BadgerDB</th><th>NoKV</th></tr>
</thead>
<tbody>
<tr><td>Hot/cold tiers</td><td>Configurable multiple caches</td><td>Single cache</td><td>Ristretto (hot) + OS page cache (cold)</td></tr>
<tr><td>Bloom cache</td><td>Enabled per table, no explicit cache</td><td>Optional</td><td>Dedicated LRU storing filters</td></tr>
<tr><td>Metrics</td><td>Block cache stats via <code>GetAggregatedIntProperty</code></td><td>Limited</td><td><code>NoKV.Stats.Cache.*</code> hit rates</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="8-operational-tips-1"><a class="header" href="#8-operational-tips-1">8. Operational Tips</a></h2>
<ul>
<li>If bloom hit rate falls below ~60%, consider increasing bits-per-key or Bloom cache size.</li>
<li>Track <code>nokv stats --json</code> cache metrics over time; drops often indicate iterator misuse or working-set shifts.</li>
</ul>
<p>More on SST layout lives in <a href="#manifest--version-management"><code>docs/manifest.md</code></a> and <a href="#4-read-path--iterators"><code>docs/architecture.md</code></a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="hotring--hot-key-tracking"><a class="header" href="#hotring--hot-key-tracking">HotRing â€“ Hot Key Tracking</a></h1>
<p><code>hotring</code> is NoKVâ€™s built-in hot-key tracker. It samples read/write access frequency per key and exposes the hottest entries to the stats subsystem and CLI. The implementation resides in <a href="../hotring"><code>hotring/</code></a>.</p>
<hr>
<h2 id="1-motivation"><a class="header" href="#1-motivation">1. Motivation</a></h2>
<ul>
<li><strong>Cache hints</strong> â€“ <code>DB.prefetchLoop</code> (see <a href="../db.go"><code>db.go</code></a>) consumes hot keys to schedule asynchronous reads into the block cache.</li>
<li><strong>Operational insight</strong> â€“ <code>StatsSnapshot.HotKeys</code> and <code>nokv stats --json</code> surface the hottest keys, aiding debugging of traffic hotspots.</li>
<li><strong>Throttling</strong> â€“ <code>HotRing.TouchAndClamp</code> enables simple rate caps: once a key crosses a threshold, callers can back off or log alerts.</li>
</ul>
<p>Compared with RocksDB (which exposes block access stats via <code>perf_context</code>) and Badger (which lacks built-in hot-key reporting), NoKV offers a lightweight but concurrent-friendly tracker out of the box.</p>
<hr>
<h2 id="2-data-structure"><a class="header" href="#2-data-structure">2. Data Structure</a></h2>
<pre><code class="language-text">HotRing
  buckets[] -&gt; per-bucket lock-free linked list (Node)
  hashFn   -&gt; hash(key) -&gt; uint32
  hashMask -&gt; selects bucket (power of two size)
</code></pre>
<ul>
<li>Each bucket stores a sorted linked list of <a href="../hotring/node.go"><code>Node</code></a> ordered by <code>(tag, key)</code>, where <code>tag</code> is derived from the upper bits of the hash. Head pointers are <code>atomic.Pointer[Node]</code>, so readers walk the list without taking locks; writers use CAS to splice nodes while preserving order.</li>
<li><code>defaultTableBits = 12</code> â†’ 4096 buckets by default (<code>NewHotRing</code>). The mask ensures cheap modulo operations.</li>
<li>Nodes keep a <code>count</code> (int32) updated atomically and a <code>next</code> pointer stored via <code>unsafe.Pointer</code>. Sliding-window state is guarded by a tiny per-node spin lock instead of a process-wide mutex.</li>
</ul>
<pre class="mermaid">flowchart LR
  Key(key) --&gt;|hash| Bucket["buckets[index] (atomic head)"]
  Bucket --&gt; Node1
  Node1 --&gt; Node2
  Node2 --&gt; Node3
  Node3 --&gt; Nil[(nil)]
</pre>

<hr>
<h2 id="3-core-operations"><a class="header" href="#3-core-operations">3. Core Operations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Behaviour</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td><a href="../hotring/hotring.go"><code>Touch</code></a></td><td>Insert or increment keyâ€™s counter.</td><td>CAS-splices a new node if missing, then increments (window-aware when enabled).</td></tr>
<tr><td><a href="../hotring/hotring.go"><code>Frequency</code></a></td><td>Read-only counter lookup.</td><td>Lock-free lookup; uses sliding-window totals when configured.</td></tr>
<tr><td><a href="../hotring/hotring.go"><code>TouchAndClamp</code></a></td><td>Increment unless <code>count &gt;= limit</code>, returning <code>(count, limited)</code>.</td><td>Throttling follows sliding-window totals so hot bursts clamp quickly.</td></tr>
<tr><td><a href="../hotring/hotring.go"><code>TopN</code></a></td><td>Snapshot hottest keys sorted by count desc.</td><td>Walks buckets without locks, then sorts a copy.</td></tr>
<tr><td><a href="../hotring/hotring.go"><code>KeysAbove</code></a></td><td>Return all keys with counters â‰¥ threshold.</td><td>Handy for targeted throttling or debugging hot shards.</td></tr>
</tbody>
</table>
</div>
<p>Bucket ordering is preserved by <code>findOrInsert</code>, which CASes either the bucket head or the predecessorâ€™s <code>next</code> pointer to splice new nodes. Reads never take locks; only per-node sliding-window updates spin briefly to avoid data races.</p>
<hr>
<h2 id="4-integration-points-1"><a class="header" href="#4-integration-points-1">4. Integration Points</a></h2>
<ul>
<li><strong>DB reads</strong> â€“ <code>Txn.Get</code> and iterators call <code>db.recordRead</code>, which in turn invokes <code>HotRing.Touch</code> for every successful lookup. Writes touch the ring only when <code>Options.WriteHotKeyLimit</code> is set, so throttling can clamp abusive keys.</li>
<li><strong>Stats</strong> â€“ <a href="../stats.go#L41-L87"><code>StatsSnapshot</code></a> copies <code>hot.TopN</code> into <code>HotKeys</code>. <code>expvar</code> publishes the same view under <code>NoKV.Stats.HotKeys</code> for automation.</li>
<li><strong>Caching</strong> â€“ <code>lsm/cache</code> can promote blocks referenced by frequently touched keys, keeping the hot tier warm.</li>
</ul>
<hr>
<h2 id="5-comparisons"><a class="header" href="#5-comparisons">5. Comparisons</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Engine</th><th>Approach</th></tr>
</thead>
<tbody>
<tr><td>RocksDB</td><td>External â€“ <code>TRACE</code> / <code>perf_context</code> requires manual sampling.</td></tr>
<tr><td>Badger</td><td>None built-in.</td></tr>
<tr><td>NoKV</td><td>In-process ring with expvar/CLI export and throttling helpers.</td></tr>
</tbody>
</table>
</div>
<p>The HotRing emphasises simplicity: lock-free bucket lists with atomic counters (plus optional per-node window tracking), avoiding sketches while staying light enough for hundreds of thousands of hot keys.</p>
<hr>
<h2 id="6-operational-tips"><a class="header" href="#6-operational-tips">6. Operational Tips</a></h2>
<ul>
<li><code>Options.HotRingTopK</code> controls how many keys show up in stats; default 16. Increase it when investigating workloads with broad hot sets.</li>
<li>Combine <code>TouchAndClamp</code> with request middleware to detect abusive tenants: when <code>limited</code> is true, log the key and latency impact.</li>
<li>Resetting the ring is as simple as instantiating a new <code>HotRing</code>â€”useful for benchmarks that require clean counters between phases.</li>
</ul>
<p>For end-to-end examples see <a href="#hot-key-export"><code>docs/stats.md</code></a> and the CLI walkthrough in <a href="#hot-key-output"><code>docs/cli.md</code></a>.</p>
<hr>
<h2 id="7-write-path-throttling"><a class="header" href="#7-write-path-throttling">7. Write-Path Throttling</a></h2>
<p><code>Options.WriteHotKeyLimit</code> wires HotRing into the write path. When set to a positive integer, every call to <code>DB.Set*</code> or transactional <code>Txn.Set*</code> invokes <code>HotRing.TouchAndClamp</code> with the limit. Once a key (optionally scoped by column family via <code>cfHotKey</code>) reaches the limit, the write is rejected with <code>utils.ErrHotKeyWriteThrottle</code>. This keeps pathological tenants or hot shards from overwhelming a single Raft group without adding heavyweight rate-limiters to the client stack.</p>
<p>Operational hints:</p>
<ul>
<li><code>StatsSnapshot.HotWriteLimited</code> and the CLI line <code>Write.HotKeyThrottled</code> expose how many writes were rejected since the process started.</li>
<li>Applications should surface <code>utils.ErrHotKeyWriteThrottle</code> to callers (e.g. HTTP 429) so clients can back off.</li>
<li>Prefetching continues to run independentlyâ€”only writes are rejected; reads still register hotness so the cache layer knows what to prefetch.</li>
<li>Set the limit conservatively (e.g. a few dozen) and pair it with richer <code>HotRing</code> analytics (top-K stats, expvar export) to identify outliers before tuning.</li>
</ul>
<hr>
<h2 id="8-time-based-decay--sliding-window"><a class="header" href="#8-time-based-decay--sliding-window">8. Time-Based Decay &amp; Sliding Window</a></h2>
<p>HotRing now exposes two complementary controls so â€œoldâ€ hotspots fade away automatically:</p>
<ol>
<li><strong>Periodic decay (<code>Options.HotRingDecayInterval</code> + <code>HotRingDecayShift</code>)</strong><br>Every <code>interval</code> the global counters are right-shifted (<code>count &gt;&gt;= shift</code>). This keeps <code>TopN</code> and stats output focused on recent traffic even if writes stop abruptly.</li>
<li><strong>Sliding window (<code>Options.HotRingWindowSlots</code> + <code>HotRingWindowSlotDuration</code>)</strong><br>Per-key windows split time into <code>slots</code>, each lasting <code>slotDuration</code>. <code>Touch</code> only accumulates inside the current slot; once the window slides past, the stale contribution is dropped. <code>TouchAndClamp</code> and <code>Frequency</code> use the sliding-window total, so write throttling reflects short-term pressure instead of lifetime counts.</li>
</ol>
<p>Disable either mechanism by setting the interval/durations to zero. Typical starting points:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default value</th><th>Effect</th></tr>
</thead>
<tbody>
<tr><td><code>HotRingDecayInterval</code></td><td><code>1s</code></td><td>Halve legacy counters once per second.</td></tr>
<tr><td><code>HotRingDecayShift</code></td><td><code>1</code></td><td>Simple divide-by-two decay.</td></tr>
<tr><td><code>HotRingWindowSlots</code></td><td><code>8</code></td><td>Keep ~8 buckets of recency data.</td></tr>
<tr><td><code>HotRingWindowSlotDuration</code></td><td><code>250ms</code></td><td>Roughly 2s window for throttling.</td></tr>
</tbody>
</table>
</div>
<p>With both enabled, the decay loop keeps background stats tidy while the sliding window powers precise, short-term throttling logic.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="transaction--mvcc-design"><a class="header" href="#transaction--mvcc-design">Transaction &amp; MVCC Design</a></h1>
<p>NoKV provides snapshot-isolated transactions backed by a lightweight <strong>oracle</strong> that hands out timestamps, tracks conflicts, and coordinates with the write pipeline. The implementation lives entirely in <a href="../txn.go"><code>txn.go</code></a> with metrics surfaced via <a href="../stats.go"><code>stats.go</code></a>.</p>
<hr>
<h2 id="1-components-at-a-glance"><a class="header" href="#1-components-at-a-glance">1. Components at a Glance</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Purpose</th><th>Key Functions</th></tr>
</thead>
<tbody>
<tr><td><code>oracle</code></td><td>Issues read/commit timestamps, performs conflict checks, persists watermark progress.</td><td><a href="../txn.go#L80-L100"><code>readTs</code></a>, <a href="../txn.go#L142-L190"><code>newCommitTs</code></a>, <a href="../txn.go#L206-L208"><code>doneCommit</code></a></td></tr>
<tr><td><code>Txn</code></td><td>User-facing transaction state: pending writes, read-set fingerprints, MVCC metadata.</td><td><a href="../txn.go#L412-L425"><code>SetEntry</code></a>, <a href="../txn.go#L428-L502"><code>Get</code></a>, <a href="../txn.go#L618-L671"><code>Commit</code></a></td></tr>
<tr><td><code>pendingWritesIterator</code></td><td>Allows iterator merge to see unflushed txn writes.</td><td><a href="../txn.go#L324-L347"><code>newPendingWritesIterator</code></a></td></tr>
<tr><td>Metrics</td><td>Tracks counts of started/committed/conflicted txns.</td><td><a href="../txn.go#L58-L66"><code>trackTxnStart</code></a>, <a href="../txn.go#L72-L78"><code>txnMetricsSnapshot</code></a></td></tr>
</tbody>
</table>
</div>
<p>The oracle is initialised during <code>DB.Open</code>, sharing lineage with BadgerDBâ€™s MVCC model. Unlike RocksDBâ€”which relies on WriteBatch/TwoPhaseCommit extensionsâ€”transactions are first-class citizens, and the core engine enforces ordering.</p>
<hr>
<h2 id="2-timestamp--conflict-flow"><a class="header" href="#2-timestamp--conflict-flow">2. Timestamp &amp; Conflict Flow</a></h2>
<pre class="mermaid">sequenceDiagram
    participant Client
    participant DB
    participant Oracle
    participant Commit as commitWorker
    participant Mgr as vlog.Manager
    participant WAL
    participant Mem as MemTable
    Client-&gt;&gt;DB: NewTransaction(update)
    DB-&gt;&gt;Oracle: readTs()
    Oracle--&gt;&gt;DB: snapshot ts (nextTxnTs-1)
    Client-&gt;&gt;DB: Set/Delete/Get
    DB-&gt;&gt;Txn: stage pendingWrites, record read hashes
    Client-&gt;&gt;DB: Commit
    DB-&gt;&gt;Oracle: newCommitTs(txn)
    alt conflict
        Oracle--&gt;&gt;DB: ErrConflict
    else success
        Oracle--&gt;&gt;DB: commitTs
        DB-&gt;&gt;Commit: batch requests
        Commit-&gt;&gt;Mgr: AppendEntries(entries, writeMask)
        Commit-&gt;&gt;WAL: Append(entries with commitTs)
        Commit-&gt;&gt;Mem: apply to skiplist
        DB-&gt;&gt;Oracle: doneCommit(commitTs)
    end
</pre>

<ol>
<li><strong>Start</strong> â€“ <code>DB.newTransaction</code> calls <a href="../txn.go#L80-L100"><code>oracle.readTs</code></a>, which waits for all prior commits to finish (<code>txnMark.WaitForMark</code>) so new readers see a consistent snapshot. In distributed deployments, clients must obtain the <code>startVersion</code> themselves (see <a href="#timestamp-sources">Timestamp sources</a>).</li>
<li><strong>Reads</strong> â€“ <code>Txn.Get</code> first checks <code>pendingWrites</code>; otherwise it merges LSM iterators and value-log pointers under the read timestamp. For update transactions the read key fingerprint is recorded in <code>Txn.reads</code> via <a href="../txn.go#L511-L526"><code>addReadKey</code></a>.</li>
<li><strong>Conflict detection</strong> â€“ When <code>Options.DetectConflicts</code> is enabled, <code>oracle.newCommitTs</code> iterates <code>oracle.committedTxns</code> and compares read fingerprints against keys written by newer commits. This mirrors Badgerâ€™s optimistic strategy.</li>
<li><strong>Commit timestamp</strong> â€“ <code>newCommitTs</code> increments <code>nextTxnTs</code>, registers the commit in <code>txnMark</code>, and stores the conflict key set for future comparisons.</li>
<li><strong>Apply</strong> â€“ <code>Txn.commitAndSend</code> assigns the commit timestamp to each pending entry (<code>kv.KeyWithTs</code>), enqueues them through <code>sendToWriteCh</code>, and returns a callback that waits for the batch completion. Only after the callback runs does the oracleâ€™s <code>doneCommit</code> release the commit watermark.</li>
<li><strong>Value log ordering</strong> â€“ As with non-transactional writes, the commit worker runs <code>valueLog.write</code> (which calls <code>Manager.AppendEntries</code>) before the WAL append. On failure <code>vlog.manager.Rewind</code> ensures partial writes do not leak.</li>
</ol>
<p>RocksDBâ€™s default WriteBatch lacks conflict detection, relying on application-level locking; NoKV ships with snapshot isolation and optional detection, similar to Badgerâ€™s <code>Txn</code> but with integrated metrics and iterator pooling.</p>
<hr>
<h2 id="3-data-structures"><a class="header" href="#3-data-structures">3. Data Structures</a></h2>
<h3 id="oracle-watermarks"><a class="header" href="#oracle-watermarks">Oracle Watermarks</a></h3>
<pre><code class="language-text">oracle{
  nextTxnTs       // next commit timestamp to assign
  txnMark         // watermark waiting for WAL/vlog durability
  readMark        // tracks oldest active read timestamp
  committedTxns[] // sliding window of conflict key sets
}
</code></pre>
<ul>
<li><code>txnMark</code> / <code>readMark</code> are <code>utils.WaterMark</code> instances. They guarantee all writes with timestamp â‰¤ <code>readTs</code> are durable before a new read snapshot begins, mirroring Badgerâ€™s approach to avoid reading half-committed data.</li>
<li><code>cleanupCommittedTransactions</code> prunes conflict history based on the oldest outstanding read, preventing unbounded memory use.</li>
</ul>
<h3 id="txn-state"><a class="header" href="#txn-state">Txn State</a></h3>
<pre><code class="language-go">type Txn struct {
    readTs   uint64
    commitTs uint64
    pendingWrites map[string]*kv.Entry
    conflictKeys  map[uint64]struct{}
    reads         []uint64
    numIterators  int32
    discarded     bool
    update        bool
}
</code></pre>
<ul>
<li>Pending writes retain the callerâ€™s entry pointers until commit; NoKV copies values only when moving them into the write batch.</li>
<li>Read fingerprints use <code>kv.MemHash</code>, so conflict detection is order-independent and compact.</li>
<li>MVCC versions are encoded in the key suffix (<code>KeyWithTs</code>), matching the LSMâ€™s descending version order.</li>
</ul>
<h3 id="iterator-integration"><a class="header" href="#iterator-integration">Iterator Integration</a></h3>
<ul>
<li><code>Txn.newPendingWritesIterator</code> materialises staged entries as a sorted slice, allowing transaction iterators to merge them with memtables/SST tables. This ensures <code>Txn.NewIterator</code> sees writes immediately without affecting other snapshots.</li>
<li><code>Txn.numIterators</code> enforces that all iterators close before commit/discardâ€”helpful for catching resource leaks in tests (<code>txn_iterator_test.go</code>).</li>
</ul>
<hr>
<h2 id="4-commit--error-handling"><a class="header" href="#4-commit--error-handling">4. Commit &amp; Error Handling</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Stage</th><th>Failure Handling</th></tr>
</thead>
<tbody>
<tr><td>Conflict</td><td><code>oracle.newCommitTs</code> returns <code>(0, true)</code>; <code>Txn.Commit</code> surfaces <code>utils.ErrConflict</code> and leaves state untouched.</td></tr>
<tr><td>Value log append</td><td><code>valueLog.write</code> rewinds via <code>Manager.Rewind</code>; <code>req.Wait</code> returns the error so callers can retry safely.</td></tr>
<tr><td>WAL append</td><td><code>sendToWriteCh</code> propagates WAL errors; commit watermark is cleared immediately in that case.</td></tr>
<tr><td>Callback mode</td><td><code>Txn.CommitWith</code> schedules <code>runTxnCallback</code> on a goroutine; user callbacks always execute (success or error).</td></tr>
</tbody>
</table>
</div>
<p>The final call to <code>Txn.Discard</code> runs regardless of success, marking the read watermark done and decrementing the oracleâ€™s active counter.</p>
<hr>
<h2 id="5-comparisons-1"><a class="header" href="#5-comparisons-1">5. Comparisons</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Feature</th><th>RocksDB</th><th>BadgerDB</th><th>NoKV</th></tr>
</thead>
<tbody>
<tr><td>Isolation</td><td>Optional (WritePrepared/2PC)</td><td>Snapshot isolation</td><td>Snapshot isolation with <code>WaterMark</code> barriers</td></tr>
<tr><td>Conflict detection</td><td>External</td><td>Optional optimistic</td><td>Optional optimistic keyed by <code>utils.MemHash</code></td></tr>
<tr><td>Iterator view</td><td>Snapshot handles, manual merging</td><td>Built-in</td><td>Built-in with pending write iterator</td></tr>
<tr><td>Metrics</td><td><code>rocksdb.transactions.*</code> when enabled</td><td>Basic stats</td><td><code>NoKV.Txns.*</code> expvar counters + CLI</td></tr>
</tbody>
</table>
</div>
<p>NoKV inherits Badgerâ€™s optimistic concurrency but strengthens durability ordering by coupling commits with the same write pipeline that non-transactional writes use. Compared with RocksDBâ€™s transactional library, the Go implementation remains lightweight and requires no external locks.</p>
<hr>
<h2 id="6-operational-notes-2"><a class="header" href="#6-operational-notes-2">6. Operational Notes</a></h2>
<ul>
<li><strong>Long-running reads</strong>: watch <code>NoKV.Txns.Active</code> and <code>oracle.readMark.DoneUntil()</code>â€”slow consumers keep old versions alive, delaying vlog GC and compaction reclamation.</li>
<li><strong>Non-transactional APIs</strong>: <code>DB.Set/Get/Del</code> and <code>SetCF/GetCF/DelCF</code> use a MaxUint64 sentinel version for â€œlatestâ€. Do not mix these writes with MVCC/Txn writes in the same database.</li>
<li><strong>Managed mode</strong>: exposing <code>Txn.SetEntry</code> with pre-set versions allows replication/replay flows. Because commit timestamps may diverge, transaction markers are only set when all entries share a single commitTs.</li>
<li><strong>Throttling</strong>: combine <code>HotRing.TouchAndClamp</code> with per-transaction analytics to detect hot-key write storms that lead to frequent conflicts.</li>
</ul>
<p>See <a href="#transactions"><code>docs/testing.md</code></a> for the regression matrix covering conflict detection, iterator semantics, and managed timestamps.</p>
<hr>
<h2 id="7-timestamp-sources"><a class="header" href="#7-timestamp-sources">7. Timestamp Sources</a></h2>
<p>Replica nodes do <strong>not</strong> generate timestamps during TinyKV RPC handling; the values sent in <code>KvPrewrite</code>/<code>KvCommit</code> are applied verbatim. For teaching and prototyping you can pick from two approaches:</p>
<ul>
<li>
<p><strong>Single-client experiments</strong> â€“ choose monotonically increasing integers in your client code (as shown in <code>raftstore/client/client_test.go</code>).</p>
</li>
<li>
<p><strong>Shared allocator</strong> â€“ run the sample TSO service under <code>scripts/tso</code> to hand out globally increasing timestamps:</p>
<pre><code class="language-bash">go run ./scripts/tso --addr 127.0.0.1:9494 --start 100

# request one timestamp
curl -s http://127.0.0.1:9494/tso
# request a batch of 16
curl -s "http://127.0.0.1:9494/tso?batch=16"
</code></pre>
<p>Each call returns JSON (<code>{"timestamp":123,"count":1}</code>), where <code>timestamp</code> is the first value in the allocated range. Clients can use the first value for <code>startVersion</code>, or the entire range to provision multiple transactions. This keeps the learning focus on the Percolator flow while demonstrating how production systems would obtain globally ordered timestamps.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="raftstore-deep-dive"><a class="header" href="#raftstore-deep-dive">RaftStore Deep Dive</a></h1>
<p><code>raftstore</code> powers NoKVâ€™s distributed mode by layering multi-Raft replication on top of the embedded storage engine. This note explains the major packages, the boot and command paths, how transport and storage interact, and the supporting tooling for observability and testing.</p>
<hr>
<h2 id="1-package-structure"><a class="header" href="#1-package-structure">1. Package Structure</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Package</th><th>Responsibility</th></tr>
</thead>
<tbody>
<tr><td><a href="../raftstore/store"><code>store</code></a></td><td>Orchestrates peer set, command pipeline, region manager, scheduler/heartbeat loops; exposes helpers such as <code>StartPeer</code>, <code>ProposeCommand</code>, <code>SplitRegion</code>.</td></tr>
<tr><td><a href="../raftstore/peer"><code>peer</code></a></td><td>Wraps etcd/raft <code>RawNode</code>, drives Ready processing (persist to WAL, send messages, apply entries), tracks snapshot resend/backlog.</td></tr>
<tr><td><a href="../raftstore/engine"><code>engine</code></a></td><td>WALStorage/DiskStorage/MemoryStorage across all Raft groups, leveraging the NoKV WAL while keeping manifest metadata in sync.</td></tr>
<tr><td><a href="../raftstore/transport"><code>transport</code></a></td><td>gRPC transport with retry/TLS/backpressure; exposes the raft Step RPC and can host additional services (TinyKv).</td></tr>
<tr><td><a href="../raftstore/kv"><code>kv</code></a></td><td>TinyKv RPC implementation, bridging Raft commands to MVCC operations via <code>kv.Apply</code>.</td></tr>
<tr><td><a href="../raftstore/server"><code>server</code></a></td><td><code>ServerConfig</code> + <code>New</code> that bind DB, Store, transport, and TinyKv server into a reusable node primitive.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="2-boot-sequence"><a class="header" href="#2-boot-sequence">2. Boot Sequence</a></h2>
<ol>
<li>
<p><strong>Construct Server</strong></p>
<pre><code class="language-go">srv, _ := raftstore.NewServer(raftstore.ServerConfig{
    DB: db,
    Store: raftstore.StoreConfig{StoreID: 1},
    Raft: myraft.Config{ElectionTick: 10, HeartbeatTick: 2, PreVote: true},
    TransportAddr: "127.0.0.1:20160",
})
</code></pre>
<ul>
<li>A gRPC transport is created, the TinyKv service is registered, and <code>transport.SetHandler(store.Step)</code> wires raft Step handling.</li>
<li><code>store.Store</code> loads <code>manifest.RegionSnapshot()</code> to rebuild the Region catalog (router + metrics).</li>
</ul>
</li>
<li>
<p><strong>Start local peers</strong></p>
<ul>
<li>CLI (<code>nokv serve</code>) iterates the manifest snapshot and calls <code>Store.StartPeer</code> for every region that includes the local store.</li>
<li>Each <code>peer.Config</code> carries raft parameters, the transport reference, <code>kv.NewEntryApplier</code>, WAL/manifest handles, and Region metadata.</li>
<li><code>StartPeer</code> registers the peer through the peer-set/routing layer and may bootstrap or campaign for leadership.</li>
</ul>
</li>
<li>
<p><strong>Peer connectivity</strong></p>
<ul>
<li><code>transport.SetPeer(storeID, addr)</code> defines outbound raft connections; the CLI exposes it via <code>--peer storeID=addr</code>.</li>
<li>Additional services can reuse the same gRPC server through <code>transport.WithServerRegistrar</code>.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="3-command-execution"><a class="header" href="#3-command-execution">3. Command Execution</a></h2>
<h3 id="read-strong-leader-read"><a class="header" href="#read-strong-leader-read">Read (strong leader read)</a></h3>
<ol>
<li><code>kv.Service.KvGet</code> builds <code>pb.RaftCmdRequest</code> and invokes <code>Store.ReadCommand</code>.</li>
<li><code>validateCommand</code> ensures the region exists, epoch matches, and the local peer is leader; a RegionError is returned otherwise.</li>
<li><code>peer.Flush()</code> drains pending Ready, guaranteeing the latest committed log is applied.</li>
<li><code>commandApplier</code> (i.e. <code>kv.Apply</code>) runs GET/SCAN directly against the DB, using MVCC readers to honour locks and version visibility.</li>
</ol>
<h3 id="write-via-propose"><a class="header" href="#write-via-propose">Write (via Propose)</a></h3>
<ol>
<li>Write RPCs (Prewrite/Commit/â€¦) call <code>Store.ProposeCommand</code>, encoding the command and routing to the leader peer.</li>
<li>The leader appends the encoded request to raft, replicates, and once committed the command pipeline hands data to <code>kv.Apply</code>, which maps Prewrite/Commit/ResolveLock to the <code>percolator</code> package.</li>
<li><code>engine.WALStorage</code> persists raft entries/state snapshots and updates manifest raft pointers. This keeps WAL GC and raft truncation aligned.</li>
</ol>
<hr>
<h2 id="4-transport"><a class="header" href="#4-transport">4. Transport</a></h2>
<ul>
<li>gRPC transport listens on <code>TransportAddr</code>, serving both raft Step RPC and TinyKv RPC.</li>
<li><code>SetPeer</code> updates the mapping of remote store IDs to addresses; <code>BlockPeer</code> can be used by tests or chaos tooling.</li>
<li>Configurable retry/backoff/timeout options mirror production requirements. Tests cover message loss, blocked peers, and partitions.</li>
</ul>
<hr>
<h2 id="5-storage-backend-engine"><a class="header" href="#5-storage-backend-engine">5. Storage Backend (engine)</a></h2>
<ul>
<li><code>WALStorage</code> piggybacks on the embedded WAL: each Raft group writes typed entries, HardState, and snapshots into the shared log.</li>
<li><code>LogRaftPointer</code> and <code>LogRaftTruncate</code> edit manifest metadata so WAL GC knows how far it can compact per group.</li>
<li>Alternative storage backends (<code>DiskStorage</code>, <code>MemoryStorage</code>) are available for tests and special scenarios.</li>
</ul>
<hr>
<h2 id="6-tinykv-rpc-integration"><a class="header" href="#6-tinykv-rpc-integration">6. TinyKv RPC Integration</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>RPC</th><th>Execution Path</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td><code>KvGet</code> / <code>KvScan</code></td><td><code>ReadCommand</code> â†’ <code>kv.Apply</code> (read mode)</td><td>No raft round-trip; leader-only.</td></tr>
<tr><td><code>KvPrewrite</code> / <code>KvCommit</code> / <code>KvBatchRollback</code> / <code>KvResolveLock</code> / <code>KvCheckTxnStatus</code></td><td><code>ProposeCommand</code> â†’ command pipeline â†’ raft log â†’ <code>kv.Apply</code></td><td>Pipeline matches proposals with apply results; MVCC latch manager prevents write conflicts.</td></tr>
</tbody>
</table>
</div>
<p>The <code>cmd/nokv serve</code> command uses <code>raftstore.Server</code> internally and prints a manifest summary (key ranges, peers) so operators can verify the nodeâ€™s view at startup.</p>
<hr>
<h2 id="7-client-interaction-raftstoreclient"><a class="header" href="#7-client-interaction-raftstoreclient">7. Client Interaction (<code>raftstore/client</code>)</a></h2>
<ul>
<li>Region-aware routing with NotLeader/EpochNotMatch retry.</li>
<li><code>Mutate</code> splits mutations by region and performs two-phase commit (primary first). <code>Put</code> / <code>Delete</code> are convenience wrappers.</li>
<li><code>Scan</code> transparently walks region boundaries.</li>
<li>End-to-end coverage lives in <code>raftstore/server/server_client_integration_test.go</code>, which launches real servers, uses the client to write and delete keys, and verifies the results.</li>
</ul>
<hr>
<h2 id="8-control-plane--region-operations"><a class="header" href="#8-control-plane--region-operations">8. Control Plane &amp; Region Operations</a></h2>
<h3 id="81-topology--routing"><a class="header" href="#81-topology--routing">8.1 Topology &amp; Routing</a></h3>
<ul>
<li>Topology is sourced from <code>raft_config.example.json</code> (via <code>config.LoadFile</code>) and
reused by scripts, Docker Compose, and the Redis gateway.</li>
<li>The client builds a static region map (<code>[]RegionConfig</code>) and store endpoints
from the same file; there is no dynamic PD-style reconfiguration today.</li>
<li>The built-in scheduler currently emits leader-transfer operations only
(see <code>raftstore/scheduler</code>), acting as a minimal control plane.</li>
</ul>
<h3 id="82-split--merge"><a class="header" href="#82-split--merge">8.2 Split / Merge</a></h3>
<ul>
<li><strong>Split</strong>: leaders call <code>Store.ProposeSplit</code>, which writes a split
<code>AdminCommand</code> into the parent regionâ€™s raft log. On apply,
<code>Store.SplitRegion</code> updates the parent range/epoch and starts the child peer.</li>
<li><strong>Merge</strong>: leaders call <code>Store.ProposeMerge</code>, writing a merge <code>AdminCommand</code>.
On apply, the target region range/epoch is expanded and the source peer is
stopped/removed from the manifest.</li>
<li>These operations are explicit and are not auto-triggered by size/traffic
heuristics; a higher-level controller could call the same APIs.</li>
</ul>
<hr>
<h2 id="9-observability"><a class="header" href="#9-observability">9. Observability</a></h2>
<ul>
<li><code>store.RegionMetrics()</code> feeds into <code>StatsSnapshot</code>, making region counts and backlog visible via expvar and <code>nokv stats</code>.</li>
<li><code>nokv regions</code> shows manifest-backed regions: ID, range, peers, state.</li>
<li><code>scripts/transport_chaos.sh</code> exercises transport metrics under faults; <code>scripts/run_local_cluster.sh</code> spins up multi-node clusters for manual inspection.</li>
</ul>
<h3 id="store-internals-at-a-glance"><a class="header" href="#store-internals-at-a-glance">Store internals at a glance</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>File</th><th>Responsibility</th></tr>
</thead>
<tbody>
<tr><td>Peer set</td><td><a href="../raftstore/store/peer_set.go"><code>peer_set.go</code></a></td><td>Tracks active peers, synchronises router registration, exposes thread-safe lookups/iteration.</td></tr>
<tr><td>Command pipeline</td><td><a href="../raftstore/store/command_pipeline.go"><code>command_pipeline.go</code></a></td><td>Assigns request IDs, records proposals, matches apply results, returns responses/errors to callers.</td></tr>
<tr><td>Region manager</td><td><a href="../raftstore/store/region_manager.go"><code>region_manager.go</code></a></td><td>Validates state transitions, writes manifest edits, updates peer metadata, triggers region hooks.</td></tr>
<tr><td>Operation scheduler</td><td><a href="../raftstore/store/operation_scheduler.go"><code>operation_scheduler.go</code></a></td><td>Buffers planner output, enforces cooldown &amp; burst limits, dispatches leader transfers or other operations.</td></tr>
<tr><td>Heartbeat loop</td><td><a href="../raftstore/store/heartbeat_loop.go"><code>heartbeat_loop.go</code></a></td><td>Periodically publishes region/store heartbeats and re-runs the planner to produce scheduling actions.</td></tr>
<tr><td>Global registry</td><td><a href="../raftstore/store/global.go"><code>global.go</code></a></td><td>Records live stores for CLI/scripting (<code>Store.Close()</code> automatically unregisters instances).</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="10-extending-raftstore"><a class="header" href="#10-extending-raftstore">10. Extending raftstore</a></h2>
<ul>
<li><strong>Adding peers</strong>: update the manifest with new Region metadata, then call <code>Store.StartPeer</code> on the target node.</li>
<li><strong>Follower or lease reads</strong>: extend <code>ReadCommand</code> to include ReadIndex or leader lease checks; current design only serves leader reads.</li>
<li><strong>Scheduler integration</strong>: pair <code>RegionSnapshot()</code> and <code>RegionMetrics()</code> with an external scheduler (PD-like) for dynamic balancing.</li>
</ul>
<p>This layering keeps the embedded storage engine intact while providing a production-ready replication path, robust observability, and straightforward integration in both CLI and programmatic contexts.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="crash-recovery-playbook"><a class="header" href="#crash-recovery-playbook">Crash Recovery Playbook</a></h1>
<p>This playbook documents how NoKV rebuilds state after a crash and which automated checks ensure correctness. It ties together WAL replay, manifest reconciliation, ValueLog GC, and flush pipelinesâ€”mirroring RocksDBâ€™s layered recovery while incorporating Badger-style value log hygiene.</p>
<hr>
<h2 id="1-recovery-phases"><a class="header" href="#1-recovery-phases">1. Recovery Phases</a></h2>
<pre class="mermaid">flowchart TD
    Start[DB.Open]
    Verify[runRecoveryChecks]
    Manifest[manifest.Open â†’ replay]
    WAL[wal.Manager.Replay]
    VLog[valueLog.recover]
    Flush[Recreate memtables]
    Stats[Stats.StartStats]

    Start --&gt; Verify --&gt; Manifest --&gt; WAL --&gt; VLog --&gt; Flush --&gt; Stats
</pre>

<ol>
<li><strong>Directory verification</strong> â€“ <code>DB.runRecoveryChecks</code> calls <code>manifest.Verify</code>, <code>wal.VerifyDir</code>, and initialises the vlog directory. Missing directories fail fast.</li>
<li><strong>Manifest replay</strong> â€“ <code>manifest.Open</code> reads <code>CURRENT</code>, replays <code>EditAddFile/DeleteFile</code>, <code>EditLogPointer</code>, and vlog edits into an in-memory <code>Version</code>.</li>
<li><strong>WAL replay</strong> â€“ <code>wal.Manager.Replay</code> processes segments newer than the manifest checkpoint, rebuilding memtables from committed entries.</li>
<li><strong>ValueLog reconciliation</strong> â€“ <code>valueLog.recover</code> scans existing <code>.vlog</code> files, drops segments marked invalid, and trims torn tails to the last valid entry.</li>
<li><strong>Flush backlog</strong> â€“ Immutable memtables recreated from WAL are resubmitted to <code>flush.Manager</code>; temporary <code>.sst.tmp</code> files are either reinstalled or cleaned up.</li>
<li><strong>Stats bootstrap</strong> â€“ the metrics goroutine restarts so CLI commands immediately reflect queue backlogs and GC status.</li>
</ol>
<p>This mirrors RocksDBâ€™s <code>DBImpl::Recover</code> while extending to handle value log metadata automatically.</p>
<hr>
<h2 id="2-failure-scenarios--expected-outcomes"><a class="header" href="#2-failure-scenarios--expected-outcomes">2. Failure Scenarios &amp; Expected Outcomes</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Failure Point</th><th>Example Simulation</th><th>Expected Recovery Behaviour</th><th>Tests</th></tr>
</thead>
<tbody>
<tr><td>WAL tail truncation</td><td>truncate last 2 bytes of <code>000005.wal</code></td><td>Replay stops at truncated record, previously flushed SST remains intact</td><td><code>wal/manager_test.go::TestReplayTruncatedTail</code></td></tr>
<tr><td>Flush crash before install</td><td>crash after writing <code>.sst.tmp</code></td><td>WAL replay rebuilds memtable; temp file removed; no manifest edit present</td><td><code>db_recovery_test.go::TestRecoveryWALReplayRestoresData</code></td></tr>
<tr><td>Flush crash after install</td><td>crash after logging manifest edit but before WAL release</td><td>Manifest still lists SST; recovery verifies file exists and releases WAL on reopen</td><td><code>db_recovery_test.go::TestRecoveryCleansMissingSSTFromManifest</code></td></tr>
<tr><td>ValueLog GC crash</td><td>delete edit written, file still on disk</td><td>Recovery removes stale <code>.vlog</code> file and keeps manifest consistent</td><td><code>db_recovery_test.go::TestRecoveryRemovesStaleValueLogSegment</code></td></tr>
<tr><td>Manifest rewrite crash</td><td>new MANIFEST written, CURRENT not updated</td><td>Recovery keeps using old manifest; stale temp file cleaned</td><td><code>db_recovery_test.go::TestRecoveryManifestRewriteCrash</code></td></tr>
<tr><td>Transaction in-flight</td><td>crash between WAL append and memtable update</td><td>WAL replay reapplies entry; transactions remain atomic because commit order is vlog â†’ WAL â†’ memtable</td><td><code>txn_test.go::TestTxnCommitPersists</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="3-automation--tooling"><a class="header" href="#3-automation--tooling">3. Automation &amp; Tooling</a></h2>
<h3 id="31-go-test-matrix"><a class="header" href="#31-go-test-matrix">3.1 Go Test Matrix</a></h3>
<pre><code class="language-bash">GOCACHE=$PWD/.gocache GOMODCACHE=$PWD/.gomodcache go test ./... -run 'Recovery'
</code></pre>
<ul>
<li>Exercises WAL replay, manifest cleanup, vlog GC, and managed transaction recovery.</li>
<li>Set <code>RECOVERY_TRACE_METRICS=1</code> to emit structured logs (key/value pairs) for each scenario.</li>
</ul>
<h3 id="32-shell-script-harness"><a class="header" href="#32-shell-script-harness">3.2 Shell Script Harness</a></h3>
<p><code>scripts/recovery_scenarios.sh</code> orchestrates the matrix end-to-end:</p>
<ol>
<li>Spins up a temporary database, injects writes, and crashes at chosen checkpoints.</li>
<li>Reopens the database and validates via CLI (<code>nokv stats</code>, <code>nokv manifest</code>, <code>nokv vlog</code>).</li>
<li>Archives logs under <code>artifacts/recovery/&lt;scenario&gt;.log</code> for CI inspection.</li>
</ol>
<h3 id="33-cli-validation"><a class="header" href="#33-cli-validation">3.3 CLI Validation</a></h3>
<ul>
<li><code>nokv manifest --workdir &lt;dir&gt;</code>: confirm WAL checkpoint, level files, vlog head.</li>
<li><code>nokv stats --workdir &lt;dir&gt;</code>: observe flush backlog drop to zero after replay.</li>
<li><code>nokv vlog --workdir &lt;dir&gt;</code>: ensure stale segments disappear after GC recovery.</li>
</ul>
<p>These commands give the same insight as RocksDBâ€™s <code>ldb manifest_dump</code> or Badgerâ€™s CLI but with JSON output for automation.</p>
<hr>
<h2 id="4-metrics-emitted-during-recovery"><a class="header" href="#4-metrics-emitted-during-recovery">4. Metrics Emitted During Recovery</a></h2>
<p>When <code>RECOVERY_TRACE_METRICS=1</code>:</p>
<ul>
<li><code>RECOVERY_METRIC phase="manifest" ...</code> â€“ manifest replay progress.</li>
<li><code>RECOVERY_METRIC phase="wal" segment=... offset=...</code> â€“ WAL records applied.</li>
<li><code>RECOVERY_METRIC phase="vlog_gc" fid=... action="delete"</code> â€“ vlog cleanup status.</li>
</ul>
<p><code>StatsSnapshot</code> also exposes:</p>
<ul>
<li><code>NoKV.Flush.Queue</code> â€“ remaining flush tasks.</li>
<li><code>NoKV.ValueLog.HeadFID</code> â€“ head file after recovery.</li>
<li><code>NoKV.Txns.Active</code> â€“ should reset to zero post-recovery.</li>
</ul>
<hr>
<h2 id="5-comparison-with-rocksdb--badger"><a class="header" href="#5-comparison-with-rocksdb--badger">5. Comparison with RocksDB &amp; Badger</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>RocksDB</th><th>BadgerDB</th><th>NoKV</th></tr>
</thead>
<tbody>
<tr><td>WAL replay</td><td><code>DBImpl::RecoverLogFiles</code> replays per log number</td><td>Journal (value log) is replayed into LSM</td><td>Dedicated WAL manager with manifest checkpoint, plus vlog trim</td></tr>
<tr><td>Manifest reconciliation</td><td>Removes missing files, handles CURRENT rewrite</td><td>Minimal manifest (mainly tables)</td><td>Tracks SST + vlog metadata; auto-cleans missing SST/vlog</td></tr>
<tr><td>Value log recovery</td><td>Optional (BlobDB) requires external blob manifest</td><td>Primary log, re-scanned on start</td><td>Manifest-backed head + discard stats to avoid rescan</td></tr>
<tr><td>Tooling</td><td><code>ldb</code> for manifest dump</td><td><code>badger</code> CLI</td><td><code>nokv</code> CLI with JSON output</td></tr>
</tbody>
</table>
</div>
<p>NoKV inherits RocksDBâ€™s strict manifest semantics and Badgerâ€™s value log durability, yielding deterministic restart behaviour even under mixed workloads.</p>
<hr>
<h2 id="6-extending-the-matrix"><a class="header" href="#6-extending-the-matrix">6. Extending the Matrix</a></h2>
<p>Future enhancements to cover:</p>
<ul>
<li><strong>Compaction crash</strong> â€“ simulate partial compaction output and verify manifest rollback.</li>
<li><strong>Prefetch queue state</strong> â€“ ensure hot-key prefetch map resets cleanly.</li>
<li><strong>Raft integration</strong> â€“ once replication is added, validate raft log catch-up interacts correctly with WAL replay.</li>
</ul>
<p>Contributions adding new recovery scenarios should update this document and the shell harness to keep observability aligned.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="stats--observability-pipeline"><a class="header" href="#stats--observability-pipeline">Stats &amp; Observability Pipeline</a></h1>
<p>NoKV exposes internal health via the Go <code>expvar</code> package and the <code>nokv stats</code> CLI. The statistics subsystem is implemented in <a href="../stats.go"><code>stats.go</code></a> and runs continuously once the DB is open.</p>
<hr>
<h2 id="1-architecture"><a class="header" href="#1-architecture">1. Architecture</a></h2>
<pre class="mermaid">flowchart TD
    subgraph Collectors
        Flush[lsm.FlushMetrics]
        Levels[lsm.CompactionStats]
        VLog[valueLog.metrics]
        WAL[wal.Manager.Metrics]
        Txn[oracle.txnMetricsSnapshot]
        Cache[lsm.CacheMetrics]
        Hot[hotring.TopN]
    end
    Collectors --&gt; Stats
    Stats --&gt;|expvar publish| Runtime
    Stats --&gt;|Snapshot| CLI
</pre>

<ul>
<li><code>newStats</code> wires together reusable <code>expvar.Int/Float</code> gauges (avoiding duplicates if the process restarts an embedded DB).</li>
<li><code>Stats.StartStats</code> launches a goroutine that ticks every 5s (configurable via <code>Stats.interval</code>) to refresh values.</li>
<li><code>Stats.Snapshot</code> can be called on-demand (e.g. CLI) without mutating expvar state.</li>
</ul>
<hr>
<h2 id="2-snapshot-fields"><a class="header" href="#2-snapshot-fields">2. Snapshot Fields</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Source</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>Entries</code></td><td><code>lsm.EntryCount()</code></td><td>Total MVCC entries (L0-Ln + memtables). Mirrors <code>Stats.EntryNum</code> for backwards compat.</td></tr>
<tr><td><code>FlushPending/Queue/Active</code></td><td><code>lsm.FlushMetrics()</code></td><td>Pending immutables, queue length, workers currently building SSTs.</td></tr>
<tr><td><code>FlushWait/Build/ReleaseMs</code></td><td>Derived from <code>WaitNs/BuildNs/ReleaseNs</code> averages</td><td>End-to-end latency of flush pipeline stages.</td></tr>
<tr><td><code>CompactionBacklog/MaxScore</code></td><td><code>lsm.CompactionStats()</code></td><td>How many level files await compaction and the hottest score.</td></tr>
<tr><td><code>ValueLogSegments/PendingDel/DiscardQueue/Head</code></td><td><code>valueLog.metrics()</code></td><td>Tracks vlog utilisation and GC backlog.</td></tr>
<tr><td><code>WALActiveSegment/SegmentCount/Removed/ActiveSize</code></td><td><code>wal.Manager.Metrics()</code></td><td>Observes WAL rotation cadence and current segment byte usage (pairs with raft lag metrics).</td></tr>
<tr><td><code>WALTypedRecordRatio/Warning/Reason</code></td><td>WAL backlog watchdog (<code>Stats.Snapshot</code>)</td><td>Tracks ratio of raft typed records in the WAL and surfaces warnings with reasons when exceeding thresholds.</td></tr>
<tr><td><code>WALAutoGCRuns/Removed/LastUnix</code></td><td>WAL backlog watchdog</td><td>Automated WAL GC passes, total segments removed, and the Unix timestamp of the last run.</td></tr>
<tr><td><code>WriteQueueDepth/Entries/Bytes</code></td><td><code>writeMetrics.snapshot()</code></td><td>Size of the asynchronous write queue.</td></tr>
<tr><td><code>WriteAvg*</code></td><td><code>writeMetrics</code> averages</td><td>Request wait times, vlog latency, apply latency.</td></tr>
<tr><td><code>WriteBatchesTotal</code></td><td><code>writeMetrics</code></td><td>Lifetime batches processed.</td></tr>
<tr><td><code>HotWriteLimited</code></td><td><code>db.hotWriteLimited</code></td><td>Number of write attempts rejected by <code>Options.WriteHotKeyLimit</code> (HotRing write throttling).</td></tr>
<tr><td><code>WriteThrottleActive</code></td><td><code>db.blockWrites</code></td><td>Indicates when writes are being throttled.</td></tr>
<tr><td><code>TxnsActive/Started/Committed/Conflicts</code></td><td><code>oracle.txnMetricsSnapshot()</code></td><td>MVCC activity counters.</td></tr>
<tr><td><code>HotKeys</code></td><td><code>hotring.TopN()</code></td><td>Top-K hot key counts.</td></tr>
<tr><td><code>BlockL0/L1/BloomHitRate</code></td><td><code>lsm.CacheMetrics()</code></td><td>Block and bloom cache hit ratios.</td></tr>
<tr><td><code>IndexHitRate</code></td><td><code>lsm.CacheMetrics()</code></td><td>SST ç´¢å¼•å—ç¼“å­˜å‘½ä¸­ç‡ã€‚</td></tr>
<tr><td><code>IteratorReused</code></td><td><code>iteratorPool.reused()</code></td><td>Frequency of iterator pooling hits.</td></tr>
<tr><td><code>RaftGroupCount/LaggingGroups/MaxLagSegments/LagWarnThreshold/RaftLagWarning</code></td><td><code>manifest.RaftPointerSnapshot()</code></td><td>Tracks follower backlogs; <code>LagWarnThreshold</code> comes from <code>Options.RaftLagWarnSegments</code>, and <code>RaftLagWarning</code> toggles when any group exceeds it.</td></tr>
<tr><td><code>RegionTotal/New/Running/Removing/Tombstone/Other</code></td><td><code>store.RegionMetrics</code></td><td>Multi-Raft region state distribution. CLI attaches the first available <code>RegionMetrics</code> by default; pass <code>--no-region-metrics</code> to disable.</td></tr>
</tbody>
</table>
</div>
<p>All values are exported under the <code>NoKV.*</code> namespace via expvar (see <code>newStats</code>).</p>
<hr>
<h2 id="3-cli--json-output"><a class="header" href="#3-cli--json-output">3. CLI &amp; JSON Output</a></h2>
<ul>
<li><code>nokv stats --workdir &lt;dir&gt;</code> prints a human-readable table (queue lengths, throughput, hot keys, region totals). It automatically attaches <code>RegionMetrics</code> when available; add <code>--no-region-metrics</code> to produce a manifest-only snapshot.</li>
<li>When <code>RaftLagWarning=true</code> the CLI emits an extra <code>Raft.Warning</code> line; it also surfaces <code>Regions.Total (...)</code> so operators can quickly gauge Region lifecycle health.</li>
<li><code>nokv stats --json</code> emits the raw snapshot for automation. Example snippet:</li>
</ul>
<pre><code class="language-json">{
  "entries": 1048576,
  "flush_queue_length": 2,
  "vlog_head": {"fid": 5, "offset": 184320},
  "hot_keys": [{"key": "user:123", "count": 42}]
}
</code></pre>
<p>The CLI internally instantiates a read-only DB handle, calls <code>Stats.Snapshot</code>, and formats the responseâ€”no background goroutine is needed.</p>
<hr>
<h2 id="4-integration-with-other-modules"><a class="header" href="#4-integration-with-other-modules">4. Integration with Other Modules</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Contribution</th></tr>
</thead>
<tbody>
<tr><td>WAL</td><td><code>wal.Manager.Metrics()</code> counts active/removable segments, aiding post-recovery validation.</td></tr>
<tr><td>Value Log</td><td><code>valueLog.metrics()</code> exposes GC backlog, enabling alerting when discard queues stall.</td></tr>
<tr><td>HotRing</td><td>Publishes hot key JSON via expvar so dashboards can visualise top offenders.</td></tr>
<tr><td>Transactions</td><td>Oracle counters help gauge contention (high conflicts â†’ tune workload).</td></tr>
<tr><td>Cache</td><td>Hit rates clarify whether cache sizing (hot/cold tier) needs adjustment.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="5-comparisons-2"><a class="header" href="#5-comparisons-2">5. Comparisons</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Engine</th><th>Observability</th></tr>
</thead>
<tbody>
<tr><td>RocksDB</td><td><code>iostats</code>, <code>perf_context</code>, <code>ldb</code> commands. Requires manual parsing.</td></tr>
<tr><td>Badger</td><td>Prometheus metrics (optional).</td></tr>
<tr><td>NoKV</td><td>Built-in expvar gauges + CLI + recovery trace toggles.</td></tr>
</tbody>
</table>
</div>
<p>NoKV emphasises zero-dependency observability. Everything is consumable via HTTP <code>/debug/vars</code> or the CLI, making it easy to integrate with Go services.</p>
<hr>
<h2 id="6-operational-guidance"><a class="header" href="#6-operational-guidance">6. Operational Guidance</a></h2>
<ul>
<li>Watch <code>FlushQueueLength</code> and <code>CompactionBacklog</code> togetherâ€”if both grow, increase flush workers or adjust level sizes.</li>
<li><code>ValueLogDiscardQueue &gt; 0</code> for extended periods indicates GC is blocked; inspect <code>NoKV.ValueLog.GcRuns</code> and consider tuning thresholds.</li>
<li><code>WriteThrottleActive</code> toggling frequently suggests L0 is overwhelmed; cross-check <code>BlockL0HitRate</code> and compaction metrics.</li>
<li><code>HotWriteLimited</code> climbing steadily means HotRing write throttling is firingâ€”surface <code>utils.ErrHotKeyWriteThrottle</code> to clients and investigate abusive keys via the <code>HotKeys</code> list.</li>
<li><code>RaftLagWarning</code> toggling to <code>true</code> means at least one follower lags the leader by more than <code>Options.RaftLagWarnSegments</code>; inspect <code>Raft.Warning</code> from the CLI and consider snapshot resend or throttling the offending node.</li>
<li><code>Regions.Total</code> should match the expected cluster topology; sustained <code>Removing/Tombstone</code> counts indicate stalled cleanupâ€”investigate split/merge logic or stuck replicas.</li>
</ul>
<p>Refer to <a href="#4-observability-in-tests"><code>docs/testing.md</code></a> for scripted checks that validate stats during CI runs.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="testing--validation-matrix"><a class="header" href="#testing--validation-matrix">Testing &amp; Validation Matrix</a></h1>
<p>This document inventories NoKVâ€™s automated coverage and provides guidance for extending tests. It aligns module-level unit tests, integration suites, and benchmarking harnesses with the architectural features described elsewhere.</p>
<hr>
<h2 id="1-quick-commands"><a class="header" href="#1-quick-commands">1. Quick Commands</a></h2>
<pre><code class="language-bash"># All unit + integration tests (uses local module caches)
GOCACHE=$PWD/.gocache GOMODCACHE=$PWD/.gomodcache go test ./...

# Focused transaction suite
go test ./... -run '^TestTxn|TestConflict|TestTxnIterator'

# Crash recovery scenarios
RECOVERY_TRACE_METRICS=1 ./scripts/recovery_scenarios.sh

# gRPC transport chaos tests + watchdog metrics
CHAOS_TRACE_METRICS=1 ./scripts/transport_chaos.sh

# Sample timestamp allocator (TSO) for multi-client transaction tests
go run ./scripts/tso --addr 127.0.0.1:9494 --start 100

# Local three-node cluster (includes manifest bootstrap + optional TSO)
./scripts/run_local_cluster.sh --config ./raft_config.example.json
# Tear down with Ctrl+C

# Docker-compose sandbox (3 nodes + TSO)
docker compose up --build
docker compose down -v

# Build RocksDB locally (installs into ./third_party/rocksdb/dist by default)
./scripts/build_rocksdb.sh
# YCSB baseline (records=1e6, ops=1e6, warmup=1e5, conc=16)
./scripts/run_benchmarks.sh
# YCSB with RocksDB (requires CGO, `benchmark_rocksdb`, and the RocksDB build above)
LD_LIBRARY_PATH="$(pwd)/third_party/rocksdb/dist/lib:${LD_LIBRARY_PATH}" \
CGO_CFLAGS="-I$(pwd)/third_party/rocksdb/dist/include" \
CGO_LDFLAGS="-L$(pwd)/third_party/rocksdb/dist/lib -lrocksdb -lz -lbz2 -lsnappy -lzstd -llz4" \
YCSB_ENGINES="nokv,badger,rocksdb" ./scripts/run_benchmarks.sh
# One-click script (auto-detect RocksDB, supports `YCSB_*` env vars to override defaults)
./scripts/run_benchmarks.sh
# Quick smoke run (smaller dataset)
NOKV_RUN_BENCHMARKS=1 YCSB_RECORDS=10000 YCSB_OPS=50000 YCSB_WARM_OPS=0 \
./scripts/run_benchmarks.sh -ycsb_workloads=A -ycsb_engines=nokv
</code></pre>
<blockquote>
<p>Tip: Pin <code>GOCACHE</code>/<code>GOMODCACHE</code> in CI to keep build artefacts local and avoid permission issues.</p>
</blockquote>
<hr>
<h2 id="2-module-coverage-overview"><a class="header" href="#2-module-coverage-overview">2. Module Coverage Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Module</th><th>Tests</th><th>Coverage Highlights</th><th>Gaps / Next Steps</th></tr>
</thead>
<tbody>
<tr><td>WAL</td><td><code>wal/manager_test.go</code></td><td>Segment rotation, sync semantics, replay tolerance for truncation, directory bootstrap.</td><td>Add IO fault injection, concurrent append stress.</td></tr>
<tr><td>LSM / Flush / Compaction</td><td><code>lsm/lsm_test.go</code>, <code>lsm/compact_test.go</code>, <code>lsm/flush/*_test.go</code></td><td>Memtable correctness, iterator merging, flush pipeline metrics, compaction scheduling.</td><td>Extend backpressure assertions, test cache hot/cold split.</td></tr>
<tr><td>Manifest</td><td><code>manifest/manager_test.go</code>, <code>manifest/levels_test.go</code></td><td>CURRENT swap safety, rewrite crash handling, vlog metadata persistence.</td><td>Simulate partial edit corruption, column family extensions.</td></tr>
<tr><td>ValueLog</td><td><code>vlog/vlog_test.go</code>, <code>vlog/gc_test.go</code></td><td>ValuePtr encoding/decoding, GC rewrite, concurrent iterator safety.</td><td>Long-running GC with transactions, discard ratio edge cases.</td></tr>
<tr><td>Transactions / Oracle</td><td><code>txn_test.go</code>, <code>txn_iterator_test.go</code>, <code>txn_metrics_test.go</code></td><td>MVCC timestamps, conflict detection, iterator snapshots, metrics accounting.</td><td>Mixed workload fuzzing, managed transactions with TTL.</td></tr>
<tr><td>DB Integration</td><td><code>db_test.go</code>, <code>db_recovery_test.go</code>, <code>db_recovery_managed_test.go</code></td><td>End-to-end writes, recovery, managed vs. unmanaged transactions, throttle behaviour.</td><td>Combine ValueLog GC + compaction stress, multi-DB interference.</td></tr>
<tr><td>CLI &amp; Stats</td><td><code>cmd/nokv/main_test.go</code>, <code>stats_test.go</code></td><td>Golden JSON output, stats snapshot correctness, hot key ranking.</td><td>CLI error handling, expvar HTTP integration tests.</td></tr>
<tr><td>Redis Gateway</td><td><code>cmd/nokv-redis/backend_embedded_test.go</code>, <code>cmd/nokv-redis/server_test.go</code>, <code>cmd/nokv-redis/backend_raft_test.go</code></td><td>Embedded backend semantics (NX/XX, TTL, counters), RESP parser, raft backend config wiring &amp; TSO discovery.</td><td>End-to-end multi-region CRUD with raft backend, TTL lock cleanup under failures.</td></tr>
<tr><td>Scripts &amp; Tooling</td><td><code>scripts/scripts_test.go</code>, <code>cmd/nokv-config/main_test.go</code></td><td><code>serve_from_config.sh</code> address scoping (host/docker) and manifest skipping, <code>nokv-config</code> JSON/simple formats, manifest logging CLI.</td><td>Golden coverage for <code>run_local_cluster.sh</code>, failure-path diagnostics.</td></tr>
<tr><td>Benchmark</td><td><code>benchmark/ycsb_test.go</code>, <code>benchmark/ycsb_runner.go</code></td><td>YCSB throughput/latency comparisons across engines with detailed percentile + operation mix reporting.</td><td>Automate multi-node deployments, add more workloads (D/E/F) and multi-GB datasets.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="3-system-scenarios"><a class="header" href="#3-system-scenarios">3. System Scenarios</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scenario</th><th>Coverage</th><th>Focus</th></tr>
</thead>
<tbody>
<tr><td>Crash recovery</td><td><code>db_recovery_test.go</code>, <code>scripts/recovery_scenarios.sh</code></td><td>WAL replay, missing SST cleanup, vlog GC restart, manifest rewrite safety.</td></tr>
<tr><td>WAL pointer desync</td><td><code>raftstore/engine/wal_storage_test.go::TestWALStorageDetectsTruncatedSegment</code></td><td>Detects manifest pointer offsets beyond truncated WAL tails to avoid silent corruption.</td></tr>
<tr><td>Transaction contention</td><td><code>TestConflict</code>, <code>TestTxnReadAfterWrite</code>, <code>TestTxnDiscard</code></td><td>Oracle watermark handling, conflict errors, managed commit path.</td></tr>
<tr><td>Value separation + GC</td><td><code>vlog/gc_test.go</code>, <code>db_recovery_test.go::TestRecoveryRemovesStaleValueLogSegment</code></td><td>GC correctness, manifest integration, iterator stability.</td></tr>
<tr><td>Iterator consistency</td><td><code>txn_iterator_test.go</code>, <code>lsm/iterator_test.go</code></td><td>Snapshot visibility, merging iterators across levels and memtables.</td></tr>
<tr><td>Throttling / backpressure</td><td><code>lsm/compact_test.go</code>, <code>db_test.go::TestWriteThrottle</code></td><td>L0 backlog triggers, flush queue growth, metrics observation.</td></tr>
<tr><td>Distributed TinyKv client</td><td><code>raftstore/client/client_test.go::TestClientTwoPhaseCommitAndGet</code>, <code>raftstore/transport/grpc_transport_test.go::TestGRPCTransportManualTicksDriveElection</code></td><td>Region-aware routing, NotLeader retries, manual tick-driven elections, cross-region 2PC sequencing.</td></tr>
<tr><td>Performance regression</td><td><code>benchmark</code> package</td><td>Compare NoKV vs Badger/RocksDB, produce human-readable reports under <code>benchmark/benchmark_results</code>.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="4-observability-in-tests"><a class="header" href="#4-observability-in-tests">4. Observability in Tests</a></h2>
<ul>
<li><strong>RECOVERY_METRIC logs</strong> â€“ produced when <code>RECOVERY_TRACE_METRICS=1</code>; consumed by recovery script and helpful when triaging CI failures.</li>
<li><strong>TRANSPORT_METRIC logs</strong> â€“ emitted by <code>scripts/transport_chaos.sh</code> when <code>CHAOS_TRACE_METRICS=1</code>, capturing gRPC watchdog counters during network partitions and retries.</li>
<li><strong>Stats snapshots</strong> â€“ <code>stats_test.go</code> verifies JSON structure so CLI output remains backwards compatible.</li>
<li><strong>Benchmark artefacts</strong> â€“ stored under <code>benchmark/benchmark_results/*.txt</code> for historical comparison. Aligns with README instructions.</li>
</ul>
<hr>
<h2 id="5-extending-coverage"><a class="header" href="#5-extending-coverage">5. Extending Coverage</a></h2>
<ol>
<li><strong>Property-based testing</strong> â€“ integrate <code>testing/quick</code> or third-party generators to randomise transaction sequences (Badger uses similar fuzz tests for transaction ordering).</li>
<li><strong>Stress harness</strong> â€“ add a Go-based stress driver to run mixed read/write workloads for hours, capturing metrics akin to RocksDBâ€™s <code>db_stress</code> tool.</li>
<li><strong>Distributed readiness</strong> â€“ when Raft or replication is introduced, craft tests that validate WAL shipping combined with manifest updates.</li>
<li><strong>CLI smoke tests</strong> â€“ simulate corrupted directories to ensure CLI emits actionable errors.</li>
</ol>
<p>Keep this matrix updated when adding new modules or scenarios so documentation and automation remain aligned.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scripts-overview"><a class="header" href="#scripts-overview">Scripts Overview</a></h1>
<p>NoKV ships a small collection of helper scripts to streamline local experimentation, demos, diagnostics, and automation. This page summarises what each script does, how to use it, and which shared configuration it consumes.</p>
<hr>
<h2 id="cluster-helpers"><a class="header" href="#cluster-helpers">Cluster helpers</a></h2>
<h3 id="scriptsrun_local_clustersh"><a class="header" href="#scriptsrun_local_clustersh"><code>scripts/run_local_cluster.sh</code></a></h3>
<ul>
<li><strong>Purpose</strong> â€“ builds <code>nokv</code>, <code>nokv-config</code>, and <code>nokv-tso</code>, reads <code>raft_config.json</code>, seeds manifests, and starts the TinyKv nodes (plus TSO when configured). If a store directory already contains a manifest (<code>CURRENT</code>), the seeding step is skipped so previously bootstrapped data is reused.</li>
<li><strong>Usage</strong>
<pre><code class="language-bash">./scripts/run_local_cluster.sh --config ./raft_config.example.json --workdir ./artifacts/cluster
</code></pre>
</li>
</ul>
<p><code>--config</code> defaults to the repositoryâ€™s <code>raft_config.example.json</code>; <code>--workdir</code> chooses the data root (<code>./artifacts/cluster</code> by default). For every entry under <code>stores</code> the script creates <code>store-&lt;id&gt;</code>, calls <code>nokv-config manifest</code>, and, if <code>tso.listen_addr</code> is set, launches <code>nokv-tso</code>. The script runs in the foregroundâ€”press <code>Ctrl+C</code> to stop all spawned processes.</p>
<blockquote>
<p>â—ï¸ <strong>Shutdown / restart note</strong> â€” To avoid WAL/manifest mismatches, always stop the script with <code>Ctrl+C</code> and wait for the <code>Shutting down...</code> message. If you crash the process or the host, clean the workdir (<code>rm -rf ./artifacts/cluster</code>) before starting again; otherwise the replay step may panic when it encounters truncated WAL segments.</p>
</blockquote>
<h3 id="scriptsbootstrap_from_configsh"><a class="header" href="#scriptsbootstrap_from_configsh"><code>scripts/bootstrap_from_config.sh</code></a></h3>
<ul>
<li><strong>Purpose</strong> â€“ manifest-only bootstrap, typically used in Docker Compose before the nodes start. Stores that already hold a manifest are detected and skipped.</li>
<li><strong>Usage</strong>
<pre><code class="language-bash">./scripts/bootstrap_from_config.sh --config /etc/nokv/raft_config.json --path-template /data/store-{id}
</code></pre>
The script iterates over every store in the config and writes Region metadata via <code>nokv-config manifest</code> into the provided path template.</li>
</ul>
<h3 id="scriptsserve_from_configsh"><a class="header" href="#scriptsserve_from_configsh"><code>scripts/serve_from_config.sh</code></a></h3>
<ul>
<li><strong>Purpose</strong> â€“ translate <code>raft_config.json</code> into a <code>nokv serve</code> command, avoiding manual <code>--peer</code> lists. It resolves peer IDs from the region metadata and maps every peer (other than the local store) to its advertised address so that gRPC transport works out of the box.</li>
<li><strong>Usage</strong>
<pre><code class="language-bash">./scripts/serve_from_config.sh \
    --config ./raft_config.json \
    --store-id 1 \
    --workdir ./artifacts/cluster/store-1 \
    --scope local   # use --scope docker inside containers
</code></pre>
<code>--scope</code> decides whether to use the local addresses or the container-friendly ones. The script assembles all peer mappings (excluding the local store) and execs <code>nokv serve</code>.</li>
</ul>
<hr>
<h2 id="diagnostics--benchmarking"><a class="header" href="#diagnostics--benchmarking">Diagnostics &amp; benchmarking</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Script</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>scripts/recovery_scenarios.sh</code></td><td>Runs crash-recovery scenarios across WAL/manifest/vlog. Set <code>RECOVERY_TRACE_METRICS=1</code> to collect metrics under <code>artifacts/recovery/</code>.</td></tr>
<tr><td><code>scripts/transport_chaos.sh</code></td><td>Injects disconnects/blocks/delay into the <code>raftstore</code> transport to observe behaviour under faulty networks.</td></tr>
<tr><td><code>scripts/run_benchmarks.sh</code></td><td>Executes the comparison benchmarks (NoKV vs Badger/RocksDB).</td></tr>
<tr><td><code>scripts/analyze_pprof.sh</code></td><td>Aggregates CPU/heap profiles from <code>pprof_output/</code> and renders SVG/PNG summaries.</td></tr>
<tr><td><code>scripts/debug.sh</code></td><td>Convenience wrapper around <code>dlv test</code> for targeted debugging.</td></tr>
<tr><td><code>scripts/gen.sh</code></td><td>Generates mock data or helper artefacts (see inline comments for details).</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="other-helpers"><a class="header" href="#other-helpers">Other helpers</a></h2>
<h3 id="scriptstso"><a class="header" href="#scriptstso"><code>scripts/tso</code></a></h3>
<p>A small Go program (not shell) that exposes an HTTP timestamp oracle:</p>
<pre><code class="language-bash">go run ./scripts/tso --addr 0.0.0.0:9494 --start 100
</code></pre>
<p><code>run_local_cluster.sh</code> and Docker Compose invoke it automatically when <code>tso.listen_addr</code> is present in the shared config.</p>
<hr>
<h2 id="relationship-with-nokv-config"><a class="header" href="#relationship-with-nokv-config">Relationship with <code>nokv-config</code></a></h2>
<ul>
<li><code>nokv-config stores</code> / <code>regions</code> / <code>tso</code> provide structured views over <code>raft_config.json</code>, making it easy for scripts and CI to query the topology.</li>
<li><code>nokv-config manifest</code> writes Region metadata into manifests and replaces the historical <code>manifestctl</code> binary.</li>
<li><code>cmd/nokv-redis</code> reads the same config; when <code>--tso-url</code> is omitted it falls back to the <code>tso</code> section.</li>
<li>Go tools or custom scripts can import <code>github.com/feichai0017/NoKV/config</code> and call <code>config.LoadFile</code> / <code>Validate</code> to consume the same <code>raft_config.json</code>, avoiding divergent schemas.</li>
</ul>
<p>Maintaining a single <code>raft_config.json</code> keeps local scripts, Docker Compose, Redis gateway, and automated tests aligned.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="redis-gateway"><a class="header" href="#redis-gateway">Redis Gateway</a></h1>
<p><code>cmd/nokv-redis</code> exposes NoKV through a RESP-compatible endpoint. The gateway reuses the engineâ€™s MVCC/transaction semantics and can operate in two modes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Description</th><th>Key flags</th></tr>
</thead>
<tbody>
<tr><td>Embedded (<code>embedded</code>)</td><td>Opens a local <code>*NoKV.DB</code> work directory. Commands (<code>SET</code>, <code>SET NX/XX</code>, <code>EX/PX/EXAT/PXAT</code>, <code>MSET</code>, <code>INCR/DECR</code>, <code>DEL</code>, <code>MGET</code>, <code>EXISTS</code>, â€¦) run inside <code>db.Update</code> / <code>db.View</code>, providing atomic single-key updates and snapshot reads across multiple keys.</td><td><code>--workdir &lt;dir&gt;</code></td></tr>
<tr><td>Raft (<code>raft</code>)</td><td>Routes requests through <code>raftstore/client</code> and a TinyKv cluster. Writes execute via TwoPhaseCommit; TTL metadata is stored under <code>!redis:ttl!&lt;key&gt;</code>. When <code>--tso-url</code> is omitted, the gateway consults the <code>tso</code> block in <code>raft_config.json</code> and falls back to a local oracle if the block is absent.</td><td><code>--raft-config &lt;file&gt;</code><br><code>--tso-url http://host:port</code> (optional)</td></tr>
</tbody>
</table>
</div>
<h2 id="usage-examples"><a class="header" href="#usage-examples">Usage examples</a></h2>
<h3 id="embedded-backend"><a class="header" href="#embedded-backend">Embedded backend</a></h3>
<pre><code class="language-bash">go run ./cmd/nokv-redis \
  --addr 127.0.0.1:6380 \
  --workdir ./work_redis \
  --metrics-addr 127.0.0.1:9100  # optional expvar endpoint
</code></pre>
<p>Validate with <code>redis-cli -p 6380 ping</code>. Metrics are exposed at <code>http://127.0.0.1:9100/debug/vars</code> under the <code>NoKV.Redis</code> key.</p>
<h3 id="raft-backend"><a class="header" href="#raft-backend">Raft backend</a></h3>
<ol>
<li>
<p>Start TinyKv and, if configured, the TSO using the helper script or Docker Compose. Both consume <code>raft_config.example.json</code>, initialise manifests for each store, and launch <code>nokv-tso</code> automatically when <code>tso.listen_addr</code> is present:</p>
<pre><code class="language-bash">./scripts/run_local_cluster.sh
# or: docker compose up --build
</code></pre>
</li>
<li>
<p>Run the gateway:</p>
<pre><code class="language-bash">go run ./cmd/nokv-redis \
  --addr 127.0.0.1:6380 \
  --raft-config raft_config.example.json
</code></pre>
<p>Supply <code>--tso-url</code> only when you need to override the config file; otherwise the gateway uses <code>tso.advertise_url</code> (or <code>listen_addr</code>) from the same JSON. If the block is missing, it falls back to the embedded timestamp oracle.</p>
</li>
</ol>
<h2 id="supported-commands"><a class="header" href="#supported-commands">Supported commands</a></h2>
<ul>
<li>String operations: <code>GET</code>, <code>SET</code>, <code>SET NX/XX</code>, <code>EX/PX/EXAT/PXAT</code>, <code>DEL</code>, <code>MGET</code>, <code>MSET</code>, <code>EXISTS</code></li>
<li>Integer operations: <code>INCR</code>, <code>DECR</code>, <code>INCRBY</code>, <code>DECRBY</code></li>
<li>Utility: <code>PING</code>, <code>ECHO</code>, <code>QUIT</code></li>
</ul>
<p>In both modes write commands are atomic. The Raft backend batches multi-key updates (<code>MSET</code>, <code>DEL</code>, â€¦) into a single TwoPhaseCommit, matching the embedded semantics. Reads use snapshot transactions locally (<code>db.View</code>) and leader reads with TTL checks remotely.</p>
<h2 id="configuration-file"><a class="header" href="#configuration-file">Configuration file</a></h2>
<p><code>raft_config.example.json</code> is shared by <code>scripts/run_local_cluster.sh</code>, Docker Compose, and the Redis gateway. Important fields:</p>
<ul>
<li><code>stores</code> â€“ store ID, gRPC address, and optional container listen/advertise addresses</li>
<li><code>regions</code> â€“ region ID, start/end keys (use <code>hex:&lt;bytes&gt;</code> for binary data), epoch, peer list, leader store ID</li>
<li><code>max_retries</code> â€“ maximum retries for region errors in the distributed client</li>
</ul>
<p>Use <code>nokv-config</code> to inspect or validate the configuration:</p>
<pre><code class="language-bash">nokv-config stores --config raft_config.json
nokv-config regions --config raft_config.json --format json | jq '.[] | {id:.id, peers:.peers}'
</code></pre>
<p>For Go tooling, import <code>github.com/feichai0017/NoKV/config</code> and call <code>config.LoadFile</code> / <code>Validate</code> to reuse the same schema and defaults across CLIs, scripts, and applications.</p>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<p>With <code>--metrics-addr</code> enabled the gateway publishes <code>NoKV.Redis</code> on <code>/debug/vars</code>, for example:</p>
<pre><code class="language-json">{
  "commands_total": 128,
  "errors_total": 0,
  "connections_active": 1,
  "connections_accepted": 4,
  "commands_per_operation": {
    "PING": 4,
    "SET": 32,
    "GET": 64,
    "MGET": 8,
    "DEL": 10,
    "INCR": 10
  }
}
</code></pre>
<p>These counters are part of the process-wide expvar output and can be scraped alongside the rest of NoKVâ€™s metrics.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="notes"><a class="header" href="#notes">Notes</a></h1>
<p>Use this folder to capture per-debug or per-investigation notes. Keep entries short, factual, and easy to skim.</p>
<h2 id="add-a-new-note"><a class="header" href="#add-a-new-note">Add a new note</a></h2>
<ol>
<li>Create a new file in <code>docs/notes/</code> named <code>YYYY-MM-DD-short-title.md</code>.</li>
<li>Add it to <code>docs/SUMMARY.md</code> under Notes.</li>
<li>Use the template below to keep entries consistent.</li>
</ol>
<h2 id="template"><a class="header" href="#template">Template</a></h2>
<h3 id="context"><a class="header" href="#context">Context</a></h3>
<h3 id="symptom"><a class="header" href="#symptom">Symptom</a></h3>
<h3 id="repro"><a class="header" href="#repro">Repro</a></h3>
<h3 id="investigation"><a class="header" href="#investigation">Investigation</a></h3>
<h3 id="root-cause"><a class="header" href="#root-cause">Root cause</a></h3>
<h3 id="fix"><a class="header" href="#fix">Fix</a></h3>
<h3 id="follow-ups"><a class="header" href="#follow-ups">Follow-ups</a></h3>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="2026-01-16-mmap-choice"><a class="header" href="#2026-01-16-mmap-choice">2026-01-16 mmap choice</a></h1>
<p>è¿™æ˜¯ä¸€äº›ç¢ç¢å¿µè®°å½•ï¼Œæƒ³æŠŠ mmap çš„é€‰æ‹©ç†ç”±å†™å¾—æ¸…æ¥šä¸€äº›ï¼Œå°¤å…¶æ˜¯å›´ç»• SSTable å’Œ VLog çš„å®šä¹‰ã€ä½¿ç”¨åœºæ™¯å’Œè¯»å†™äº¤äº’é€»è¾‘ã€‚</p>
<h2 id="æ¦‚å¿µä¸å®šä½"><a class="header" href="#æ¦‚å¿µä¸å®šä½">æ¦‚å¿µä¸å®šä½</a></h2>
<p>SSTable æ˜¯ LSM çš„æ ¸å¿ƒæŒä¹…åŒ–æ–‡ä»¶ï¼ŒæŒ‰ key æœ‰åºä¸”ä¸å¯å˜ï¼Œå†…éƒ¨ç”±ç´¢å¼•ã€æ•°æ®å—ä¸è¿‡æ»¤å™¨ç­‰ç»“æ„ç»„æˆï¼Œå› æ­¤å®ƒåœ¨è¯»è·¯å¾„é‡Œå‡ ä¹æ— å¤„ä¸åœ¨ã€‚VLog åˆ™ç”¨äºå­˜æ”¾è¾ƒå¤§çš„ valueï¼Œå†™å…¥æ—¶é¡ºåºè¿½åŠ ï¼ŒLSM å†…åªä¿å­˜ value pointerï¼Œè¯»å–æ—¶å†å›æŸ¥ VLogã€‚ç”¨ä¸€å¥è¯æ¦‚æ‹¬å°±æ˜¯ï¼šSSTable è¯»å¯†é›†ä¸”ä¸å¯å˜ï¼ŒVLog é¡ºåºå†™ä½†è¯»æ˜¯éšæœºçš„ã€‚</p>
<h2 id="è¯»å†™äº¤äº’é€»è¾‘"><a class="header" href="#è¯»å†™äº¤äº’é€»è¾‘">è¯»å†™äº¤äº’é€»è¾‘</a></h2>
<p>ä¸‹é¢è¿™å¼ å›¾å±•ç¤ºäº†å†™å…¥ä¸è¯»å–çš„ä¸»è¦äº¤äº’è·¯å¾„ï¼Œé‡ç‚¹æ˜¯è¯»è·¯å¾„å‡ ä¹ä¸€å®šè§¦è¾¾ SSTableï¼Œè€Œ VLog åªåœ¨ value å¤–ç½®æ—¶æ‰å‚ä¸ã€‚</p>
<pre class="mermaid">flowchart LR
  W[Write] --&gt; M[Memtable + WAL]
  M --&gt; C{Value large?}
  C -- no --&gt; I[Inline value]
  C -- yes --&gt; V[Append to VLog]
  V --&gt; P[Store ValuePtr]
  M --&gt; F[Flush/Compaction]
  F --&gt; S[SSTable]

  R[Read] --&gt; Q[Memtable/LSM search]
  Q --&gt; T{Inline?}
  T -- yes --&gt; U[Return value]
  T -- no --&gt; G[ValuePtr -&gt; VLog read]
  G --&gt; U
</pre>

<h2 id="io-æ–¹æ¡ˆå¯¹æ¯”çš„ç›´è§‚ç†è§£"><a class="header" href="#io-æ–¹æ¡ˆå¯¹æ¯”çš„ç›´è§‚ç†è§£">IO æ–¹æ¡ˆå¯¹æ¯”çš„ç›´è§‚ç†è§£</a></h2>
<p>mmap çš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯éšæœºè¯»æˆæœ¬ä½ï¼Œç³»ç»Ÿè°ƒç”¨å°‘ï¼Œè€Œä¸”è¯»å–å¯ä»¥ç›´æ¥è½åœ¨ OS çš„é¡µç¼“å­˜è·¯å¾„ä¸Šï¼›ä½†å®ƒçš„ç¼ºç‚¹ä¹Ÿå¾ˆæ˜ç¡®ï¼ŒRSS å’Œ page cache ä¸å¯æ§ï¼Œå†™å…¥å¿…é¡»å¤„ç†å¥½ msync è¯­ä¹‰ï¼Œå¹¶ä¸”è·¨å¹³å°ç»†èŠ‚å·®å¼‚è¾ƒå¤šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œpread æˆ– buffered read é…åˆè‡ªå»º cache æ›´å®¹æ˜“æ§åˆ¶å†…å­˜å’Œè¡Œä¸ºï¼Œä½†ä¼šå¼•å…¥é¢å¤–æ‹·è´å’Œç³»ç»Ÿè°ƒç”¨æˆæœ¬ã€‚direct I/O èƒ½ç»•è¿‡ page cacheï¼Œé¿å…æ±¡æŸ“ï¼Œä½†å·¥ç¨‹å¤æ‚åº¦é«˜ï¼Œå¹¶ä¸”åœ¨éšæœºè¯»åœºæ™¯å¹¶ä¸æ€»æ˜¯æ›´å¿«ã€‚</p>
<h2 id="ä¸ºä»€ä¹ˆ-sstable-æ›´é€‚åˆ-mmap"><a class="header" href="#ä¸ºä»€ä¹ˆ-sstable-æ›´é€‚åˆ-mmap">ä¸ºä»€ä¹ˆ SSTable æ›´é€‚åˆ mmap</a></h2>
<p>SSTable ä¸å¯å˜ä¸”è¯»å–é¢‘ç¹ï¼Œæ˜ å°„ç¨³å®šï¼Œå¾ˆå°‘éœ€è¦ remapï¼Œè¿™ä½¿å¾— mmap çš„å·¥ç¨‹æˆæœ¬ä½è€Œæ”¶ç›Šæ˜æ˜¾ã€‚åŠ ä¸Šè¯»è·¯å¾„ä»¥éšæœºè¯»ä¸ºä¸»ï¼Œmmap èƒ½æŠŠå¾ˆå¤šè¯»è½¬åŒ–æˆè½»é‡é¡µç¼ºå¤±ï¼Œé…åˆ OS çš„é¡µç¼“å­˜å½¢æˆè‡ªç„¶çš„çƒ­ç‚¹å‘½ä¸­ã€‚å› æ­¤åœ¨ SSTable ä¸Šé‡‡ç”¨ mmap é€šå¸¸æ˜¯å¯é¢„æœŸä¸”åˆç†çš„é€‰æ‹©ã€‚</p>
<h2 id="ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨-vlog-ä¸Šä¹Ÿç”¨äº†-mmap"><a class="header" href="#ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨-vlog-ä¸Šä¹Ÿç”¨äº†-mmap">ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨ VLog ä¸Šä¹Ÿç”¨äº† mmap</a></h2>
<p>æˆ‘ä»¬ç›®å‰çš„å®ç°æ–¹å¼æ˜¯è®© VLog ç›´æ¥èµ° mmapï¼Œè¿™æ ·è¯»è·¯å¾„å¯ä»¥ç”¨ Bytes/View ç›´æ¥å¾—åˆ°åˆ‡ç‰‡ï¼Œå†™å…¥ä¹Ÿå¯ä»¥é€šè¿‡ mmap buffer è¿½åŠ å¹¶é…åˆ msync è½ç›˜ï¼Œè¿™è®©å®ç°ä¿æŒç®€æ´å¹¶ä¸ SSTable çš„é£æ ¼ä¸€è‡´ã€‚ä»£ä»·åœ¨äº VLog æ–‡ä»¶å¾€å¾€æ›´å¤§ï¼Œéšæœºè¯»æ›´åˆ†æ•£ï¼Œpage cache æ±¡æŸ“é£é™©æ˜¾è‘—æ›´é«˜ï¼ŒRSS æ³¢åŠ¨ä¹Ÿæ›´å®¹æ˜“å‡ºç°ã€‚å¦‚æœ value çš„å†·çƒ­åˆ†å¸ƒä¸ç¨³å®šï¼Œmmap å¸¦æ¥çš„ç¼“å­˜æ”¶ç›Šä¸ä¸€å®šèƒ½æŠµæ¶ˆå®ƒçš„å‰¯ä½œç”¨ã€‚</p>
<h2 id="ä¸-badger-çš„æ€è·¯å¯¹æ¯”"><a class="header" href="#ä¸-badger-çš„æ€è·¯å¯¹æ¯”">ä¸ Badger çš„æ€è·¯å¯¹æ¯”</a></h2>
<p>Badger æ›´å€¾å‘äºæŠŠ mmap ç”¨åœ¨ SSTableï¼Œè€Œåœ¨ VLog ä¸Šåå‘ FileIO æˆ– preadï¼Œç›®çš„å°±æ˜¯å‡å°‘å¤§æ–‡ä»¶å¯¹é¡µç¼“å­˜çš„å†²å‡»ï¼Œè®©çƒ­ç‚¹é›†ä¸­åœ¨ SSTable çš„ block ä¸Šã€‚å®ƒä¹Ÿæä¾›äº†å¯é…ç½®çš„æ¨¡å¼ï¼Œä½†æ•´ä½“å€¾å‘ä½“ç°äº†ä¸€ä¸ªç†å¿µï¼šçƒ­ç‚¹åº”ä¸»è¦ç”± SSTable é©±åŠ¨ï¼ŒVLog æ›´åº”è¯¥è°¨æ…æ¶ˆè€— page cacheã€‚</p>
<h2 id="linux-ä¾§çš„-io-é€‰æ‹©"><a class="header" href="#linux-ä¾§çš„-io-é€‰æ‹©">Linux ä¾§çš„ IO é€‰æ‹©</a></h2>
<p>åœ¨ Linux ä¸Šæˆ‘ä»¬å¯ä»¥ç»„åˆä½¿ç”¨å¤šç§ IO æ‰‹æ®µï¼Œæ¯”å¦‚å¸¸è§„çš„ read/pread/writeï¼Œä»¥åŠ mmap é…åˆ madvise æç¤ºè®¿é—®æ¨¡å¼ï¼Œä¹Ÿå¯ä»¥ç”¨ posix_fadvise æˆ– readahead åšé¢„è¯»æç¤ºï¼›å¦‚æœéœ€è¦æ›´ç»†ç²’åº¦æ§åˆ¶ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ O_DIRECT è¿›è¡Œ direct I/Oï¼Œæˆ–è€…åŸºäº io_uring åšå¼‚æ­¥ IOã€‚æˆ‘ä»¬åœ¨ file åŒ…ä¸­å·²ç»å®ç°äº†ä¸€ä¸ªåŸºç¡€çš„ io_uring æ¡†æ¶ï¼Œåç»­å¦‚æœè¦åšæ›´å¼ºçš„å¼‚æ­¥è¯»å†™æˆ–å¹¶å‘è°ƒåº¦ï¼Œå¯ä»¥åŸºäºå®ƒæ‰©å±•ã€‚</p>
<h2 id="å°ç»“"><a class="header" href="#å°ç»“">å°ç»“</a></h2>
<p>SSTable çš„è¯»å¯†é›†ä¸ä¸å¯å˜ç‰¹æ€§è®© mmap æˆä¸ºä¸€ä¸ªç›¸å¯¹ç¨³å¦¥çš„é»˜è®¤é€‰æ‹©ï¼Œè€Œ VLog çš„å¤§æ–‡ä»¶ä¸éšæœºè¯»ç‰¹æ€§è®© mmap çš„ä»£ä»·æ›´æ˜æ˜¾ã€‚å½“å‰å®ç°åå‘å·¥ç¨‹ç®€åŒ–ï¼Œä½†ä»é•¿æœŸæ¥çœ‹ï¼ŒVLog å¯èƒ½æ›´é€‚åˆ pread + å°å‹ç¼“å­˜çš„ç­–ç•¥ï¼Œå¹¶åœ¨çƒ­ç‚¹ç¨³å®šæ—¶å†å¼€æ”¾ mmap ä½œä¸ºå¯é€‰æ¨¡å¼ã€‚</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="2026-01-16-hotring-design"><a class="header" href="#2026-01-16-hotring-design">2026-01-16 hotring design</a></h1>
<p>è¿™æ¡è®°å½•å’Œä¹‹å‰çš„ mmap choice ä¸€æ ·ï¼Œæ˜¯ä¸€ä»½åå™è¿°çš„ noteï¼Œç”¨æ¥è®² HotRing çš„è®¾è®¡åŠ¨æœºã€äº¤äº’æµç¨‹ä»¥åŠæˆ‘å¯¹å®ƒçš„ç†è§£ã€‚å®ƒä¸æ˜¯ä¸€ä¸ªâ€œç†è®ºæœ€ä¼˜â€çš„ç»“æ„ï¼Œä½†å®ƒè¶³å¤Ÿè½»ã€è¶³å¤Ÿå¿«ï¼Œä¹Ÿè¶³å¤Ÿå®ç”¨ã€‚</p>
<h2 id="è®¾è®¡åŠ¨æœº"><a class="header" href="#è®¾è®¡åŠ¨æœº">è®¾è®¡åŠ¨æœº</a></h2>
<p>åœ¨ LSM ç³»ç»Ÿé‡Œï¼Œçƒ­ç‚¹é€šå¸¸ä¸æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œä¸€å°æ’® key ä¼šæŒç»­æ”¾å¤§ç¼“å­˜æŠ–åŠ¨ã€è¯»æ”¾å¤§å’Œå†™å†²çªã€‚HotRing çš„å®šä½å°±æ˜¯æŠŠè¿™ç§çƒ­ç‚¹å¿«é€Ÿâ€œå˜æˆå¯è§â€ï¼Œè®©æˆ‘ä»¬èƒ½åœ¨ç›‘æ§ã€é™æµã€è°ƒä¼˜æ—¶å¿«é€Ÿæ‰¾åˆ°çœŸæ­£çš„çƒ­æºï¼Œè€Œä¸æ˜¯åªçœ‹åˆ°ä¸€å †æ¨¡ç³Šçš„å…¨å±€æŒ‡æ ‡ã€‚</p>
<h2 id="äº¤äº’é€»è¾‘"><a class="header" href="#äº¤äº’é€»è¾‘">äº¤äº’é€»è¾‘</a></h2>
<p>HotRing å¹¶ä¸æ”¹å˜è¯»å†™è·¯å¾„ï¼Œåªæ˜¯ä»¥æ—è·¯çš„æ–¹å¼è®°å½•è®¿é—®é¢‘æ¬¡ã€‚è¯»è¯·æ±‚æˆåŠŸå‘½ä¸­åè°ƒç”¨ Touchï¼Œå†™è¯·æ±‚åœ¨å¯ç”¨äº† WriteHotKeyLimit æ—¶è°ƒç”¨ TouchAndClampã€‚ç»Ÿè®¡ç³»ç»Ÿå®šæœŸæ‹‰å– TopNï¼ŒCLI å¯ä»¥ç›´æ¥æ˜¾ç¤ºçƒ­ç‚¹ã€‚</p>
<pre class="mermaid">flowchart LR
  R[Read path] --&gt; L[LSM lookup]
  L --&gt; H[HotRing.Touch]
  W[Write path] --&gt; C[HotRing.TouchAndClamp]
  H --&gt; B[Bucket list update]
  C --&gt; B
  B --&gt; S[TopN snapshot]
  S --&gt; X[Stats/CLI/Debug]
</pre>

<h2 id="ç¤ºä¾‹ä»£ç "><a class="header" href="#ç¤ºä¾‹ä»£ç ">ç¤ºä¾‹ä»£ç </a></h2>
<pre><code class="language-go">ring := hotring.NewHotRing(12, nil)
ring.EnableSlidingWindow(8, 250*time.Millisecond)
ring.EnableDecay(time.Second, 1)

ring.Touch("user:42")
count, limited := ring.TouchAndClamp("user:42", 128)
if limited {
    // å¯ä»¥è®°å½•å‘Šè­¦ï¼Œæˆ–è§¦å‘å†™å…¥é™æµ
    _ = count
}

hot := ring.TopN(16)
_ = hot
</code></pre>
<h2 id="ç»“æ„ç›´è§‰ä¸å®ç°é€‰æ‹©"><a class="header" href="#ç»“æ„ç›´è§‰ä¸å®ç°é€‰æ‹©">ç»“æ„ç›´è§‰ä¸å®ç°é€‰æ‹©</a></h2>
<p>HotRing çš„å†…éƒ¨æ˜¯â€œå›ºå®šæ¡¶ + æœ‰åºé“¾è¡¨â€ã€‚key å…ˆå“ˆå¸Œåˆ°æ¡¶ï¼Œç„¶ååœ¨æ¡¶å†…æŒ‰ tag + key æ’åºã€‚è¯»è·¯å¾„æ— é”ï¼Œå†™è·¯å¾„ä½¿ç”¨ CAS æ’å…¥èŠ‚ç‚¹ï¼Œé¿å…å…¨å±€é”å¸¦æ¥çš„æŠ–åŠ¨ã€‚å®ƒæ²¡æœ‰å¼•å…¥å¤æ‚çš„è¿‘ä¼¼ç»“æ„ï¼Œè€Œæ˜¯å°½é‡ä¿æŒæ•°æ®ç»“æ„ç®€å•ï¼Œè®©å®ƒèƒ½é•¿æœŸå­˜åœ¨äºè¯»å†™è·¯å¾„ä¸Šè€Œä¸æˆä¸ºè´Ÿæ‹…ã€‚</p>
<p>æ—¶é—´è¯­ä¹‰æ–¹é¢å®ƒæä¾›äº†ä¸¤ç§æ‰‹æ®µï¼šæ»‘åŠ¨çª—å£è®©çªå‘çƒ­ç‚¹è¿…é€Ÿå‡ºç°ï¼Œè¡°å‡æœºåˆ¶è®©å†å²çƒ­ç‚¹è‡ªç„¶æ·¡å‡ºï¼Œè¿™ä¸¤è€…å åŠ åï¼Œç»“æœæ›´ç¬¦åˆâ€œå®é™…çƒ­åº¦â€çš„ç›´è§‰ã€‚</p>
<h2 id="ä¸ªäººå¿ƒå¾—"><a class="header" href="#ä¸ªäººå¿ƒå¾—">ä¸ªäººå¿ƒå¾—</a></h2>
<p>HotRing æœ€æœ‰æ„æ€çš„ç‚¹ä¸æ˜¯â€œèªæ˜â€ï¼Œè€Œæ˜¯â€œå¤Ÿç”¨ä¸”ç¨³å®šâ€ã€‚å®ƒæŠŠçƒ­ç‚¹ä»ä¸å¯è§å˜æˆå¯è§ï¼Œåˆä¸ä¼šå› ä¸ºè‡ªå·±å¤ªå¤æ‚è€Œåˆ¶é€ æ–°çš„çƒ­ç‚¹ã€‚å¾ˆå¤šæ—¶å€™å·¥ç¨‹ä¸ŠçœŸæ­£éœ€è¦çš„æ˜¯â€œä¸€ä¸ªå¾ˆå¿«èƒ½å·¥ä½œçš„çƒ­é”®æ¢æµ‹å™¨â€ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç†è®ºä¸Šæ›´æ¼‚äº®ã€ä½†æˆæœ¬æ›´é«˜çš„ç»“æ„ã€‚</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="2026-02-01-vlog-design-and-gc"><a class="header" href="#2026-02-01-vlog-design-and-gc">2026-02-01 vlog design and gc</a></h1>
<p>æœ¬æ–‡æ¡£è¯¦ç»†è®°å½•äº† NoKV ä¸­ ValueLog ç»„ä»¶çš„è®¾è®¡æ€è·¯ã€æ ¸å¿ƒæœºåˆ¶ï¼ˆGCï¼‰ä»¥åŠåº•å±‚æ–‡ä»¶ç³»ç»Ÿçš„å®ç°ç»†èŠ‚ã€‚NoKV é‡‡ç”¨äº†ç±»ä¼¼ BadgerDB çš„é”®å€¼åˆ†ç¦»ï¼ˆKey-Value Separationï¼‰æ¶æ„ï¼Œå…¶ä¸­ ValueLog æ˜¯æ‰¿è½½å®é™…æ•°æ®çš„ä¸»ä½“ã€‚</p>
<hr>
<h2 id="1-æ ¸å¿ƒæ¶æ„ä¸‰å±‚æ´‹è‘±æ¨¡å‹"><a class="header" href="#1-æ ¸å¿ƒæ¶æ„ä¸‰å±‚æ´‹è‘±æ¨¡å‹">1. æ ¸å¿ƒæ¶æ„ï¼šä¸‰å±‚æ´‹è‘±æ¨¡å‹</a></h2>
<p>ValueLog çš„å®ç°é‡‡ç”¨äº†å…¸å‹çš„åˆ†å±‚æ¶æ„ï¼Œè‡ªé¡¶å‘ä¸‹åˆ†ä¸ºä¸‰å±‚ï¼š</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th style="text-align: left">å±‚çº§</th><th style="text-align: left">æ¨¡å—/æ–‡ä»¶</th><th style="text-align: left">æ ¸å¿ƒèŒè´£</th><th style="text-align: left">å…³é”®æŠ€æœ¯</th></tr>
</thead>
<tbody>
<tr><td style="text-align: left"><strong>ä¸šåŠ¡é€»è¾‘å±‚</strong></td><td style="text-align: left"><code>vlog.go</code></td><td style="text-align: left">è´Ÿè´£ GC è°ƒåº¦ã€åƒåœ¾ç»Ÿè®¡ (Discard Stats)ã€ä¸ LSM çš„äº¤äº’ï¼ˆç´¢å¼•é‡å»ºï¼‰</td><td style="text-align: left">åç¨‹è°ƒåº¦, ç»Ÿè®¡èšåˆ</td></tr>
<tr><td style="text-align: left"><strong>æ–‡ä»¶ç®¡ç†å±‚</strong></td><td style="text-align: left"><code>vlog/manager.go</code></td><td style="text-align: left">ç®¡ç† <code>.vlog</code> æ–‡ä»¶é›†åˆï¼Œå¤„ç†æ–‡ä»¶è½®è½¬ (Rotation)ã€ID æ˜ å°„ã€å¹¶å‘å¼•ç”¨</td><td style="text-align: left">è¯»å†™é”, åŸå­è®¡æ•° (Pinning)</td></tr>
<tr><td style="text-align: left"><strong>åº•å±‚ IO å±‚</strong></td><td style="text-align: left"><code>file/</code> (<code>mmap</code>)</td><td style="text-align: left">å°è£… OS æ–‡ä»¶æ“ä½œï¼Œæä¾›é«˜æ€§èƒ½çš„å†…å­˜æ˜ å°„è¯»å†™æ¥å£</td><td style="text-align: left"><code>mmap</code>, <code>msync</code>, Zero-Copy</td></tr>
</tbody>
</table>
</div>
<h3 id="æ¶æ„ç¤ºæ„å›¾-mermaid"><a class="header" href="#æ¶æ„ç¤ºæ„å›¾-mermaid">æ¶æ„ç¤ºæ„å›¾ (Mermaid)</a></h3>
<pre class="mermaid">graph TD
    subgraph "LSM Tree (Keys)"
        LSM[LSM Components] --&gt;|ValuePtr| VLogPtr[Value Pointer]
    end

    subgraph "ValueLog Subsystem"
        VLog[vlog.go&lt;br&gt;(Coordinator)]
        Mgr[vlog/manager.go&lt;br&gt;(Segment Manager)]
        
        subgraph "Physical Files (.vlog)"
            F1[00001.vlog&lt;br&gt;(Sealed)]
            F2[00002.vlog&lt;br&gt;(Sealed)]
            F3[00003.vlog&lt;br&gt;(Active)]
        end
        
        VLog --&gt;|Write/GC| Mgr
        Mgr --&gt;|Append| F3
        Mgr --&gt;|Read| F1
        Mgr --&gt;|Read| F2
    end

    LSM -.-&gt;|Points to| F1
    LSM -.-&gt;|Points to| F2
    LSM -.-&gt;|Points to| F3
</pre>

<hr>
<h2 id="2-åº•å±‚æ–‡ä»¶ç³»ç»Ÿè®¾è®¡-file"><a class="header" href="#2-åº•å±‚æ–‡ä»¶ç³»ç»Ÿè®¾è®¡-file">2. åº•å±‚æ–‡ä»¶ç³»ç»Ÿè®¾è®¡ (<code>file/</code>)</a></h2>
<p>NoKV æ”¾å¼ƒäº†æ ‡å‡†çš„ <code>read/write</code> ç³»ç»Ÿè°ƒç”¨ï¼Œå…¨é¢é‡‡ç”¨ <strong>Memory Mapped Files (mmap)</strong>ã€‚</p>
<h3 id="21-ä¸ºä»€ä¹ˆé€‰æ‹©-mmap"><a class="header" href="#21-ä¸ºä»€ä¹ˆé€‰æ‹©-mmap">2.1 ä¸ºä»€ä¹ˆé€‰æ‹© mmapï¼Ÿ</a></h3>
<ul>
<li><strong>æè‡´è¯»æ€§èƒ½</strong>ï¼šè¯»å–æ–‡ä»¶ç­‰åŒäºè®¿é—®å†…å­˜æ•°ç»„ (<code>data[offset : offset+len]</code>)ã€‚å®ç°äº†ç”¨æˆ·æ€çš„ <strong>Zero Copy</strong>ï¼ˆé›¶æ‹·è´ï¼‰ï¼Œé¿å…äº†å†…æ ¸åˆ°ç”¨æˆ·ç©ºé—´çš„ bufferæ‹·è´ã€‚</li>
<li><strong>ç®€åŒ– IO é€»è¾‘</strong>ï¼šä¸éœ€è¦å¤„ç†å¤æ‚çš„ <code>seek</code> å’Œ <code>read</code> å¾ªç¯ï¼Œç”±æ“ä½œç³»ç»Ÿè´Ÿè´£ç¼ºé¡µåŠ è½½ï¼ˆPage Faultï¼‰ã€‚</li>
</ul>
<h3 id="22-å®ç°ç»†èŠ‚-filemmap_go"><a class="header" href="#22-å®ç°ç»†èŠ‚-filemmap_go">2.2 å®ç°ç»†èŠ‚ (<code>file/mmap_*.go</code>)</a></h3>
<ul>
<li><strong>å†™å…¥ (Append)</strong>ï¼šç›´æ¥ <code>copy</code> æ•°æ®åˆ°æ˜ å°„çš„ byte slice ä¸­ã€‚</li>
<li><strong>æ‰©å®¹ (Truncate &amp; Remap)</strong>ï¼š
<ul>
<li><code>mmap</code> æ˜ å°„åŒºåŸŸå¤§å°å›ºå®šã€‚</li>
<li>å½“ç©ºé—´ä¸è¶³æ—¶ï¼Œè°ƒç”¨ <code>ftruncate</code> å¢å¤§ç‰©ç†æ–‡ä»¶ -&gt; <code>munmap</code> è§£é™¤æ˜ å°„ -&gt; <code>mmap</code> é‡æ–°æ˜ å°„æ›´å¤§çš„ç©ºé—´ã€‚</li>
<li><strong>ä¼˜åŒ–</strong>ï¼šé€šå¸¸é¢„åˆ†é…ä¸€å®šå¤§å°ï¼ˆå¦‚ 1GBï¼‰ä»¥å‡å°‘é‡æ˜ å°„å¸¦æ¥çš„æ€§èƒ½æŠ–åŠ¨ã€‚</li>
</ul>
</li>
<li><strong>æŒä¹…åŒ– (Sync)</strong>ï¼šè°ƒç”¨ <code>msync</code>ï¼Œå¼ºåˆ¶è„é¡µï¼ˆDirty Pagesï¼‰åˆ·ç›˜ã€‚</li>
</ul>
<hr>
<h2 id="3-æ–‡ä»¶ç®¡ç†ä¸å¹¶å‘æ§åˆ¶-vlogmanagergo"><a class="header" href="#3-æ–‡ä»¶ç®¡ç†ä¸å¹¶å‘æ§åˆ¶-vlogmanagergo">3. æ–‡ä»¶ç®¡ç†ä¸å¹¶å‘æ§åˆ¶ (<code>vlog/manager.go</code>)</a></h2>
<h3 id="31-æ–‡ä»¶åˆ†ç‰‡-segmentation"><a class="header" href="#31-æ–‡ä»¶åˆ†ç‰‡-segmentation">3.1 æ–‡ä»¶åˆ†ç‰‡ (Segmentation)</a></h3>
<p>æ•°æ®è¢«åˆ‡åˆ†ä¸ºå¤šä¸ªå›ºå®šå¤§å°ï¼ˆé…ç½®é¡¹ <code>ValueLogFileSize</code>ï¼‰çš„ Segment æ–‡ä»¶ã€‚</p>
<ul>
<li><strong>Active Segment</strong>ï¼šå½“å‰å”¯ä¸€å¯å†™çš„å¤´éƒ¨æ–‡ä»¶ã€‚</li>
<li><strong>Sealed Segment</strong>ï¼šå†™æ»¡åè½¬ä¸ºåªè¯»ï¼ˆé€»è¾‘ä¸Šï¼‰ï¼Œç­‰å¾… GC æˆ–è¯»å–ã€‚</li>
</ul>
<h3 id="32-å¹¶å‘å®‰å…¨æœºåˆ¶-pinning"><a class="header" href="#32-å¹¶å‘å®‰å…¨æœºåˆ¶-pinning">3.2 å¹¶å‘å®‰å…¨æœºåˆ¶ (Pinning)</a></h3>
<p>ä¸ºäº†é˜²æ­¢åœ¨ <strong>GC åˆ é™¤æ–‡ä»¶</strong> æ—¶ï¼Œä»æœ‰ <strong>è¯»è¯·æ±‚</strong> è®¿é—®è¯¥æ–‡ä»¶ï¼ŒNoKV å®ç°äº†ä¸€å¥—åŸºäºåŸå­è®¡æ•°çš„å¼•ç”¨ä¿æŠ¤æœºåˆ¶ã€‚</p>
<ul>
<li><strong>Pin (åŠ é”)</strong>ï¼šè¯»è¯·æ±‚å¼€å§‹æ—¶ï¼Œ<code>atomic.AddInt32(&amp;pinCount, 1)</code>ã€‚</li>
<li><strong>Unpin (è§£é”)</strong>ï¼šè¯»è¯·æ±‚ç»“æŸæ—¶ï¼Œ<code>atomic.AddInt32(&amp;pinCount, -1)</code>ã€‚</li>
<li><strong>Wait (é˜»å¡)</strong>ï¼šGC çº¿ç¨‹åœ¨åˆ é™¤æ–‡ä»¶å‰ï¼Œè°ƒç”¨ <code>waitForNoPins()</code>ï¼Œé˜»å¡ç›´åˆ° <code>pinCount</code> å½’é›¶ã€‚</li>
</ul>
<hr>
<h2 id="4-åƒåœ¾å›æ”¶-gc-è®¾è®¡"><a class="header" href="#4-åƒåœ¾å›æ”¶-gc-è®¾è®¡">4. åƒåœ¾å›æ”¶ (GC) è®¾è®¡</a></h2>
<p>ç”±äº LSM Tree åªèƒ½åˆ é™¤ Key çš„ç´¢å¼•ï¼ŒValueLog ä¸­çš„æ—§æ•°æ®ï¼ˆæ— æ•ˆ Valueï¼‰ä¼šæ°¸ä¹…å ç”¨ç£ç›˜ã€‚GC æ˜¯ KV åˆ†ç¦»æ¶æ„ä¸­æœ€å…³é”®çš„ä¸€ç¯ã€‚</p>
<h3 id="41-æ ¸å¿ƒç­–ç•¥é‡å†™-rewrite"><a class="header" href="#41-æ ¸å¿ƒç­–ç•¥é‡å†™-rewrite">4.1 æ ¸å¿ƒç­–ç•¥ï¼šé‡å†™ (Rewrite)</a></h3>
<p>NoKV ä¸æ”¯æŒåœ¨æ–‡ä»¶ä¸­é—´â€œæ‰“æ´â€åˆ é™¤ï¼Œè€Œæ˜¯é‡‡ç”¨ <strong>â€œé‡‡æ ·æ¬è¿ + æ•´ä½“åˆ é™¤â€</strong> çš„ç­–ç•¥ã€‚</p>
<h3 id="42-gc-æµç¨‹å›¾"><a class="header" href="#42-gc-æµç¨‹å›¾">4.2 GC æµç¨‹å›¾</a></h3>
<pre class="mermaid">sequenceDiagram
    participant GC as GC Thread
    participant Stats as Discard Stats
    participant OldFile as Old Segment (Fid: 10)
    participant LSM as LSM Tree
    participant NewFile as Active Segment

    Note over GC, Stats: 1. é€‰å¦ƒ (Pick Candidate)
    GC-&gt;&gt;Stats: æŸ¥è¯¢åƒåœ¾æœ€å¤šçš„æ–‡ä»¶
    Stats--&gt;&gt;GC: æ¨è Fid: 10

    Note over GC, OldFile: 2. éªŒè´§ (Sampling)
    GC-&gt;&gt;OldFile: éšæœºè¯»å– 10% æ•°æ®
    GC-&gt;&gt;LSM: æ£€æŸ¥ Key æ˜¯å¦å­˜æ´»?
    LSM--&gt;&gt;GC: è¿”å› Key çŠ¶æ€
    
    alt åƒåœ¾æ¯”ä¾‹ &gt; é˜ˆå€¼
        Note over GC, NewFile: 3. æ¬å®¶ (Rewrite)
        loop éå†æ•´ä¸ªæ–‡ä»¶
            GC-&gt;&gt;OldFile: è¯»å– Entry
            GC-&gt;&gt;LSM: Double Check (å¹¶å‘å®‰å…¨æ£€æŸ¥)
            alt æŒ‡é’ˆä»æŒ‡å‘ OldFile
                GC-&gt;&gt;NewFile: Append (ä½œä¸ºæ–°æ•°æ®å†™å…¥)
                GC-&gt;&gt;LSM: æ›´æ–° Key æŒ‡å‘æ–°åœ°å€
            else æŒ‡é’ˆå·²å˜æ›´
                Note right of GC: æ•°æ®å·²è¢«ç”¨æˆ·æ›´æ–°ï¼Œä¸¢å¼ƒ
            end
        end
        
        Note over GC, OldFile: 4. æ‹†è¿ (Delete)
        GC-&gt;&gt;OldFile: WaitForNoPins()
        GC-&gt;&gt;OldFile: rm 00010.vlog
    else åƒåœ¾æ¯”ä¾‹ä½
        Note right of GC: æ”¾å¼ƒï¼Œç¨åå†è¯•
    end
</pre>

<h3 id="43-å…³é”®è®¾è®¡å¿ƒå¾—"><a class="header" href="#43-å…³é”®è®¾è®¡å¿ƒå¾—">4.3 å…³é”®è®¾è®¡å¿ƒå¾—</a></h3>
<ol>
<li><strong>Discard Stats (åƒåœ¾ç»Ÿè®¡)</strong>ï¼š
<ul>
<li>æ¯æ¬¡ LSM Compaction ä¸¢å¼ƒ Key æ—¶ï¼Œéƒ½ä¼šè®°å½•å¯¹åº”çš„ ValueLog æ–‡ä»¶ ID å¢åŠ äº†å¤šå°‘åƒåœ¾ã€‚</li>
<li>ç»Ÿè®¡ä¿¡æ¯å®šæœŸæŒä¹…åŒ–åˆ°å†…éƒ¨ Key <code>!badger!discard</code> ä¸­ï¼Œé˜²æ­¢é‡å¯ä¸¢å¤±ã€‚</li>
</ul>
</li>
<li><strong>å¹¶å‘æ›´æ–°ä¿æŠ¤</strong>ï¼š
<ul>
<li>åœ¨ GC æ¬è¿æ•°æ®çš„ç¬é—´ï¼Œç”¨æˆ·å¯èƒ½å¹¶å‘æ›´æ–°äº†è¯¥ Keyã€‚</li>
<li><strong>è§£å†³æ–¹æ¡ˆ</strong>ï¼šGC åœ¨å†™å…¥æ–°å€¼å‰ï¼Œå†æ¬¡æ£€æŸ¥ LSM Treeã€‚å¦‚æœå‘ç° LSM ä¸­çš„ ValuePtr å·²ç»æŒ‡å‘äº†<strong>æ›´æ–°çš„æ–‡ä»¶</strong>ï¼ˆFid æ›´å¤§ï¼‰æˆ–<strong>æ›´æ–°çš„åç§»é‡</strong>ï¼Œè¯´æ˜ç”¨æˆ·æŠ¢å…ˆæ›´æ–°äº†ï¼ŒGC æ”¾å¼ƒæ¬è¿æ—§å€¼ï¼Œä¿è¯äº†æ•°æ®ä¸€è‡´æ€§ã€‚</li>
</ul>
</li>
<li><strong>IO ä¼˜åŒ–</strong>ï¼š
<ul>
<li>åˆ©ç”¨ <code>mmap</code> è¿›è¡Œå¿«é€Ÿé‡‡æ ·ã€‚</li>
<li>åªæœ‰åœ¨ç¡®ä¿¡â€œæœ‰å¤§é‡åƒåœ¾â€æ—¶æ‰æ‰§è¡Œæ˜‚è´µçš„å…¨æ–‡ä»¶æ‰«æå’Œé‡å†™ã€‚</li>
</ul>
</li>
</ol>
<hr>
<h2 id="5-æ€»ç»“"><a class="header" href="#5-æ€»ç»“">5. æ€»ç»“</a></h2>
<p>NoKV çš„ ValueLog è®¾è®¡åœ¨<strong>å·¥ç¨‹å®ç°</strong>ä¸Šåšå‡ºäº†ä»¥ä¸‹æƒè¡¡ï¼š</p>
<ul>
<li><strong>ç©ºé—´æ¢æ—¶é—´</strong>ï¼šå…è®¸ä¸€å®šçš„ç£ç›˜ç©ºé—´æµªè´¹ï¼ˆGC é˜ˆå€¼ï¼‰ï¼Œæ¢å–æé«˜çš„å†™å…¥æ€§èƒ½ï¼ˆé¡ºåºå†™ï¼‰ã€‚</li>
<li><strong>å¤æ‚æ€§æ¢æ€§èƒ½</strong>ï¼šå¼•å…¥äº†å¤æ‚çš„ GC æœºåˆ¶å’Œå¹¶å‘å¼•ç”¨è®¡æ•°ï¼Œæ¢å–äº† KV åˆ†ç¦»å¸¦æ¥çš„ LSM å†™æ”¾å¤§é™ä½ï¼ˆWrite Amplification Reductionï¼‰ã€‚</li>
<li><strong>OS äº²å’Œæ€§</strong>ï¼šå……åˆ†åˆ©ç”¨ <code>mmap</code> å’Œ Page Cacheï¼Œå‡å°‘äº†ç”¨æˆ·æ€çš„å†…å­˜æ‹·è´å’Œç¼“å†²ç®¡ç†å¼€é”€ã€‚</li>
</ul>
<hr>
<h2 id="6-æœªæ¥å±•æœ›kv-åˆ†ç¦»æ¶æ„çš„å‰æ²¿æ¼”è¿›"><a class="header" href="#6-æœªæ¥å±•æœ›kv-åˆ†ç¦»æ¶æ„çš„å‰æ²¿æ¼”è¿›">6. æœªæ¥å±•æœ›ï¼šKV åˆ†ç¦»æ¶æ„çš„å‰æ²¿æ¼”è¿›</a></h2>
<p>å¯¹äº KV åˆ†ç¦»ï¼ˆKey-Value Separationï¼‰å­˜å‚¨æ¶æ„ï¼ŒNoKV ç›®å‰å¤„äºâ€œç»å…¸ WiscKey å®ç°â€çš„é˜¶æ®µã€‚é’ˆå¯¹ Range Scan æ€§èƒ½å·®å’Œ GC å¼€é”€å¤§è¿™ä¸¤å¤§ç—›ç‚¹ï¼Œä»¥ä¸‹æ˜¯ NoKV æœªæ¥æ¼”è¿›çš„æ–¹å‘ï¼š</p>
<h3 id="61-ä¼˜åŒ–-gc-å¼€é”€ä»ç›²ç›®è¿½åŠ åˆ°å†·çƒ­åˆ†ç¦»"><a class="header" href="#61-ä¼˜åŒ–-gc-å¼€é”€ä»ç›²ç›®è¿½åŠ åˆ°å†·çƒ­åˆ†ç¦»">6.1 ä¼˜åŒ– GC å¼€é”€ï¼šä»â€œç›²ç›®è¿½åŠ â€åˆ°â€œå†·çƒ­åˆ†ç¦»â€</a></h3>
<p><strong>ç°çŠ¶</strong>ï¼šNoKV ç›®å‰æ˜¯ç®€å•çš„ Append-Onlyï¼Œå†·çƒ­æ•°æ®æ··æ‚åœ¨åŒä¸€ä¸ª Vlog æ–‡ä»¶ä¸­ï¼Œå¯¼è‡´ GC æ—¶éœ€è¦é‡å¤æ¬è¿å†·æ•°æ®ã€‚</p>
<p><strong>æ¼”è¿›æ–¹å‘ï¼ˆå‚è€ƒ HashKVï¼‰</strong>ï¼š</p>
<ul>
<li><strong>Hash åˆ†åŒº</strong>ï¼šæ ¹æ® <code>Hash(Key)</code> å°† Value å†™å…¥ä¸åŒçš„ Partitionã€‚</li>
<li><strong>æ”¶ç›Š</strong>ï¼šåŒä¸€ä¸ª Key çš„å†å²ç‰ˆæœ¬é›†ä¸­åœ¨ç‰¹å®šåˆ†åŒºã€‚æ›´æ–°é¢‘ç¹çš„ Key æ‰€åœ¨åˆ†åŒºå˜è„æå¿«ï¼ŒGC æ—¶å¯èƒ½ç›´æ¥åˆ é™¤æ•´ä¸ªåˆ†åŒºæ–‡ä»¶ï¼Œå®ç°â€œé›¶æ¬è¿â€å›æ”¶ã€‚</li>
</ul>
<h3 id="62-ä¼˜åŒ–èŒƒå›´æŸ¥è¯¢å°-value-å†…è”-inlining"><a class="header" href="#62-ä¼˜åŒ–èŒƒå›´æŸ¥è¯¢å°-value-å†…è”-inlining">6.2 ä¼˜åŒ–èŒƒå›´æŸ¥è¯¢ï¼šå° Value å†…è” (Inlining)</a></h3>
<p><strong>ç°çŠ¶</strong>ï¼šæ— è®º Value å¤šå°éƒ½ä¼šåˆ†ç¦»ï¼Œå¯¼è‡´ Range Scan æ—¶äº§ç”Ÿå¤§é‡éšæœº I/Oã€‚</p>
<p><strong>æ¼”è¿›æ–¹å‘ï¼ˆå‚è€ƒ RocksDB BlobDBï¼‰</strong>ï¼š</p>
<ul>
<li><strong>é˜ˆå€¼æ§åˆ¶</strong>ï¼šå¼•å…¥ <code>min_blob_size</code> å‚æ•°ï¼ˆå¦‚ 1KBï¼‰ã€‚</li>
<li><strong>é€»è¾‘</strong>ï¼šå°äº 1KB çš„ Value ç›´æ¥å­˜å…¥ SSTableï¼ˆIn-placeï¼‰ï¼Œå¤§äº 1KB çš„æ‰è¿›å…¥ ValueLogã€‚</li>
<li><strong>æ”¶ç›Š</strong>ï¼šåœ¨ YCSB-Scan åœºæ™¯ä¸‹ï¼Œç”±äºå°å¯¹è±¡éš Key é¡ºåºè¯»å–ï¼Œæ€§èƒ½å°†è·å¾—è´¨çš„æå‡ã€‚</li>
</ul>
<h3 id="63-ç¡¬ä»¶ååŒzns-ssd-ä¸-nvm"><a class="header" href="#63-ç¡¬ä»¶ååŒzns-ssd-ä¸-nvm">6.3 ç¡¬ä»¶ååŒï¼šZNS SSD ä¸ NVM</a></h3>
<ul>
<li><strong>ZNS (Zoned Namespace) SSD</strong>ï¼šåˆ©ç”¨å…¶é¡ºåºå†™çš„ç‰©ç†ç‰¹æ€§ï¼Œæ¶ˆé™¤ SSD å†…éƒ¨çš„ FTL GCï¼Œå®ç°åº•å±‚å±‚é¢çš„é›¶å†™æ”¾å¤§ã€‚</li>
<li><strong>NVM (éæ˜“å¤±å†…å­˜)</strong>ï¼šå°† VLog çš„ç´¢å¼•æˆ– L0/L1 å±‚å­˜æ”¾åœ¨ NVMï¼Œæå¤§é™ä½å…ƒæ•°æ®è®¿é—®å»¶è¿Ÿã€‚</li>
</ul>
<h3 id="64-nokv-çš„ç‹¬ç‰¹åœ°ä½"><a class="header" href="#64-nokv-çš„ç‹¬ç‰¹åœ°ä½">6.4 NoKV çš„ç‹¬ç‰¹åœ°ä½</a></h3>
<p>NoKV ç›®å‰çš„ <strong>WiscKey + HotRing</strong> ç»„åˆå·²ç»è§£å†³äº†ä¸€ä¸ªç»å…¸ç—›ç‚¹ï¼šLSM åœ¨çƒ­ç‚¹è®¿é—®ä¸‹çš„å€¾æ–œé—®é¢˜ã€‚æœªæ¥é€šè¿‡å¼•å…¥ <strong>å°å¯¹è±¡å†…è”</strong> å’Œ <strong>å†·çƒ­åˆ†åŒº</strong>ï¼ŒNoKV å°†è¿›åŒ–ä¸ºæ›´å…·ç«äº‰åŠ›çš„å·¥ä¸šçº§åˆ†å¸ƒå¼å­˜å‚¨å¼•æ“ã€‚</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->
        <script src="docs/mermaid-init-8a66ee6c.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
